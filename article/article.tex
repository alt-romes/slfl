\documentclass{llncs}

\usepackage{cmll}
\usepackage{mathpartir}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{todonotes}
\usepackage{multirow}

\newcommand{\lolli}{\multimap}
\newcommand{\tensor}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bang}{{!}}
\newcommand{\mypara}[1]{\paragraph{\textbf{#1}.}}

\newcommand{\llet}[2]{\mathsf{let}\ #1\ \mathsf{in}\ #2}

\newcommand{\synname}{\emph{SILI}}
\def\Rho{P}
\newcommand{\te}[1]{\textrm{\emph{#1}}}

\usemintedstyle{manni}

% TODO: Temos de ter o título assim? ficava mais fixe sem o número e sem o
% supervisor :P
\title{Synthesis of Linear Functional Programs}
\author{Rodrigo Mesquita - Nº 55902 \\
    Supervisor: Bernardo Toninho}
\date{July 2021}
\institute{NOVA School of Science and Technology}

\begin{document}

\maketitle

\section{Introduction}

Program synthesis is an automated or semi-automated process of deriving a
program, i.e.~generating code, from a high-level specification. Synthesis can be
seen as a means to improve programmer productivity and program correctness
(e.g. through suggestion and autocompletion).
%
Specifications can take many forms such as natural
language~\cite{chen2021evaluating},
examples~\cite{DBLP:conf/popl/FrankleOWZ16} or rich types such as
polymorphic refinement types~\cite{DBLP:conf/pldi/PolikarpovaKS16} or
graded types~\cite{DBLP:conf/lopstr/HughesO20}. Regardless of the kind
of specification, program synthesis must deal with two main
inherent sources of complexity -- search over the space of
valid programs, and interpreting user intent.

Synthesis is said to be \emph{type-driven} when it uses types as a form of program
specification and produces an expression whose type matches the specification.
Type-driven synthesis frameworks usually leverage rich types as a way to make
specifications more expressive and prune the valid programs search space, while
maintaining a ``familiar'' specification interface (types) for the user.
%
Richer type systems allow for more precise types, which can statically eliminate
various kinds of logical errors by making certain invalid program states
ill-typed (e.g.,~a ``null aware'' type system will ensure at compile-time that
you cannot dereference a null-pointer).
%
For instance, the type $\mathsf{Int} \rightarrow \mathsf{Int} \rightarrow
\mathsf{Int}$ specifies a (curried) function that takes two integers and
produces an integer. Viewed as a specification, it is extremely imprecise (there
are an infinite number of functions that satisfy this specification).  However,
the richer type $(x{:}\mathsf{Int}) \rightarrow (y{:}\mathsf{Int}) \rightarrow
\{z{:}\mathsf{Int} \mid z = x+y\}$ very precisely specifies a function that
takes integer arguments $x$ and $y$ and returns an integer $z$ that is the sum
of $x$ and $y$.

The focus of our work is on type-driven synthesis where specifications take the
form of linear types.
Linear types constrain resource
usage in programs by \emph{statically} limiting the number of times certain
resources can be used during their lifetime (linear resources must be used
\emph{exactly once}).
%
They can be applied to resource-aware programming such as concurrent programming
(e.g. session types for message passing
concurrency~\cite{DBLP:journals/mscs/CairesPT16}), memory-management
(e.g.~Rust's ownership types), safely updating-in-place mutable
structures~\cite{Bernardy_2018}, enforcing protocols for external \textsc{api}s~\cite{Bernardy_2018}, to name a few.
% TODO: Repensar esta parte dos exemplos de aplicação e de como podemos
% relacionar o nosso trabalho com futuras aplicações nestes campos

\paragraph{Contributions and Outline.} Despite their long-known potential
\cite{Wadler90lineartypes,DBLP:journals/mscs/CairesPT16,Bernardy_2018}
and strong proof-theoretic
foundations
\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05,DBLP:journals/tcs/CervesatoHP00},
synthesis with linear types combined with other advanced typing features
has generally been overlooked in the literature.  In this work we present a framework for synthesis
with linear types extended with recursive algebraic data types, parametric
polymorphism and refinements.
% TODO: extensible? proving feasability of... ? showing potential for ... ?
We first introduce linear types as specifications
and outline the synthesis process, leveraging linearity, by example
(\S~\ref{sec:overview}). We then present the formal system driving the
synthesis (\S~\ref{sec:formal_system}) and describe the architecture
of our framework named \synname, examining technical details and key
implementation challenges (\S~\ref{sec:architecture}). Finally, 
\todo{como formular a frase sobre o apêndice?}
we evaluate our work through expressiveness and performance benchmarks
(\S~\ref{sec:evaluation}) and discuss related work
(\S~\ref{sec:related}).

%This work explores the synthesis of linear functional
%programs from types based on linear logic propositions (i.e.~linear
%types) by leveraging the Curry-Howard correspondence. 

% It
% states that propositions in a logic have a direct
% mapping to types, and well-typed programs correspond to proofs of
% those propositions.  As such, having a type be a proposition in linear
% logic, we can relate a proof of it directly to a linear
% functional program — finding a proof is finding a program with that
% type. Formulating synthesis as proof search in linear logic
% allows us to inform our work with approaches from the relevant 
% literature.

%\mypara{Goals}
% % Goals
%In the end, we intend to be able to do full and partial synthesis of
%well-typed programs. Full synthesis consists of the production of a
%function (or set of) satisfying the specification; partial synthesis
%is the ``completion'' of a partial program (i.e. a function with a
%\emph{hole} in it). The work will start from a small core linear --
%i.e. \emph{resource-aware} functional language, building up to
%recursive types/functions, with potential other avenues of further
%extension. 
%%
%We will evaluate the work through
%expressiveness benchmarks (i.e. can we synthesize a term of type $T$?) and time
%measurements (i.e. how fast can we synthesize a term of type $T$?').

\section{Overview}\label{sec:overview}

The \synname\ synthetiser combines linear types with recursion,
parametric polymorphism, recursive algebraic data types, and
refinement types. The synthetiser is built on top of a system of proof-search for
linear logic. Proof search relates to program synthesis via the Curry-Howard
correspondence~\cite{}, which states that \emph{propositions in a logic} are
\emph{types}, and \emph{proofs} of those propositions are well-typed
\emph{programs} -- finding a proof of a proposition is finding a program with
that type.

Linear types make for more precise specifications than simple types
because information on which resources must necessarily be used is
encoded in the type. For instance the type $\mathsf{Int} \lolli
\mathsf{Int} \lolli \mathsf{Int}$ denotes a function that \emph{must}
use its two integer arguments to produce an integer.
Their preciseness also affects the search space:
all programs where a linear resource is used non-linearly (i.e. not
\emph{exactly once}) are ill-typed. With linearity built into the
synthesis process, usage of a linear proposition more than once is not
considered, and unused propositions are identified during synthesis,
constraining the space of valid programs.  % TODO: Reword?

The core of the synthesis is a \emph{sound} and \emph{complete} system
consisting of \emph{bottom-up} proof-search in propositional linear
logic based on \emph{focusing}~\cite{}. Our approach, being grounded
by propositions-as-types, ensures that all synthetised programs
(i.e.~proofs) are well-typed \emph{by construction} (i.e.~if the
synthesis procedure produces a program, then the program intrinsically
satisfies its specification). Moreover, we can leverage the modularity
of the proof-search based approach along two axes: first, since proof
search need not construct only closed proofs, we can effectively
synthesize program sub-expressions (i.e.~synthesis based on typed
holes); secondly, the framework is amenable to extensions to the core
propositional language, allowing for the introduction of a richer type
structure while preserving the correctness of programs by
construction.

The \synname\ programming language is naturally developed alongside
the synthetiser. Synthesis goals are inserted in the program by use of
a \emph{synth} keyword or mark, indicating a program should be
generated for a given type. Additionally, it opens the possibility of
synthesis within a context, that is, with knowledge of other functions and
structures defined in the same program.


\paragraph{Core linear calculus.} Initially, we synthesize from
specifications that only use literals and propositional linear types, such as linear logic theorems. Namely,
the specification $(A \lolli B \lolli C) \lolli (A \tensor B) \lolli C$ produces
the function \mintinline{haskell}{\a -> \b -> let c*d = b in a c d},
which takes an argument \mintinline{haskell}{a}, a
\emph{curried} linear function of type $A \lolli B \lolli C$,  and an
argument \mintinline{haskell}{b}, a \emph{linear
  pair} of an $A$ and a $B$ and produces a value of type $C$ by
deconstructing the pair and applying \mintinline{haskell}{a} to the
corresponding elements of the pair (i.e.,~\emph{uncurries} the
function \mintinline{haskell}{a}).


\paragraph{Beyond propositional logic.} To be able to synthesize more
interesting programs -- and thus, empirically prove the feasability of
more relevant synthesis in a linear context, we extend the syntax and
type system with recursiveness, parametric polymorphism, algebraic
data types (ADTs) and type refinements.


\paragraph{ADTs.} We reiterate the expressiveness of linear types: by requiring
some types to be used linearly (consumed), we can assure, e.g., the
deconstruction of an ADT. As such, the specification that identifies a function
that takes as arguments an unrestricted ($\bang$) linear function\footnote{might be used non-linearly, but its parameter is used
linearly in its body} 
($\lolli$) that converts $a$s in $b$s, a list of $a$s, and produces a
list of $b$s, written $\bang(a \lolli b) \lolli List\ a \lolli List\ b$, isn't
satisfied by the program \mintinline{haskell}{(\x -> \y -> Nil)}. To synthetise
a valid program from it, \mintinline{haskell}{(List a)} has to be deconstructed,
and its constructor arguments used. \synname\ would output the following
function (\emph{map}) without relying on any additional information:
%
% TODO: Aqui não estou bem a falar de polymorphism mas parece estranho não por
% visto que falo dos outros temas? se calhar faço um à parte?
%
\begin{minted}{haskell}
map c d = let !e = c in
  case d of
      Nil -> Nil
    | Cons f -> let g*h = f in Cons (e g, map (!e) h);
\end{minted}

\paragraph{Refinements.} Refinements are logical predicates
which must hold for any element that inhabits a given type. The \synname\
language supports simple refinements on integers with arithmetic
expressions in the predicates. They assert the robustness of our main
framework, i.e., how it is amenable to significant additions without
necessarily interfering with the overall approach.
%
As an example, from the specification $x \{Int\} \lolli y \{Int\} \lolli z
\{Int\} \lolli k \{Int\ \vert\ x + k = y * z\}$ (a function that takes three
integers and produces an integer that satisfies the given predicate), the
program\ \mintinline{haskell}{(\a -> \b -> \c -> ((0 - 1) * a) + (c * b))} is
synthetised.

\paragraph{Guiding the Search.} Specifications can be additionally
augmented with four keywords: ``using'', ``depth'', ``assert'' and ``choose''.
Through them, we can fine-tune the synthesis process and,
respectively, force certain functions to be present in the synthesis
outcome, allow a \emph{deeper} search in the valid programs space,
require given predicates to hold true in the program after the
synthesis is complete, and select between alternative results.

Taken directly from the Linear Haskell paper~\cite{Bernardy_2018}, we present a
more intricate example of synthesis: given the linear type signatures for array
primitives (\emph{newMArray} passes a new mutable array to a function that uses
it linearly, \emph{write} takes a mutable array and writes a pair to it,
\emph{freeze} consumes a mutable array and produces an imutable array,
\emph{foldl} has the default definition) we synthesize a function
``array'' that provides an immutable array given a list of pairs to write. The input program is formulated as
follows:
%
\begin{minted}{haskell}
foldl :: (a -o b -o a) -> a -o (List b) -o a;
newMArray :: Int -> (MArray a -o !b) -o b;
write :: MArray a -o (Int * a) -> MArray a;
freeze :: MArray a -o !(Array a);
synth array :: Int -> List (!(Int * a)) -> Array a
    | using (foldl) | depth 3;
\end{minted}
%
Matching exactly the linear definition for
\emph{array} from~\cite{Bernardy_2018}, \synname\
outputs (in a fraction of a second):
%
\begin{minted}{haskell}
array b c = let !d = b in let !e = c in
    newMArray (!d) (\j -> freeze (foldl (!write) j e));
\end{minted}

\section{The \synname\ Synthesis Framework}\label{sec:formal_system}
% with Linear Types, Polymorphism and Refinements

%\todo[inline]{Aqui parece-me que deves comecar por explicar o que vais fazer
%nesta seccao. Sinto falta da sintaxe da linguagem, possivelmente
%incrementalmente. Comecava por listar os tipos/termos do lambda linear
%``simples'', depois em cada subseccao acrescentava as construcoes novas e como a
%sintese funciona para esse contexto. Enfatizar que a novelty é a combinacao
%destas features todas.}

%The development of \synname\ was based on an almost direct reading of concrete
%inference rules and theoretical principles (~\cite{frank pfenning notes});
%%
%\todo{aqui queres dizer que começaste por te basear num sistema de focusing para
%a logica linear (era sequer linear a formulacao que comecaste a seguir?) O que
%são theoretical principles?}
%%
%and work on its environment was also guided by formal systems and methods
%(e.g.~Hindley-Milner type inference~\cite{}).
%%
%\todo{esta frase parece-me desnecessaria. O que queres dizer neste paragrafo,
%parece-me, é que partes de um sistema focused da literatura que depois vais
%extender, inspirando te em tecnicas standard de proof search / inferencia.}

%However, with the addition of recursion and richer types to the synthesis and
%language, formal guidelines
%%
%\todo{nao sao formal guidelines, nao existem trabalhos a combinar estas features
%todas, essa parte é importante. Além de que ao incluir estas coisas deixamos de
%estar completamente em proof search, pois muitas destas coisas (e.g. recursao)
%nao sao necessariamente sound logicamente.  Tenta usar termos especificos.
%Richer types? Richer como?}
%%
%to support the implementation become scarcer, or more complex.
%%
%\todo{nao existem ou sao complicados? ambos? citacao?}
%%
%Accordingly, rather than derived
%from rules, solutions appeared first as techniques to tame the extensions'
%inherent complex problems (such as infinite recursion), and we now present
%those techniques as rules.
%%
% \todo{ Nao percebo o que queres dizer aqui. O que sao explicit rules e implicit rules?
% Ou há regras ou nao há. O que estas a
% dizer é que inventaste tecnicas de sintese para estes casos, que correspondem
% as regras que vais apresentar. E tao explicito ou implicito como o que esta na
% literatura.}

Program synthesis from linear types with polymorphism, recursive
algebraic data types, and refinements, is essentially new in the synthesis 
literature. Despite the substantial amount of research on linear logic and
proof-search upon which we base our core synthesizer, formal guidelines for
richer types and their intrinsic challenges (such as infinite
recursion) must be developed.

In this section we formalize  the techniques that guide
synthesis from our more expressive specifications, alongside the already well
defined rules that model the core of the synthetiser, putting together a
\emph{sound} set of inference rules that characterizes our framework
for linear synthesis of
recursive programs from specifications with the select richer types,
% TODO: Better?....
and describes the system in enough detail for the synthetiser to be reproducible
by a theory-driven implementation.

We note that a \emph{sound} set of rules guarantees we cannot synthesize
incorrect programs; and that the valid programs derivable through them
reflect the somewhat subjective trade-offs we committed to. Different
choices and approaches outside the core might lead to completely
distinct synths and spaces of valid programs.


\mypara{Core Rules} The core system comprises of proof search for
(intuitionistic) linear logic sequent calculus proofs, based on a system of
resource management~\cite{resman} and focusing.
%
% TODO: Onde falamos de resource management? ( 
%
In intuitionistic sequent calculi, each connective has a so-called \emph{left}
and a \emph{right} rule, which effectively define how to decompose an ambient
assumption of a given proposition and how to prove a certain proposition is
true, respectively.  Andreoli's \emph{focusing} for linear logic~\cite{Andreolli
focusing} is a technique to remove non-essential nondeterminism from
proof-search by structuring the application of so-called invertible and
non-invertible inference rules. Andreoli observed that the
connectives of linear logic can be divided into two categories, dubbed
synchronous and asynchronous. Asynchronous connectives are those whose
right rules are \emph{invertible}, i.e. they can be applied eagerly
during proof search without losing provability and so the order in
which these rules are applied is irrelevant, and whose left rules
are not invertible. Synchronous connectives are dual. The asynchronous \todo{Escrever isto depois
de apresentar o linear $\lambda$-calculus?} 
connectives are $\lolli$ and $\with$ and the synchronous are
$\tensor,\textbf{1},\oplus,\bang$.

Given this separation, focusing divides proof search into two
phases: %
the inversion phase ($\Uparrow$), in which we apply \emph{all} invertible rules
eagerly, and the focusing phase  ($\Downarrow$), in which we decide a proposition
to focus on, and then apply non-invertible rules, staying \emph{in focus}
until we reach an asynchronous (i.e. invertible proposition), the
proof is complete, or no rules are applicable, in which case the proof must
\emph{backtrack} to the state at which the focusing phase began.
As such, with focusing, a linear logic judgment in sequent calculus ($\Gamma; \Delta, B
\vdash A$) is broken down into four different judgments, grouped into the two
phases ($\Uparrow, \Downarrow$). For the invertible phase, a third % TODO: ordered??,
context ($\Omega$) is needed to hold propositions that result from
decomposing connectives (propositions we'll later try to invert, moving them to
the linear context ($\Delta$) when we fail to). The judgments of right
invertible rules are written $\Gamma; \Delta; \Omega,B \vdash A \Uparrow$, and
those of left invertible rules are $\Gamma; \Delta; \Omega,B \Uparrow\ \vdash A$.
In the focusing phase, the focus proposition will be either the goal or come
from $\Gamma$ or $\Delta$, and appliable rules will be classified as right
non-invertible, or left non-invertible, respectively. Right non-invertible
rules have the form $\Gamma;\Delta,B \vdash A \Downarrow$, and left
non-invertible rules are written $\Gamma;\Delta; B \Downarrow\ \vdash A$.


% TODO: Explicar vários judgments?
% \todo{Primeiro explica os varios judgments (as ``formas'' das
%   conclusoes das regras) e o que significam as varias componentes,
%   nomeadamente a parte da gestao de recursos.  Depois podes explicar
%   regras concretas como fazes a seguir. Tambem seria util dizer quais
%   os tipos que sao invertiveis a esquerda/direita, pois o leitor nao
%   faz ideia do que estas a dizer.}
%
% Esta parte em geral está a ficar pouco coerente, com a introdução do focusing,
  % a suposta introdução dos judgments, e a introdução da gramática... deixa

The core language is a simply-typed linear $\lambda$-calculus with linear
functions ($\lolli$), additive ($\with$) and multiplicative ($\tensor$) pairs,
multiplicative unit (\textbf{1}), additive sums ($\oplus$)
and the exponential modality ($\bang$) (to internalize unrestricted use of variables), for
which the terms ($M,N$) and types' ($\tau$) syntax is constructed with the
following grammar:
%
% TODO: Nao é um problema eu dizer que o A e B são átomos ali em cima e aqui
% usá-los como tipos gerais? NAO :)
\[
\begin{tabular}{lclc}
$M,N$ & $\ ::=\ $ & $u, v$ & \emph{variables} \\
    & & $\vert\ \lambda x. M\ \vert\ M\ N$ & $\lolli$ \\
    & & $\vert\ M \with N\ \vert\ \mathsf{fst}\ M\ \vert\ \mathsf{snd}\ M$ & $\with$ \\
    & & $\vert\ M \tensor N\ \vert\ \llet{u\tensor v = M}{N}$ & $\tensor$ \\
    & & $\vert\ \star\ \vert\ \llet{\star = M}{N}$ & $\textbf{1}$ \\
    & & $\vert\ \mathsf{inl}\ M\ \vert\ \mathsf{inr}\ M\ \vert\ (\mathsf{case}\
    M\ \mathsf{of\ inl}\ u \Rightarrow N_1\ \vert\ \mathsf{inr}\ v \Rightarrow
    N_2)$ & $\oplus$ \\
    & & $\vert\ \bang M\ \vert\ \llet{\bang u = M}{N}$ & $\bang$ \\
    \\
$\tau$ & $\ ::=\ $ & $A, B, C\ \vert\ \tau \lolli \tau\ \vert\ \tau \with \tau\
    \vert\ \tau \tensor \tau\ \vert\ \textbf{1}\ \vert\ \tau \oplus \tau\ \vert\ \bang \tau$
\end{tabular}
\]
%
Putting together linear logic and linear lambda calculus through the Curry-Howard correspondence, resource
management, and focusing, we get the following core formal system (%heavily
inspired
by Frank Pfenning's notes on focusing for linear logic~\cite{FPnotes...}) -- in which the
rule $\lolli$R is read: to synthesize a program of type $A \lolli B$ while inverting
right (the $\Uparrow$ on the goal), with unrestricted context $\Gamma$, linear
context $\Delta$, and inversion context $\Omega$, synthetise a program of type $B$ with
an additional hypothesis of type $A$ named $x$ in the $\Omega$ context,
resulting in the program $M$ and output linear
context $\Delta'$ that cannot contain the added hypothesis $x{:}A$. Finally, the
resulting program is $\lambda x . M$ and the output linear context is
$\Delta'$.

We begin with the right invertible rules:
\begin{mathpar}

    % -o R
    \infer*[right=($\lolli R$)]
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \vdash M : B \Uparrow \and x
    \notin \Delta'}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash \lambda x . M : A
    \lolli B \Uparrow}

\and

    % & R
    \infer*[right=($\with R$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \vdash M : A \Uparrow \and \Gamma ;
    \Delta/ \Delta'' ; \Omega \vdash N : B \Uparrow \and \Delta' = \Delta''}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash  (M \with N) : A
    \with B \Uparrow}

\end{mathpar}

When we reach a non-invertible proposition on the right, we start inverting the
$\Omega$ context. The rule to transition to inversion on the left is:
\begin{mathpar}
    \infer*[right=($\Uparrow$R)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash C \and C\ \textrm{not
    right asynchronous}}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash C \Uparrow}
\end{mathpar}

Followed by the left invertible rules:
\begin{mathpar}

    \infer*[right=($\tensor L$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega, y{:}A, z{:}B \Uparrow\ \vdash M : C
    \and y,z \notin \Delta'}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \tensor B \Uparrow\ \vdash\
    \textrm{let}\ y \tensor z = x\ \textrm{in}\ M : C}


\and

    \infer*[right=($1 L$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash M : C}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}1 \Uparrow\ \vdash\ \textrm{let}\
    \star =
    x\ \textrm{in}\ M : C}

\and

    \mprset{flushleft}
    \infer*[right=($\oplus L$)]
    {
    \Gamma ; \Delta/ \Delta' ; \Omega, y{:}A \Uparrow\ \vdash M : C \and
    y \notin \Delta' \\
    \Gamma ; \Delta/ \Delta'' ; \Omega, z{:}B \Uparrow\ \vdash N : C \\
    z \notin \Delta'' \\
    \Delta' = \Delta''
    }
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \oplus B \Uparrow\ \vdash\
    \textrm{case}\ x\ \textrm{of}\ \textrm{inl}\ y \rightarrow M\ |\
    \textrm{inr}\ z \rightarrow N : C}

\and

    \infer*[right=($\bang L$)]
    {\Gamma, y{:}A ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash M : C}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}\bang A \Uparrow\ \vdash\
    \textrm{let}\ \bang y = x\ \textrm{in}\ M : C}

\end{mathpar}

When we find a synchronous (i.e. non-invertible) proposition in $\Omega$,
we simply move it to the linear context $\Delta$, and keep inverting on the left:
\begin{mathpar}
    \infer*[right=($\Uparrow$L)]
    {\Gamma; \Delta, A/\Delta'; \Omega \Uparrow\ \vdash C \and A\ 
    \textrm{not left asynchronous}}
    {\Gamma; \Delta/\Delta'; \Omega, A \Uparrow\ \vdash C}
\end{mathpar}

After inverting all the asynchronous propositions in $\Omega$ we'll reach a state
where there are no more propositions to invert ($\Gamma'; \Delta'; \cdot
\Uparrow\ \vdash C$). At this point, we want to \emph{focus} on a proposition.
The focus object can be any of the three: the proposition on the right (the
goal), a proposition from the linear $\Delta$ context, or a proposition from the
unrestricted $\Gamma$ context. For these options we have three \emph{decision}
rules:
\begin{mathpar}
    \infer*[right=(decideR)]
    {\Gamma; \Delta/\Delta' \vdash C \Downarrow \and C\ \textrm{not atomic}}
    {\Gamma; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
\and
    \infer*[right=(decideL)]
    {\Gamma; \Delta/\Delta' ; A \Downarrow\ \vdash C}
    {\Gamma; \Delta, A/\Delta';\cdot \Uparrow\ \vdash C}
\and
    \infer*[right=(decideL!)]
    {\Gamma, A; \Delta/\Delta' ; A \Downarrow\ \vdash C}
    {\Gamma, A; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
\end{mathpar}

The decision rules are followed by either left or right focus rules. The left
focus rule $\lolli$L is read ``to synthesize a program of type $C$ with input
linear context $\Delta$ and while left focused on a proposition $x$ of type $A
\lolli B$, synthesize a program $M$ of type $C$ with input $\Delta$ and 
left focused ($\Downarrow$) on $y$ of type
$B$, and output the context $\Delta'$ (with the unused
linear propositions); then, switching to the right inversion judgment ($\Uparrow$), synthesize a program $N$ of type $A$ with input
$\Delta'$ and output context $\Delta''$; finally, replace in $M$ all occurrences
of variable $y$ with program $x\ N$ (apply $x$ to $N$)''. It can be understood
as the synthesis of a program of type $C$ using the return value (of type $B$) from the function $A \lolli B$, and then the
synthesis of an argument of type $A$ to apply the function to.
\begin{mathpar}
    \infer*[right=($\tensor R$)]
    {\Gamma; \Delta/\Delta' \vdash M : A \Downarrow \and \Gamma ; \Delta'/\Delta'' \vdash N
    : B \Downarrow}
    {\Gamma; \Delta/\Delta'' \vdash (M \tensor N) : A \tensor B \Downarrow}
\and
    \infer*[right=($1 R$)]
    { }
    {\Gamma; \Delta/\Delta \vdash \star : \textbf{1} \Downarrow}
\and
    \infer*[right=($\oplus R_1$)]
    {\Gamma; \Delta/\Delta' \vdash M : A \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ \textrm{inl}\ M : A \oplus B \Downarrow}
\and
    \infer*[right=($\oplus R_2$)]
    {\Gamma; \Delta/\Delta' \vdash M : B \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ \textrm{inr}\ M : A \oplus B \Downarrow}
\and
    \infer*[right=($\bang R$)]
    {\Gamma; \Delta/\Delta'; \cdot \vdash M : A \Uparrow \and \Delta = \Delta'}
    {\Gamma; \Delta/\Delta \vdash \bang M : \bang A \Downarrow}
\and
    \infer*[right=($\lolli L$)]
    {\Gamma; \Delta/\Delta'; y{:}B \Downarrow\ \vdash M : C \and \Gamma;
    \Delta'/\Delta''; \cdot \vdash N : A \Uparrow}
    {\Gamma; \Delta/\Delta''; x{:}A \lolli B \Downarrow\ \vdash M\{(x\,N)/y\} : C}
\and
    \infer*[right=($\with L_1$)]
    {\Gamma; \Delta/\Delta'; y{:}A \Downarrow\ \vdash M : C}
    {\Gamma; \Delta/\Delta'; x{:}A \with B \Downarrow\ \vdash M\{(\textrm{fst}\ x)/y\} : C}
\and
    \infer*[right=($\with L_2$)]
    {\Gamma; \Delta/\Delta'; y{:}B \Downarrow\ \vdash M : C}
    {\Gamma; \Delta/\Delta'; x{:}A \with B \Downarrow\ \vdash M\{(\textrm{snd}\ x)/y\} : C}
\end{mathpar}
%
Eventually, the focus proposition will no longer be synchronous, i.e. it's
atomic or asynchronous. If we're left focused on an atomic proposition we either
instantiate the goal or fail. Otherwise the left focus is asynchronous and we
can start inverting it. If we're right focused on a proposition that isn't right
synchronous, we switch to inversion as well. Three rules model these conditions:
%
\begin{mathpar}
    \infer*[right=(init)]
    {  }
    {\Gamma; \Delta/\Delta'; x{:}A \Downarrow\ \vdash x : A}
\and
    \infer*[right=($\Downarrow L$)]
    {\Gamma; \Delta/\Delta'; A \Uparrow\ \vdash C \and A\ \textrm{not atomic and not left synchronous}}
    {\Gamma; \Delta/\Delta'; A \Downarrow\ \vdash C}
\and
    \infer*[right=($\Downarrow R$)]
    {\Gamma; \Delta/\Delta'; \cdot \vdash A \Uparrow}
    {\Gamma; \Delta/\Delta' \vdash A \Downarrow}
\end{mathpar}
The rules written above together make the core of our synthetiser. Next, we'll
present new rules that align and build on top of these to synthetise recursive
programs from more expressive (richer) types.

\mypara{Algebraic Data Types} In its simplest form, an algebraic data type (ADT)
is a tagged sum of any type, i.e. a named type that can be instantiated by one of many
tags (or constructors) that take some value of  a fixed type, which might
be, e.g., a product type ($A \tensor B$), or unit ($1$), in practice allowing
for constructors with an arbitrary number of parameters. In the \synname\
language, the programmer can define custom ADTs; as an example, we show the
definition of an ADT which holds zero, one, or two
values of type $A$, using the syntax: \mintinline{haskell}{data Container
= None 1 | One A | Two (A * A)}. The grammar is extended as:
\[
\begin{tabular}{lclc}
    $M,N$ & $\ ::=\ $ & $\dots\ \vert\ \emph{Cons}_n\ M\ \vert\ (\mathsf{case}\
    M\ \mathsf{of}\ \dots\ \vert\ \emph{Cons}_n\ u \Rightarrow N)$ \\
    $\tau$ & $\ ::=\ $ & $\dots\ \vert\ \langle\emph{name}\rangle$ \\
    $ADT$ & $\ ::=\ $ & (\textsf{data} $\langle\emph{name}\rangle$\ $=$\ $\emph{Cons}_1\ \tau$\
$\vert$\ $\dots$\ $\vert$\ $\emph{Cons}_n\ \tau$) \\ \end{tabular}
\]
%TODO: Como definir gramática dos ADTs?

The semantics of ADTs relate to those of the plus ($\oplus$) type
-- both are additive disjunctions.  To construct a value of an ADT we
must use one of its constructors, similar to the way $\oplus$ requires
only proof of either the left or right type it consists
of. Analogously, we can deconstruct a value of an ADT via pattern
matching on its constructors, where all branches of the pattern match
must have the same type -- akin to the left rule for the $\oplus$
connective. In effect, the inference rules for a simple ADT are a
generalized form of the $\oplus$ rules.  Therefore, there's one left
rule for ADTs, and an arbitrary number of right rules, one for each
constructor:

\begin{mathpar}
    \infer*[right=(adtR)]
    {\Gamma; \Delta/\Delta' \vdash M : X_n \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ C_n \ M : T \Downarrow}
\and
    \mprset{flushleft}
    \infer*[right=(adtL)]
    {
        \Gamma ; \Delta/ \Delta'_1 ; \Omega, y_1{:}X_1 \Uparrow\ \vdash M_1 : C
        \and
        y_1 \notin \Delta'_1
        \\
        \Gamma ; \Delta/ \Delta'_2 ; \Omega, y_2{:}X_2 \Uparrow\ \vdash M_2 : C
        \\
        y_2 \notin \Delta'_2
        \\\\
        \\ \dots
        \\
        \Gamma ; \Delta/ \Delta'_n ; \Omega, y_n{:}X_n \Uparrow\ \vdash M_n : C
        \\
        y_n \notin \Delta'_n
        \\
        \Delta'_1 = \Delta'_2 = \dots = \Delta'_n
    }
    {\Gamma ; \Delta/\Delta'_1 ; \Omega, x{:}T \Uparrow\
    \vdash\ \textrm{case}\ x\ \textrm{of}\ \dots\ |\ C_n\ y_n
    \rightarrow M_n : C}
\end{mathpar}
where the ADT $T$ and its constructors stand for any ADT with form \begin{minted}{haskell}
    data T = C1 X1 | C2 X2 | ... | Cn Xn
\end{minted}

% repetition does not legitimize :p

A more general formulation of ADTs says an ADT can be recursive (or
"inductively defined"), meaning constructors can take as arguments
values of the type they are defining. This change has
a significant impact in the synthesis process. Take, for instance, the ADT
defined as \mintinline{haskell}{data T = C1 T}, the synthesis goal
$T \lolli C$, and its derivation:
%
\begin{mathpar}
    \infer*[right=($\lolli R$)]
    {
        \infer*[right=($\Uparrow R$)]
        {
            \infer*[right=(adtL)]
            {
                \infer*[right=(adtL)]
                {\dots}
                {\Gamma; \Delta/\Delta'; \Omega, y{:}T \Uparrow\ \vdash \textrm{case}\ y\ 
                \textrm{of}\ C_1\ z \rightarrow \dots : C}
            }
            {\Gamma; \Delta/\Delta'; \Omega, x{:}T \Uparrow\ \vdash \textrm{case}\ x\ 
            \textrm{of}\ C_1\ y \rightarrow \dots : C}
        }
        {\Gamma; \Delta/\Delta'; \Omega, x{:}T \vdash \dots : C
        \Uparrow}
    }
    {\Gamma; \Delta/\Delta'; \Omega \vdash \lambda x . \dots :
    T \lolli C \Uparrow}
\end{mathpar}
%
Using our current system, we are to apply an infinite number of times
the left ADT rule, never closing the proof. Symmetrically, the
derivation for the goal $T$ is also infinite.
%
\begin{mathpar}
    \infer*[left=(adtR)]
    {
        \infer*[left=(adtR)]
        {
            \infer*[left=(adtR)]
            {\dots}
            {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
        }
        {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
    }
    {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
\end{mathpar}
%
To account for this situation, we impede the decomposition of an ADT in subsequent
proofs of its branches, and, symmetrically, don't allow construction of an
ADT when trying to synthetise an argument for its constructor.
%
%
For this, we need two more contexts, $\Rho_C$
for constraints on construction and $\Rho_D$ for constraints on
deconstruction. Together, they hold a list of ADTs that cannot be constructed or
deconstructed at a given point in the proof. For convenience, they are represented by a single $\Rho$ if
unused. All non-ADT rules trivially propagate these. The ADT rules
are then extended as follows, where $\Rho'_C = \Rho_C,T$ if $T$ is
recursive and $\Rho'_C = \Rho_C$ otherwise ($\Rho'_D$ is dual):
%
%TODO: ... we can actually instance ADTs that take no arguments even if they are restricted
\begin{mathpar}
    \infer*[right=(adtR)]
    {(\Rho_C'; \Rho_D) ; \Gamma; \Delta/\Delta' \vdash M : X_n \Downarrow \and
    T \notin \Rho_C}
    {(\Rho_C; \Rho_D);\Gamma; \Delta/\Delta' \vdash\ C_n \ M : T \Downarrow}
\and
    \mprset{flushleft}
    \infer*[right=(adtL)]
    {
        T \notin \Rho_D
        \and
        \Delta'_1 = \dots = \Delta'_n 
        \\
        (\Rho_C; \Rho'_D);\Gamma ; \Delta/ \Delta'_1 ; \Omega, y_1{:}X_1 \Uparrow\ \vdash M_1 : C
        \and
        y_1 \notin \Delta'_1
        \\\\
        \\ \dots
        \\\\
        (\Rho_C; \Rho'_D);\Gamma ; \Delta/ \Delta'_n ; \Omega, y_n{:}X_n \Uparrow\ \vdash M_n : C
        \and
        y_n \notin \Delta'_n
    }
    {(\Rho_C; \Rho_D); \Gamma ; \Delta/\Delta'_1 ; \Omega, x{:}T \Uparrow\
    \vdash\ \textrm{case}\ x\ \textrm{of}\ \dots\ |\ C_n\ y_n
    \rightarrow M_n : C}
\end{mathpar}
% where
% \begin{mathpar}
%     \Rho'_C = \textrm{\textbf{if}}\ T\ \textrm{is recursive \textbf{then}}\ \Rho_C,
%     T\ \textrm{\textbf{else}}\ \Rho_C

%     \Rho'_D = \textrm{likewise}
% \end{mathpar}

These modifications prevent the infinite derivations in the scenarios
described above. However, they also greatly limit the space of
derivable programs, leaving the synthesizer effectively unable to
synthesize from specifications with recursive types. To prevent this,
we add three rules to complement the restrictions on construction
and destruction of recursive types.
%
First, since we can't deconstruct some ADTs any further because of a restriction,
but must utilize all propositions linearly in some way, all propositions in $\Omega$ whose
deconstruction is restricted are to be moved to the linear context $\Delta$.
%
Second, without any additional rules, an ADT in the linear context
will loop back to the inversion context, jumping back and forth
between the two contexts; instead, when focusing on an ADT, we should
either instantiate the goal (provided they're the same type), or switch
to inversion if and only if its decomposition isn't restricted. The
three following rules ensure this:

\begin{mathpar}
    \infer*[right=(adt$\Uparrow$L)]
    {
        (\Rho_C; \Rho_D);\Gamma; \Delta, x{:}T/\Delta'; \Omega \Uparrow\ \vdash M : C
        \and
        T \in \Rho_D
    }
    {(\Rho_C; \Rho_D);\Gamma; \Delta/\Delta'; \Omega, x{:}T \Uparrow\ \vdash M : C}
    \and
    \infer*[right=(adt-init)]
    {  }
    {\Rho; \Gamma; \Delta/\Delta'; x{:}T \Downarrow\ \vdash x : T}
    \and
    \infer*[right=(adt$\Downarrow$L)]
    {(\Rho_C; \Rho_D); \Gamma; \Delta/\Delta'; x{:}T \Uparrow\ \vdash M :
    T \and T \notin \Rho_D}
    {(\Rho_C; \Rho_D); \Gamma; \Delta/\Delta'; x{:}T \Downarrow\ \vdash M : T}
\end{mathpar}
% TODO: Adicionar regra que diz que se for ADT Rec -o A então a construção de
% ADT Rec é logo restrita?? parece que acelera minimamente mas não arranjo um
% exemplo em que seja crucial. pode ser uma coisa que tenha ficado para resolver
% uma coisa antiga que ficou resolvida mais tarde com outra modificação e então
% é agora inutil. Talvez seja demais adicionar à regra
%
Altogether, the rules above ensure that a recursive ADT will be deconstructed
once, and that subsequent equal ADTs will only be useable from the linear
context -- essentially forcing them to be used to instantiate another proposition,
which will typically be an argument for the recursive call.


\mypara{Recursion} The main idea behind synthesis of recursive
programs is the labeling of the main goal and the addition of its
type, under that name, to the unrestricted context. That is, to
synthesize a recursive function of type $A \lolli B$ named \emph{f},
the initial judgment can be written as
\begin{mathpar}
    \infer
    {\dots}
    {\Gamma, f{:}A \lolli B; \Delta/\Delta'; \Omega \vdash M :
    A \lolli B \Uparrow}
\end{mathpar}
and, by definition, all subsequent inference rules will have
($f{:}A \lolli B$) in the $\Gamma$ context too.
We can also force the usage of the recursive call by adding it not only to the
unrestricted context, but to the linear one as well.
%
However, we must restrict immediate uses of the recursive call since
otherwise every goal would have a trivial proof (a non-terminating
function that just calls itself), shadowing relevant solutions.
Instead, our framework allows the use of recursion only after having
deconstructed a recursive ADT via the following invariant: the
recursive hypothesis can only be used in \emph{recursive branches of
  ADT deconstruction}, i.e. the recursive call should only take
``smaller'', recursive, hypothesis as arguments. To illustrate, in any
recursive function with a list argument (whose type is defined as
\mintinline{haskell}{data List = Nil | Cons (A * List)}), recursive
calls are only allowed when considering a judgment of the form
$\textrm {List} \vdash C$, i.e.~when a list value is available to
produce the goal $C$, and only in the \emph{Cons}
branch. Furthermore, we also forbid the usage of the recursive function when
synthesizing arguments to use it.
% TODO : não sei se precisa de melhor explicação mas foi uma coisa que fiz para
% não gerar um programa recursivo.
% (TODO: rewrite? O bold é meio estranho não?)


\mypara{Polymorphic Types} A polymorphic specification is a type of form
$\forall \overline{\alpha}.\ \tau$ where $\overline{\alpha}$ is a set of
variables that stand for any (non-polymorphic) type in $\tau$. Such a type is
also called a \emph{scheme}.  Synthesis of a scheme comprises of turning it into
a non-quantified type, and then treating its type variables uniformly.  First,
type variables are considered \emph{atomic types}, then, we instantiate the
bound variables of the scheme as described by the Hindley-Milner inference
method's~\cite{HM-infer} instantiation rule (put simply, generate fresh names
for each bound type variable); e.g. the scheme $\forall \alpha.\ \alpha \lolli
\alpha$ could be instantiated to $\alpha0 \lolli \alpha0$. We add a rule for
this, where $\forall \overline{\alpha}.\ \tau \sqsubseteq \tau'$ indicates type
$\tau'$ is an \emph{instantiation} of type scheme $\forall \overline{\alpha}.\
\tau$:
%
\begin{mathpar}
    \infer*[right=($\forall R$)]
    { \Rho; \Gamma; \Delta/\Delta'; \Omega \vdash \tau' \Uparrow \and \forall
    \overline{\alpha}.\ \tau
    \sqsubseteq \tau'}
    {\Rho; \Gamma; \Delta/\Delta'; \Omega \vdash \forall \overline{\alpha}.\
    \tau \Uparrow}
\end{mathpar}
%
As such, the construction of a derivation in which the only rule that can derive
an atom is the \textsc{init} rule corresponds to the synthesis of a program
where some expressions are treated agnostically (nothing constrains their type),
i.e.~a polymorphic program. The simplest example is the polymorphic function
\emph{id} of type $\forall \alpha .\ \alpha \lolli \alpha$. The program
synthesized from that specification is $\lambda x . x$, a lambda abstraction
that does not constrain the type of its parameter $x$ in any way.

The main challenge of polymorphism in synthesis is the usage of schemes from the
unrestricted context.  To begin with, $\Gamma$ now holds both (monomorphic)
types and schemes. Consequently, after the rule \textsc{decideLeft!} is applied,
we are left-focused on either a type or a scheme. Since left focus on a type is
already well defined, we need only specify how to focus on a scheme.
%
Our algorithm instantiates bound type variables of the focused scheme with fresh
\emph{existential} type variables, and the instantiated type becomes the left
focus. Inspired by the Hindley-Milner system, we also generate inference
constraints on the existential type variables (postponing the decision of what
type it should be to be used in the proof), and collect them in a new
constraints context $\Theta$ that is propagated across derivation branches the
same way the linear context is (by having an input and output context
($\Theta/\Theta'$)).  In contrast to Hindley-Milner's inference, everytime a
constraint is added it is solved against all other constraints -- a branch of
the proof search is desired to fail as soon as possible.
%TODO: Como fazer? \todo{O que significa $?\alpha$ ? Não foi explicado -- no
%Hindley-Milner nao ha propriamente o problema de misturar variaveis
%existenciais com universais}
Note that we instantiate the scheme with \emph{existential} type variables
($?\alpha$) rather than just type variables ($\alpha$) since the latter
represent universal types during synthesis, and the former represent a concrete
instance of a scheme, that might induce constraints on other type variables.
Additionally, we require that all existential type variables are assigned a
type. These concepts are formalized with the following rules, where $\forall
\overline{\alpha}.\ \tau \sqsubseteq_E \tau'$ means type $\tau'$
is an \emph{existential instantiation} of scheme $\forall \overline{\alpha}.\ \tau$,
$\textrm{ftv}_E(\tau')$ is the set of free \emph{existential} type variables in
type $\tau'$, $?\alpha \mapsto \tau_x$ is a mapping from \emph{existential} type
$?\alpha$ to type $\tau_x$, and $\textsc{unify}(c, \Theta)$ indicates wether
constrain $c$ can be unified with those in $\Theta$:

\begin{mathpar}
    \infer*[right=($\forall L$)]
    {
        \Theta/\Theta'; \Rho; \Gamma; \Delta/\Delta'; \tau' \Downarrow\ \vdash C
        \\
        \forall \overline{\alpha}.\ \tau \sqsubseteq_E \tau'
        \\
        \textrm{ftv}_E(\tau') \cap \{ ?\alpha\ \vert\ (?\alpha \mapsto \tau_x) \in \Theta'\} = \emptyset
    }
    {\Theta/\Theta'; \Rho; \Gamma; \Delta/\Delta'; \forall \overline{\alpha}.\ \tau \Downarrow\ \vdash C}
    \and
    \infer*[right=($?L$)]
    {\textsc{unify}(?\alpha
    \mapsto C, \Theta)}
    {\Theta/\Theta, ?\alpha \mapsto C ; \Rho; \Gamma; \Delta/\Delta';
    x{:}?\alpha \Downarrow\ \vdash x : C}
    \and
    \infer*[right=($\Downarrow L?$)]
    {\textsc{unify}(?\alpha
    \mapsto A, \Theta)}
    {\Theta/\Theta, ?\alpha \mapsto A ; \Rho; \Gamma; \Delta/\Delta';
    x{:}A \Downarrow\ \vdash x : ?\alpha}
\end{mathpar}


\mypara{Further Challenges} We now consider two more sources of infinite
recursion in the synthesis process. The first is the use of an unrestricted
function to synthetise a term of type $\tau$ that in turn will require a term of
the same type $\tau$. An example is the sub-goal judgment $(a \lolli
b \lolli b); (a \lolli b \lolli b) \Downarrow\ \vdash b$ that
appears while synthesizing \emph{foldr} -- we apply ($\lolli$L)
until we can use \textsc{init} ($b \Downarrow\ \vdash b$), and then we must
synthesize an argument of type $b$. Without any additional restrictions, we
may become again left focused on $(a \lolli b \lolli b)$, and again require $b$,
and on and on. The solution will be to disallow the usage of the same function
to synthetise the same goal a second time further down in the derivation.

% TODO TODO: Big rewrite
The other situation occurs when using an unrestricted polymorphic function that
requires synthesis of a term with an existential type when the goal is an
existential type. In contrast to the previous problem, the type of the goal and
of the argument that will cause the loop won't match exactly, since instantiated
bound variables are always fresh. For example, for $\forall \alpha,\beta .\
\alpha\lolli\beta\lolli\beta;?\alpha\lolli?\beta\lolli?\beta \Downarrow\ \vdash\
?\sigma$, we'll unify $?\beta$ with $?\sigma$, and then require a term of type
$?\beta$ (not $?\sigma$). We want to forbid the usage of the \emph{same}
function to attain \emph{any} existential goal, provided that function might
create existential sub-goals (i.e. it's polymorphic). However, we noticed that,
even though for most tried problems this ``same function'' approach worked,
context-heavy problems such as \emph{array} (seen in Section~\ref{sec:overview})
wouldn't terminate in a reasonable amount of time.
%
% In fact, with $n$ polymorphic functions in the unrestricted context, the
% complexity of searching for a program of any existential type $?\alpha$, while
% restricting solely the used function, is $O(n!)$~\ref{exemplo_apendice?}.
%
As such, we'll instead forbid the usage of \emph{all} unrestricted polymorphic
functions to synthetise any existential type if, altogether, we've already used
$d_e$ times a polymorphic function for \emph{any} existential sub-goal, where
$d_e$ is a \emph{constant existential depth} that controls the maximum depth of
``existential synthetis''.
%
% This approach reduces the complexity of our proof-search algorithm for
% existential types to $O(\tfrac{n!}{d_e!}) = O(n^{d_e})$.

Extending the restrictions context ($\Rho$) with restrictions on using the
unrestricted context ($\Rho_{L!}$), we modify \textsc{decideLeft!} to formalize
the previous paragraph:
\begin{mathpar}
    \mprset{flushleft}
    \infer*[right=(decideL!)]
    {
    (A, C) \notin \Rho_{L!}
    \\
    \textsc{isExist}(C) \Rightarrow |\{u\ |\
    (f,u)\in\Rho_{L!},\textsc{isExist}(u),\textsc{isPoly}(f)\}| < d_e
    \\
    \Theta/\Theta';(\Rho_C,\Rho_D,\Rho_{L!}');\Gamma, A; \Delta/\Delta' ; A
    \Downarrow\ \vdash C
    }
    {\Theta/\Theta';(\Rho_C,\Rho_D,\Rho_{L!});\Gamma, A; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
    \\
    \Rho_{L!}' =\ \textbf{if}\ \textsc{isFunction}(A)\ \textbf{then}\ \Rho_{L!},(A, C)\ \textbf{else}\ \Rho_{L!}
\end{mathpar}

\mypara{Polymorphic ADTs} To allow type parameters and the use of universally quantified type
variables in ADT constructors, we must guarantee that the
\textsc{adt-init} rule can unify the type parameters and that when constructing or
destructing an ADT, type variables in constructor parameters are substituted by
the actual type (i.e. to construct \mintinline{haskell}{List Int} with
\mintinline{haskell}{data List a = Cons (a *
List a)}, we wouldn't try to synthetise \mintinline{haskell}{(a * List a)},
but rather \mintinline{haskell}{(Int * List Int)}). To unify $T_{\overline\alpha}$ with $T_{\overline\beta}$,
the sets of type parameters $\overline\alpha$ and $\overline\beta$ must satisfy $|\overline\alpha| = |\overline\beta|$
together with $\forall i\ 0 \leq i \land i < |\overline\alpha| \land
\textsc{unify}(\overline\alpha_i \mapsto \overline\beta_i)$. The constructor
type substitution needn't be explicit in the rule:
\begin{mathpar}
    \infer*[right=(adt-init)]
    {\textsc{unify}(T_{\overline\alpha} \mapsto T_{\overline\beta}, \Theta)}
    {\Theta/\Theta,T_{\overline\alpha} \mapsto T_{\overline\beta},\Rho; \Gamma; \Delta/\Delta'; x{:}T_{\overline\alpha} \Downarrow\ \vdash x :
    T_{\overline\beta}}
\end{mathpar}

\mypara{Refinement Types} Refinement types are defined as a types
with a name and with a predicate (a non-existing predicate is the same as it
being always \emph{true}); dependent types are functions with
refinement types in which the argument type name can be used in the predicates
of the return type (e.g. $x\{\mathsf{Int}\} \lolli y\{\mathsf{Int}\ \vert\ y = x\}$ specifies a function that
takes an Int named $x$ and produces an Int whose value is equal to the input $x$). We extend
the types syntax with our refinement types syntax:
\[
\begin{tabular}{lclc}
    $\tau$ & $\ ::=\ $ & $\dots\ \vert\ \langle\emph{name}\rangle \{\tau\ \vert\ P\}$ \\
    $P$ & $\ ::=\ $ & $P = P\ \vert\ P \neq P\ \vert\ P \vee P\ \vert\ P \wedge
    P\ \vert\ P \Rightarrow P\ \vert\ n = n\ \vert\ n \neq n$ \\
    & & $\vert\ n \leq n\ \vert\ n \geq n\ \vert\ n < n\ \vert\ n > n\ \vert\
    true\ \vert\ false\ \vert\ \langle\emph{name}\rangle$ \\
    $n$ & $\ ::=\ $ & $n * n\ \vert\ n + n\ \vert\ n - n\ \vert\
    \langle\emph{natural}\rangle\ \vert\ \langle\emph{name}\rangle$
    \\
\end{tabular}
\]
The addition of refinement types to the synthesizer doesn't
interfere with the rest of the process. We define the following right and
left rule, to synthesize or consume in synthesis a refinement type, where
$\textsc{getModel}(p)$ is a call to an SMT solver that returns a model of an
uninterpreted function that satisfies
$\forall_{\tilde{1},\tilde{2},\dots,\tilde{n}}\ h_{\tilde{1}} \Rightarrow h_{\tilde{2}} \Rightarrow \dots
\Rightarrow h_{\tilde{n}} \Rightarrow p$, where $\tilde{i}$ is the refinement type
name and $h_{\tilde{i}}$ its predicate for every refinement type in the
propositional contexts; and $\textsc{sat}(p_{\tilde{y}} \Rightarrow p_{\tilde{x}})$ is a call to an SMT solver that
determines universal satisfiability of the implication between predicates (the
left focused proposition subtypes the goal).
\begin{mathpar}
    \infer*[right=(refR)]
    { \textsc{getModel}(p) = M }
    {\Theta/\Theta';\Rho;\Gamma;\Delta \vdash M : \tilde{x}\{A\ \vert\
    p\}\Uparrow }
    \and
    \infer*[right=(refL)]
    { \textsc{sat}(p_{\tilde{y}} \Rightarrow p_{\tilde{x}}) }
    {\Theta/\Theta';\Rho;\Gamma;\Delta; x{:}\tilde{y}\{A\ \vert\ p_{\tilde{y}}\}\Downarrow\ \vdash x : \tilde{x}\{A\ \vert\ p_{\tilde{x}}\} }
\end{mathpar}


\mypara{Optimizations} To speed up the process and get a cleaner and sometimes more correct
output, we add a rule that lets us ``skip some rules'' if left focused on a
$\bang$-ed proposition, and the goal is $\bang$-ed:
\begin{mathpar}
    \infer*[right=($\bang\Downarrow$L)]
    { \Theta/\Theta';\Rho;\Gamma;\Delta; A\Downarrow\ \vdash M : C}
    {\Theta/\Theta';\Rho;\Gamma;\Delta; \bang A\Downarrow\ \vdash M : \bang C}
\end{mathpar}



\section{Architecture}\label{sec:architecture}

The \synname\ synthetiser operates as part of the pipeline that processes a
full program in the \synname\ language, and is called to generate terms marked
(by the syntax \emph{\{\{ ... \}\}}) for
synthesis. The main pipeline consists of:
\[
    \textrm{Parsing} \rightarrow \textrm{Desugaring} \rightarrow
    \textrm{Inference} \rightarrow \textrm{Synthesis} \rightarrow
    \textrm{Evaluation}
\]

The parsing module converts ADT declarations and top level functions written in
the language's syntax to a list of abstract syntax trees (ASTs) understood by the rest of the
pipeline.

The desugaring module converts the frontend AST to a core AST without
syntatic sugar and using the locally-nameless
representation~\cite{locally nameless}, in which all bound variables
are represented with Debruijn indices rather than names (i.e.~using
the distance to the binding site in terms of the number of traversed
binders),
% TODO: não ficou muito mais claro :\todo{Explica o que e isto}
This way inference can be done without worrying about name conflicts.

The inference module will traverse the list of ASTs in order (top-down), and
infer the type of all functions defined by the user, checking the inferred
type against a possible user given type annotation. Whenever a synth mark is
found in the AST, all the inferrence context up to that point is added to the
mark, and if not explicit, its type is inferred, essentially defining the synth
context and synth goal.

The synthesis module iterates over the synthesis marks and runs the synthetiser
with the respective context and goal for each mark. The resulting expression is checked
against the synthesis constraints defined through some keywords, continuing
synthesis if they are violated: ``using'' guarantees that certain functions are
used in the program body, and ``assert'' evaluates the given assertion within the
synthesized program to a validating boolean.  Additionally, the keyword
``depth'' controls the \emph{existential depth} (seen
in~\ref{sec:formal_system}), and ``choose'' selects a different result as long
as one is available. Finally, the expression is minimized (i.e.~$\eta$-reduced),
and the mark in the frontend AST is replaced with the synthesized program. 

The evaluation module evaluates the function named ``main'' within the
complete synthesized program, returning a final value.




\subsection{Implementation}

The implementation was carried out in Haskell. We
highlight some key points.

\mypara{Backtracking} Even though focusing reduces non-determinism
during proof search, it cannot fully eliminate it. At several points
during proof construction we must choose one of multiple rules to
apply, and in the event of not being able to construct the proof
following that choice, we want to backtrack to the decision point and
attempt to derive a proof taking a different route. To this effect,
our prototype synthetiser depends on \emph{LogicT}, "a backtracking
logic-programming monad"~\cite{logict}. Special attention is
required for ADTs. When deconstructing an ADT, since all branches must be
synthetisable and result in the same ouput linear context $\Delta'$, alternative
constructor deconstructions should be considered fairly in conjunction, so the
\textbf{fair conjunction} operator should be used. When constructing an ADT, the
disjunctive choice of constructor should be fair, and so the \textbf{fair
disjunction} operator used. Fairness at these two points usually increases
the synthesis speed slightly, but can be a relevant factor in synthesis in which a
combination of choices following the ADT deconstruction or construction can
backtrack many times or even arbitrarily.

\mypara{SMT Solving} To typecheck and synthesize refinement types and
dependent functions, we interface with an SMT solver to check
satisfiability. For this, we use the library
\emph{SBV}~\cite{sbv}. Unfortunately, this library does not support
the usage of SMT solvers to check first-order logic with uninterpreted
functions. Since this is needed for synthesis with refinement types
when right focused, we instead use unsafe library primitives to
communicate with the solver directly, constructing the logical
formulas directly. For satisfying formulas, for instance, when
typechecking, or left focused on a refinement type, we use the library
API as an oracle.

\mypara{Memoization} Some proofs attempt to synthetise the same
sub-goal with the same premises more than once. Since these proof
attempts might be expensive performance-wise, and we can rely on the
program's determinism (equal calls to synthetise result in equal
outcomes), we added memoization to our synthesizer -- an optimization
technique that stores function calls and returns cached values for
equal inputs.
%
For our synthesizer, two calls are equivalent when
contexts $\Theta, \Rho, \Gamma, \Delta$, $\Omega$, (or alternatively
the left focus), and the goal, are equivalent. Because comparing all
of these is slow, our implementation hashes the relevant values via a hashing
library (\emph{Hashable}~\cite{https://hackage.haskell.org/package/hashable}) and
uses the result as the key.
%
Furthermore, a branch we want to memoize
might fail, not providing any result -- we also want to record
synthesis failure, as it happens more often than successful
results. We capture failure of a LogicT computation using the $\mathit{ifte}$
function and represent failure or success information with the \texttt{Maybe}
type. We ran benchmarks to measure the impact of memoization on 
performance. It's quite noticeable in synthesis with multiple
functions in the unrestricted context, or more complex
specifications. For example, if we add a new function \emph{read} to
the unrestricted context in the \emph{array} problem seen in
Section~\ref{sec:overview}, synthesis with memoization will take
around $5s$, while synthesis without memoization doesn't terminate
even after $15m$ running.
%
The implementation could be improved by better combining the
memoization with the LogicT monad transformer (remembering decisions across
backtracking branches) and by finetuning key-equality by ignoring context
components that aren't relevant to the synthesis call, to name some
examples. We leave such optimizations to future work.



\mypara{Debugging} The rules described in the formal
system\ref{sec:formal_system} to handle infinite recursion might seem obvious in
retrospective, however, when faced with a program that doesn't terminate, what
went wrong is not so clear. While we were able to figure out a solution for the
first couple of "infinite examples", the following problems were much harder,
taking sometimes full days to solve. To make the debugging experience better, we
developed a tracing system that prints information whenever a rule is applied,
alongside the stack of rules applied up until that one. This improved our
debugging speed considerably, especially when put together with a correct
derivation done by hand, with which we could match what was happening against
what was supposed to happen.


\mypara{Interface} A simple, syntax-highlighted, web interface was developed
alongside a server to make possible experimenting with the \synname\ 
language without having to download and compile the complete toolchain. The web
interface is available from the repository~\cite{github repo}.

% A implementação foi feita em haskell e o código está disponível em X e há um
% live demo com exemplos pré-feitos em Y?




\section{Related Work}

Type-based program synthesis is a vast field of study and so it follows that a
lot of literature is available to inspire and complement our work. Most
works~\cite{DBLP:conf/lopstr/HughesO20,DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
follow some variation of the synthesis-as-proof-search approach. However, the
process is novel for each due to a variety of different rich types explored and
their corresponding logics and languages; or nuances of the synthesis process
itself, such as complementing types with program examples; or even the
programming paradigm of the output produced (e.g. generating heap manipulating
programs~\cite{DBLP:journals/pacmpl/PolikarpovaS19}).  Some other common
patterns we follow are the use of type
refinements~\cite{DBLP:conf/pldi/PolikarpovaKS16}, polymorphic types, and the
support for synthesis of recursive
functions~\cite{DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15}. These
works share some common ground with each other and with us -- most
famously the Curry-Howard correspondence, but also, for example, 
focusing-based proof-search~\cite{10.1093/logcom/2.3.297}.
% TODO: sinto que precisava de um toque...

% \mypara{Type-and-Example-Directed Program Synthesis} This
% work~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
% explores a purely functional, recursive program synthesis technique
% based on types/proof search (as we intend to do), with examples as an
% auxiliar technique to trim down the program synthesis vast search
% space. A good use is
% made of the general strategy of using a type system to generate
% programs (by ``inverting'' the type system), rather than using it to
% type-check one.  Moreover, the paper uses a data structure called
% ``refinement tree'' in conjunction with the extra information provided
% by the examples to allow for efficient synthesis of non-trivial
% programs. Overall it employs good engineering to achieve type-based
% synthesis of functional typed programs, with at least two additions
% that we might want to follow too (type refinements and recursive
% functions).

% or how to turn your type system upside down ~ não apagar a Kubrick reference? :)
  
\mypara{Program Synthesis from Polymorphic Refinement Types} The
work~\cite{DBLP:conf/pldi/PolikarpovaKS16} also studies synthesis of recursive
functional programs in an ``advanced'' context. Their specifications combine two
rich forms of types: polymorphic and refinement types (which correspond to a
first-order logic through the Curry-Howard isomorphism). Their approach to
refinement types consists of a new algorithm that supports decomposition of the
refinement specification, allowing for separation between the language of
specification and programs and making the approach amenable to compositional
synthesis techniques. We also support polymorphism and refinements, however, our
refinements are somewhat underdeveloped and don't have an important role in our
process, in contrast to their strong use of refinements to guide their
synthesizer. Instead, our synthesizer leverages the expressiveness of linear
types and techniques for proof-search in linear logic to guide its process.

\mypara{Resourceful Program Synthesis from Graded Linear Types} The
work~\cite{DBLP:conf/lopstr/HughesO20} synthetises programs using an
approach very similar to our own.  The work employs so-called graded
modal types, which is a refinement of pure linear types -- a more
\emph{fine-grained} version, since it allows for quantitative
specification of resource usage, in contrast to ours either
\emph{linear} or \emph{unrestricted} (via the linear logic
exponential) use of assumptions.  Their resource management is more
complex, and so they provide solutions which adapt Hodas and Miller's
resource management
approach~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:journals/tcs/LiangM09}
-- which, in contrast, is the model use in our work.

They also use focusing as a solution to trim down search space and to
ensure that synthesis only produces well-typed programs. However, since their
underlying logic is \emph{modal} rather than purely \emph{linear}, it
lacks a clear correspondence with concurrent session-typed
programs~\cite{DBLP:journals/mscs/CairesPT16,DBLP:conf/concur/CairesP10},
which is a crucial avenue of future work. Moreover, their use grading
effectively requires constraint solving to be integrated with the
synthesis procedure, which can limit the effectiveness of the overall
approach as one scales to more sophisticated settings (e.g.~refinement
types).

% \item Resource-guided synthesis \cite{DBLP:conf/pldi/KnothWP019}
% This resource-guided means something a bit different. Programs satisfy
% a functional specification and a symbolic resource bound in the sense
% of amortized analysis, but can provide technical insights.

%\section{Goals and Work Plan}

%We start with the definition of our type system and simply-typed
%functional language with linear function, sum and product types
%($\lolli$, $\tensor$, $\one$, $\with$, $\oplus$, $\bang$).  Through
%the Curry-Howard correspondence, we'll formulate the synthesis of
%typed programs in our functional language as bottom-up proof-search in
%linear logic.

%Since proof rules in \emph{natural deduction} do not entail a
%proof-search strategy, we will use the equivalent \emph{sequent
%calculus} system to formulate our typing rules in a way that is
%amenable to proof search.
%%
%In sequent calculus, rules can be naturally understood in a
%\emph{bottom-up} manner, simplifying the reasoning for the
%synthesizer. Moreover, the linear nature of our system requires a way
%to algorithmically reason about resource contexts: for instance, if we
%read rule $\tensor I$ from Section~\ref{sec:background} bottom-up, we
%must non-deterministically guess how to correctly split ambient
%resources into $\Delta_1$ and $\Delta_2$ to separately prove $A$ and
%$B$. The resource-management approach we'll adopt to solve this has been 
%previously explained in more detail.

%With the use of input/output contexts, and the sequent calculus
%formulation of the inference rules, we draw closer to an effective
%strategy for bottom-up proof search and so of program synthesis.
%%
%%
%However, the search process still encompasses significant
%non-determinism. To address this we draw on the technique of
%focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05} as
%discussed above.

%With the basic framework in place, we will consider additional
%(simply-typed) language constructs and synthesis of program fragments
%or \emph{partial synthesis} -- given a program with a typed hole,
%synthesize a term to fill out the hole accordingly. We will
%subsequently also consider synthesis of recursive functions and,
%might, later on, add refinement types also as a way to constrain the
%valid programs space.

%Putting together everything above, and the more detailed information
%in the background section -- linear types, sequent calculus, resource
%management, and focusing, we have a set of inference rules (written
%out in the appendix) that define how the synthesis process will
%progress.

%This is the theory we're interested in and have studied so far,
%which will allow us to build the actual synthetizer. From here,
%our work plan gives an overview of the steps we'll take.

%\subsection{Work Plan}

%The end goal is to have a synthetizer capable of
%taking a type (possibly refined) as input, and to produce (possibly
%interactively) one or more functions with that type, if any exist.

%As mentioned above, we will begin with (both partial and full)
%synthesis for the linear $\lambda$-calculus. We will then extend the
%language with additional type constructs (e.g. base types, records,
%algebraic data types) and consider synthesis in that setting. Finally,
%we will consider synthesis of recursive functions, and,
%time-permitting, we will consider simple forms of type refinements
%(e.g. arithmetic refinements).

%We've already implemented a parser and a type-checker for a linear
%$\lambda$-calculus using the resource management technique described
%in the previous section -- more than
%good preparation for the development of the synthesizer, they will be
%useful to \emph{validate} the synthesized functions, and to enable
%the synthesis of partial programs.
%%
%The implementations have been carried out in Haskell since it's easier
%to model and implement inference rules in a functional setting.

%\mypara{Step by step} Our work so far has been constantly accompanied
%by the study of somewhat dense theoretical material, state of the art,
%which we must understand to overcome challenges in these fields, in
%order to develop a more scalable synthetizer -- we want to display
%concrete results to prove the feasability of program synthesis from
%linear types, and thus open the possibility for future application to
%related fields, such as program synthesis based on session types for
%message-passing concurrency.

%Apart from the theory, when we started studying type systems, we
%created a simple type-checker for the simply-typed $\lambda$-calculus.
%While learning about linear type systems, and the simply-typed linear
%$\lambda$-calculus, we developed a parser, a desugaring module, and an
%evaluation module to write, analyze and execute simple programs in the
%language; culminating in writing a type-checker for the linear
%$\lambda$-calculus. At this point, we can parse, typecheck, and
%execute this small linear functional language.  The next step is to
%start developing the actual synthetizer, embedding it with all the
%techniques we've seen. Following work will be done to augment
%usability, expand the language domain, and add features such as those
%mentioned above.

% \mypara{Evaluation} Validation and evaluation of our work is fairly
% easy. Simple questions such as \emph{does it typecheck}? \emph{can
%   we synthesize}? \emph{how fast}?  are the core of our evaluation. In
% more concrete terms, we can compare our times to state of the art
% synthetizers, present a list of working examples of programs synthesized
% by our framework, and corresponding benchmarks.

% On the eventual addition of recursive functions, refinement types, and
% interactive synthesis, we'll consider, respectively, other examples to
% showcase recursive functions synthesis, updated times for benchmarks
% with refinement types, and a display of the possible user interaction
% in the user process, and the resulting function following it.


% Expand on earlier points. Start from a functional language with linear types
% ($\tensor$, $\lolli$, $\oplus$, $\bang$); build on it with more ``stuff''.
% General techniques, drawn from proof theory via props as types: explore
% focusing to tame non-determinism/search space. Partial synthesis is still
% proof search! Add other techniques as we go along (e.g. for recursion we need
% to constrain recursive calls).

% Mention that you've already implemented a type-checker for this (useful as a
% prelim. Exercise but also later, for \emph{validation}).

% Validation and evaluation: validation is as simple as ``does it typecheck''?
% can you synth? how fast?


\section{Evaluation}\label{sec:evaluation}

The benchmarks are separated into groups: theorems of linear logic, recursive
ADTs, refinements. For each goal we analyze if the synthesis process terminated
according to what was expected, the mean time and standard deviation, if the
resulting program fullfills its intended purpose

\begin{center}
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Group & Goal & Avg. time + $\sigma$ & Found & Is expected \\
        \hline
        \multirow{4}{4em}{Linear Logic Theor.} & uncurry & 1s & yes & yes \\ 
        & curry & 2s & yes & yes \\ 
        & call by name?? & 2s & yes & yes \\ 
        & 0/1?? & 2s & yes & yes \\ 
        \hline
        \multirow{8}{4em}{List} & singleton & 3s & yes & yes \\ 
        & insert & 2s & yes & unordered \\ 
        & map & 1s & yes & yes \\ 
        & append & 2s & yes & yes \\ 
        & foldl & 2s & yes & got (foldl . reverse) \\ 
        & foldr & 2s & yes & yes \\ 
        & concat & $\infty$ & no & no \\ 
        & uncons & 1s & yes & yes \\
        \hline
        \multirow{3}{4em}{Maybe} & return & 1s & yes & yes \\ 
        & $>>=$ & 1s & yes & yes \\
        & maybe & 1s & yes & yes \\
        & list to maybe & 12s & no & yes \\
        \hline
        \multirow{5}{4em}{Binary Tree} & singleton & 1s & yes & yes \\ 
        & insert & 1s & yes & unordered \\
        & map & 1s & yes & yes \\
        & append & 1s & yes & appends to rightmost leaf \\
        & join node & 1s & yes & joins at rightmost leaf \\
        \hline
        \multirow{7}{4em}{Map} & singleton & 1s & yes & yes \\ 
        & insert & 1s & yes & unordered \\
        & insertWithKey & 1s & yes & ignores function \\
        & union & 1s & yes & ignores keys \\
        & mapAccum & 1s & no & ?? \\
        & map & 1s & yes & yes \\
        & mapKeys & 1s & yes & yes \\
        \hline
        \multirow{1}{4em}{Misc} & either & 1s & yes & yes \\ 
        \hline
    \end{tabular}
\end{center}


Synthesis of 





\bibliographystyle{splncs04}
\bibliography{references}


\appendix

\section{Background}\label{sec:background}

\mypara{Type Systems} A type system can be formally described through
a set of inference rules that inductively define a judgment of the
form $\Gamma \vdash M : A$, stating that program expression $M$ has
type $A$ according to the \emph{typing assumptions} for variables
tracked in $\Gamma$. For instance,
$x{:}\mathsf{Int}, y{:}\mathsf{Int} \vdash x+y : \mathsf{Int}$ states
that $x+y$ has type $\mathsf{Int}$ under the assumption that $x$ and
$y$ have type $\mathsf{Int}$.  An expression $M$ is deemed well-typed
with a given type $A$ if one can construct a typing derivation with $M :
A$ as its conclusion, by repeated application of the inference rules.

The simply-typed $\lambda$-calculus is a typed core functional
language~\cite{10.5555/509043} that captures the essence of a type system in a simple and familiar environment. Its syntax consists of
functional abstraction, written $\lambda x{:}A.M$, denoting
an (anonymous) function that takes an argument of type $A$, bound to
$x$ in $M$; and application $M\,N$, with the standard meaning, and
variables $x$. For instance, the term $\lambda x{:}A.x$, denoting the identity function, is a functional abstraction
taking an argument of type $A$ and returning it back.

\mypara{Propositions as Types}
%
It turns out that the inference rules of the simply-typed
$\lambda$-calculus are closely related to those of a system of natural
deduction for intuitionistic logic~\cite{prawitznd65}. This
relationship, known as the Curry-Howard
correspondence~(see~\cite{DBLP:journals/cacm/Wadler15} for a
historical survey),
identifies that the propositions of intuitionistic logic can be read
as types for the simply-typed $\lambda$-calculus (``propositions as
types''), their proofs are exactly the program with the given type
(``proofs as programs''), and checking a proof is type checking a
program (``proof checking as type checking'').

Inference rules in natural deduction are categorized as introduction
or elimination rules; these correspond, respectively, to rules for
constructors and destructors in programming languages (e.g. function
abstraction vs application, construction of a pair vs projection).

We introduce select typing rules to both show how a type system can be
formalized and to show the relationship with (intuitionistic)
propositional logic. The following rule captures the nature of a
hypothetical judgment, allowing for reasoning from assumptions:
\[
    \infer*[right=(var)]
    {  }
    {\Gamma, u: A \vdash u : A}
\]
When seen as a typing rule for the $\lambda$-calculus, it
corresponds to the rule for typing variables -- variable $u$ has
type $A$ if the typing environment contains a variable $u$ of
type $A$.

As another concrete example, let us consider the rule for
implication ($A\rightarrow~B$):
\[
    \infer*[right=($\rightarrow I$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \rightarrow B}
\]
Logically, the rule states that to prove $A\rightarrow B$, we assume $A$
and prove $B$. Through the Curry-Howard correspondence, implication
corresponds to the function type -- the program $\lambda u. M$ has
type $A \rightarrow B$, provided $M$ has type
$B$ under the assumption that $u$ is a variable of type $A$. The rule
shown above is an \emph{introduction} rule, that introduces the
``implication'' connective.
 
Finally, let us consider an \emph{elimination} rule, also for implication:
\[
    \infer*[right=($\rightarrow E$)]
    {\Gamma \vdash M : A \rightarrow B \and \Gamma \vdash N : A}
    {\Gamma \vdash M\,N : B}
\]
Elimination rules are easier to think of in a top-down
manner. Logically, this rule states that if we prove $A \rightarrow B$
and $A$ we can prove $B$. Through the Curry-Howard correspondence,
implication elimination corresponds to function application, and its
type is the function return type -- the application $M N$ has type
$B$, provided $M$ has type $A \rightarrow B$ and $N$ has type $A$.

The Curry-Howard correspondence generalizes beyond the simply-typed
$\lambda$-calculus and propositional intuitionistic logic. It extends
to the realms of polymorphism (second-order
logic), dependent types (first-order
logic), and various other extensions of natural deduction and
simply-typed $\lambda$-calculus -- which include linear logic and
linear types.


\mypara{Linear Logic}

Linear logic \cite{DBLP:journals/tcs/Girard87} can be seen as a
resource-aware logic, where propositions are interpreted as resources
that are consumed during the inference process.  Where in standard
propositional logic we are able to use an assumption as many times as
we want, in linear logic every resource (i.e., every assumption) must
be used \emph{exactly once}, or \emph{linearly}. This usage
restriction gives rise to new logical connectives, based on the way
the ambient resources are used. For instance, conjunction, usually
written as $A\wedge B$, appears in two forms in linear logic:
multiplicative or simultaneous conjunction (written $A\tensor B$); and
additive or alternative conjunction (written $A\with
B$). Multiplicative conjunction denotes the simultaneous availability
of resources $A$ and $B$, requiring both of them to be
used. Alternative conjunction denotes the availability of $A$ and $B$,
but where only one of the two resources may be used. Similarly,
implication becomes linear implication, written $A\lolli B$,
denoting a resource that will consume (exactly one) resource $A$ to
produce a resource $B$.

To present the formalization of this logic, besides the new
connectives, we need to introduce the \emph{resource-aware context}
$\Delta$.  In contrast to the previously seen $\Gamma$, $\Delta$ is
also a list of variables and their types, but where each and every
variable must be used exactly once during inference.  So, to introduce
the connective $\tensor$ which defines a multiplicative pair of
propositions, we must use exactly all the resources
($\Delta_1, \Delta_2$) needed to realize the ($\Delta_1$) 
proposition $A$, and ($\Delta_2$) proposition $B$:
\[
    \infer*[right=($\tensor I$)]
    {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
    {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
\]
Out of the logical connectives, we need to mention one more, since it
augments the form of the judgment and it's the one that ensures
logical strength i.e. we're able to translate intuitionistic logic
into linear logic.  The proposition $\bang A$ (read \emph{of course}
$A$) is used (under certain conditions) to make a resource
``infinite'' i.e. to make it useable an arbitrary number of times. To
distinguish the ``infinite'' variables, a separate, unrestricted,
context is used -- $\Gamma$. So $\Gamma$ holds the ``infinite''
resources, and $\Delta$ the resources that can only be used once.  The
linear typing judgment for the introduction of the exponential $\bang A$
takes the form: 
\[
    \infer*[right=($\bang I$)]
    {\Gamma ; \emptyset \vdash M : A}
    {\Gamma ; \emptyset \vdash \bang M : \bang A}
\]
Logically, a proof of $\bang A$ cannot use linear resources since
$\bang A$ denotes an unbounded (potentially $0$) number of copies of
$A$. Proofs of $\bang A$ may use other unrestricted or exponential
resources, tracked by context $\Gamma$.
From a computational perspective, the type $\bang A$
internalizes the simply-typed $\lambda$-calculus in the linear
$\lambda$-calculus.

The elimination form for the exponential, written $\llet{ !u
= M}{ N}$, warrants the use of resource $A$ an unbounded
number of times in $N$ via the variable $u$:  
\[
    \infer*[right=($\bang E$)]
    {\Gamma ; \Delta_1 \vdash M : \bang A \and \Gamma, u{:}A ; \Delta_2 \vdash N : C}
    {\Gamma ; \Delta_1, \Delta_2 \vdash \llet{ !u = M}{ N} : C}
\]
Again, through the Curry-Howard correspondence, we can view the process
of finding a proof of a proposition in linear logic as the process of synthesizing a
linear functional program of the given type.


\mypara{Sequent Calculus} Inference rules in natural deduction (the
ones we have considered so far) are ill-suited for bottom-up
proof-search since elimination rules work top-down and introduction
rules work bottom-up.  A more suited candidate is the equivalent
\emph{sequent calculus} system in which \emph{all} inference rules can
be understood naturally in a \emph{bottom-up} manner.  The
introduction and elimination rules from natural deduction disappear,
and their place is taken by right and left rules, respectively.
%
Right rules correspond exactly to the introduction rules of natural
deduction, which were already understood bottom-up. The inference rule
for implication introduction ($\supset$ is used instead of
$\rightarrow$), seen above in natural deduction under the
\emph{propositions as types} subsection, corresponds to the following
right rule in sequent calculus:
\[
    \infer*[right=($\supset R$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \supset B}
  \]

Intuitively, left rules act as the elimination rules of natural deduction, but are altered
to work bottom-up, instead of top-down.
The inference rule for the also seen above implication elimination is
as follows:
\[
    \infer*[right=($\supset L$)]
    {\Gamma , u:B\vdash M : C \and \Gamma, u{:}A\supset B \vdash N : A}
    {\Gamma, u{:}A\supset B\vdash M\{(u\,N)/u\} : C}
\]
As implied by their name, left rules define how to decompose or make
use of a connective on the left of the turnstile $\vdash$. To use an
assumption of $A\supset B$ while attempting to show some proposition
$C$ we produce a proof of $A$, which allows us to use an assumption
of $B$ to prove $C$. In terms of the corresponding $\lambda$-terms,
the $\supset L$ rule corresponds to applying the variable $u$ to the
argument $N$ but potentially deep in the structure of $M$.

We'll define our inference rules for synthesis under the sequent
calculus system, for it simplifies the work to be done in the
synthetiser.


\mypara{Resource-Management}
Be it from the perspective of proof \emph{checking} (i.e.~type checking) or
proof \emph{search} linear logic poses a key challenge when compared
to the non-linear setting:
When constructing a derivation (bottom-up), we are seemingly forced to
guess how to correctly split the linear context such that the
sub-derivations have access to the correct resources (e.g.~the
$\tensor I$ rule above).
%
%

To solve this issue we will adopt the resource-management
approach
of~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:conf/lics/LiangM09},
which generalizes the judgment  from $\Delta \vdash M : A$ to
$\Delta_I \ \Delta_O \vdash M : A$, where $\Delta_I$ is an input
context and $\Delta_O$ is an output context.
Instead of requiring non-deterministic guesses of resource splits
during proof search, we track which resources are used and which are
remaining via the two contexts, leading to the following general strategy: to
prove $A\tensor B$ using (input) resources $\Delta$, prove $A$ with
input context $\Delta$,
consuming some subset of $\Delta$, and produce as output the leftover
resources $\Delta'$; prove $B$ using $\Delta'$ as its input context and then output the
remaining resources $\Delta''$; finally, after having proven
$A\tensor B$, output $\Delta''$, for subsequent derivations.


\mypara{Focusing}

Even with everything mentioned so far, non-determinism is still very
present in proof-search, e.g. at any given point, many proof
rules are applicable in general. The technique of focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05}
has been previously studied as a way to discipline proof search in
linear logic -- a method created to trim down the search space of
valid proofs in linear logic, by eagerly applying invertible rules
(i.e.~rules whose conclusion implies the premises), and then by
``focusing'' on a single connective when no more direct (invertible)
rules can be applied, that is, only applying rules that breakdown
the connective under focus or its subformulas. If the search is not
successful, the procedure backtracks and another connective is
chosen as the focus.

Focusing eliminates all of the ``don't care'' non-determinism from
proof search, since the order in which invertible rules are applied
does not affect the outcome of the search, leaving only the
non-determinism that pertains to unknowns (or ``don't know''
non-determinism), identifying precisely the points at which
backtracking is necessary.

\end{document}
