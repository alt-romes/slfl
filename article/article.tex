\documentclass{llncs}

\usepackage{cmll}
\usepackage{mathpartir}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}

\newcommand{\lolli}{\multimap}
\newcommand{\tensor}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bang}{{!}}
\newcommand{\mypara}[1]{\paragraph{\textbf{#1}.}}

\newcommand{\llet}[2]{\mathsf{let}\,#1\,\mathsf{in}\,#2}

\newcommand{\synname}{\emph{SILI}}

\title{Synthesis of Linear Functional Programs}
\author{Rodrigo Mesquita \and Bernardo Toninho}
\date{April 2021}
\institute{NOVA School of Science and Technology}

\begin{document}

\maketitle

\section{Introduction}

% General motivation?
Program synthesis is an automated or semi-automated process of
deriving a program (i.e.~generating code) from a high-level
specification.  
%
Synthesis can be seen as a means to improve a programmer's
productivity and/or program correctness (i.e. through suggestion
and/or autocompletion), or as a tool to automate certain parts of the
programming process (e.g. in the same way AI might generate some
boilerplate text to aid a journalist, rather than to replace the
journalist outright), just to name a few examples.

Specifications can take many forms (e.g.  polymorphic refinement
types~\cite{DBLP:conf/pldi/PolikarpovaKS16},
examples~\cite{DBLP:conf/popl/FrankleOWZ16}, graded linear
types~\cite{DBLP:conf/lopstr/HughesO20}).  Regardless of the kind
of specification, program synthesizers have to deal with two main
inherent sources of complexity: searching over a vast space of
(potentially) valid programs, and interpreting user intent.
%
In this work we will explore type-based synthesis where specifications
take the form of linear types, which are types that limit resource
usage in programs.  Concretely, we will explore the proof-theoretic
foundations of linear types via the Curry-Howard correspondence with
linear logic, viewing program synthesis under the lens of (linear
logic) bottom-up proof search.

%

\mypara{Type-Based Synthesis}
% Type based synthesis
Type-based synthesis uses types as a form of program specification,
automatically or semi-automatically producing a program expression
with the given type and so matching the specification.
%
In order to make type-based specifications more expressive, type-based
synthesis frameworks use rich type systems such as refinement
types~\cite{DBLP:conf/pldi/PolikarpovaKS16} and polymorphic types~\cite{DBLP:conf/pldi/PolikarpovaKS16}.

Richer type systems allow for more precise types, which can
statically eliminate various kinds of logical errors by making certain
invalid program states ill-typed (e.g.,~a ``null aware'' type system,
will ensure at compile-time that you cannot dereference a
null-pointer). However, they can also be a burden -- the whole point
of these type systems is to ensure that \emph{fewer} programs are
deemed well-typed, which can pose additional challenges to the
development process. Type-based synthesis leverages rich types as a
way of pruning the search space, and by using types the user 
is given a more ``familiar'' specification. For instance, the type
$\mathsf{Int} \rightarrow \mathsf{Int} \rightarrow \mathsf{Int}$
specifies a (curried) function that takes two integers and produces an
integer. Viewed as a specification, it is extremely imprecise (there
are an infinite number of functions that satisfy this specification).
However, the richer type $(x{:}\mathsf{Int}) \rightarrow
(y{:}\mathsf{Int}) \rightarrow \{z{:}\mathsf{Int} \mid z = x+y\}$
very precisely specifies a function that takes integer arguments $x$
and $y$ and returns an integer $z$ that is the sum of $x$ and $y$. 

%
This work explores the synthesis of linear functional
programs from types based on linear logic propositions (i.e.~linear
types) by leveraging the Curry-Howard correspondence. It
states that propositions in a logic have a direct
mapping to types, and well-typed programs correspond to proofs of
those propositions.  As such, having a type be a proposition in linear
logic, we can relate a proof of it directly to a linear
functional program — finding a proof is finding a program with that
type. Formulating synthesis as proof search in linear logic
allows us to inform our work with approaches from the relevant 
literature.

Linear types differ from more traditional types in
that they constrain resource usage in programs by \emph{statically}
limiting the number of times certain resources can be used during
their lifetime.  They can be applied to resource-aware programming
such as concurrent programming (e.g. session types for message passing
concurrency~\cite{DBLP:journals/mscs/CairesPT16}), and to memory-management (e.g.~Rust's ownership
types).

%\mypara{Goals}
% % Goals
%In the end, we intend to be able to do full and partial synthesis of
%well-typed programs. Full synthesis consists of the production of a
%function (or set of) satisfying the specification; partial synthesis
%is the ``completion'' of a partial program (i.e. a function with a
%\emph{hole} in it). The work will start from a small core linear --
%i.e. \emph{resource-aware} functional language, building up to
%recursive types/functions, with potential other avenues of further
%extension. 
%%
%We will evaluate the work through
%expressiveness benchmarks (i.e. can we synthesize a term of type $T$?) and time
%measurements (i.e. how fast can we synthesize a term of type $T$?').


\section{Background}\label{sec:background}

\mypara{Type Systems} A type system can be formally described through
a set of inference rules that inductively define a judgment of the
form $\Gamma \vdash M : A$, stating that program expression $M$ has
type $A$ according to the \emph{typing assumptions} for variables
tracked in $\Gamma$. For instance,
$x{:}\mathsf{Int}, y{:}\mathsf{Int} \vdash x+y : \mathsf{Int}$ states
that $x+y$ has type $\mathsf{Int}$ under the assumption that $x$ and
$y$ have type $\mathsf{Int}$.  An expression $M$ is deemed well-typed
with a given type $A$ if one can construct a typing derivation with $M :
A$ as its conclusion, by repeated application of the inference rules.

The simply-typed $\lambda$-calculus is a typed core functional
language~\cite{10.5555/509043} that captures the essence of a type system in a simple and familiar environment. Its syntax consists of
functional abstraction, written $\lambda x{:}A.M$, denoting
an (anonymous) function that takes an argument of type $A$, bound to
$x$ in $M$; and application $M\,N$, with the standard meaning, and
variables $x$. For instance, the term $\lambda x{:}A.x$, denoting the identity function, is a functional abstraction
taking an argument of type $A$ and returning it back.

\mypara{Propositions as Types}
%
It turns out that the inference rules of the simply-typed
$\lambda$-calculus are closely related to those of a system of natural
deduction for intuitionistic logic~\cite{prawitznd65}. This
relationship, known as the Curry-Howard
correspondence~(see~\cite{DBLP:journals/cacm/Wadler15} for a
historical survey),
identifies that the propositions of intuitionistic logic can be read
as types for the simply-typed $\lambda$-calculus (``propositions as
types''), their proofs are exactly the program with the given type
(``proofs as programs''), and checking a proof is type checking a
program (``proof checking as type checking'').

Inference rules in natural deduction are categorized as introduction
or elimination rules; these correspond, respectively, to rules for
constructors and destructors in programming languages (e.g. function
abstraction vs application, construction of a pair vs projection).

We introduce select typing rules to both show how a type system can be
formalized and to show the relationship with (intuitionistic)
propositional logic. The following rule captures the nature of a
hypothetical judgment, allowing for reasoning from assumptions:
\[
    \infer*[right=(var)]
    {  }
    {\Gamma, u: A \vdash u : A}
\]
When seen as a typing rule for the $\lambda$-calculus, it
corresponds to the rule for typing variables -- variable $u$ has
type $A$ if the typing environment contains a variable $u$ of
type $A$.

As another concrete example, let us consider the rule for
implication ($A\rightarrow~B$):
\[
    \infer*[right=($\rightarrow I$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \rightarrow B}
\]
Logically, the rule states that to prove $A\rightarrow B$, we assume $A$
and prove $B$. Through the Curry-Howard correspondence, implication
corresponds to the function type -- the program $\lambda u. M$ has
type $A \rightarrow B$, provided $M$ has type
$B$ under the assumption that $u$ is a variable of type $A$. The rule
shown above is an \emph{introduction} rule, that introduces the
``implication'' connective.
 
Finally, let us consider an \emph{elimination} rule, also for implication:
\[
    \infer*[right=($\rightarrow E$)]
    {\Gamma \vdash M : A \rightarrow B \and \Gamma \vdash N : A}
    {\Gamma \vdash M\,N : B}
\]
Elimination rules are easier to think of in a top-down
manner. Logically, this rule states that if we prove $A \rightarrow B$
and $A$ we can prove $B$. Through the Curry-Howard correspondence,
implication elimination corresponds to function application, and its
type is the function return type -- the application $M N$ has type
$B$, provided $M$ has type $A \rightarrow B$ and $N$ has type $A$.

The Curry-Howard correspondence generalizes beyond the simply-typed
$\lambda$-calculus and propositional intuitionistic logic. It extends
to the realms of polymorphism (second-order
logic), dependent types (first-order
logic), and various other extensions of natural deduction and
simply-typed $\lambda$-calculus -- which include linear logic and
linear types.


\mypara{Linear Logic}

Linear logic \cite{DBLP:journals/tcs/Girard87} can be seen as a
resource-aware logic, where propositions are interpreted as resources
that are consumed during the inference process.  Where in standard
propositional logic we are able to use an assumption as many times as
we want, in linear logic every resource (i.e., every assumption) must
be used \emph{exactly once}, or \emph{linearly}. This usage
restriction gives rise to new logical connectives, based on the way
the ambient resources are used. For instance, conjunction, usually
written as $A\wedge B$, appears in two forms in linear logic:
multiplicative or simultaneous conjunction (written $A\tensor B$); and
additive or alternative conjunction (written $A\with
B$). Multiplicative conjunction denotes the simultaneous availability
of resources $A$ and $B$, requiring both of them to be
used. Alternative conjunction denotes the availability of $A$ and $B$,
but where only one of the two resources may be used. Similarly,
implication becomes linear implication, written $A\lolli B$,
denoting a resource that will consume (exactly one) resource $A$ to
produce a resource $B$.

To present the formalization of this logic, besides the new
connectives, we need to introduce the \emph{resource-aware context}
$\Delta$.  In contrast to the previously seen $\Gamma$, $\Delta$ is
also a list of variables and their types, but where each and every
variable must be used exactly once during inference.  So, to introduce
the connective $\tensor$ which defines a multiplicative pair of
propositions, we must use exactly all the resources
($\Delta_1, \Delta_2$) needed to realize the ($\Delta_1$) 
proposition $A$, and ($\Delta_2$) proposition $B$:
\[
    \infer*[right=($\tensor I$)]
    {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
    {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
\]
Out of the logical connectives, we need to mention one more, since it
augments the form of the judgment and it's the one that ensures
logical strength i.e. we're able to translate intuitionistic logic
into linear logic.  The proposition $\bang A$ (read \emph{of course}
$A$) is used (under certain conditions) to make a resource
``infinite'' i.e. to make it useable an arbitrary number of times. To
distinguish the ``infinite'' variables, a separate, unrestricted,
context is used -- $\Gamma$. So $\Gamma$ holds the ``infinite''
resources, and $\Delta$ the resources that can only be used once.  The
linear typing judgment for the introduction of the exponential $\bang A$
takes the form: 
\[
    \infer*[right=($\bang I$)]
    {\Gamma ; \emptyset \vdash M : A}
    {\Gamma ; \emptyset \vdash \bang M : \bang A}
\]
Logically, a proof of $\bang A$ cannot use linear resources since
$\bang A$ denotes an unbounded (potentially $0$) number of copies of
$A$. Proofs of $\bang A$ may use other unrestricted or exponential
resources, tracked by context $\Gamma$.
From a computational perspective, the type $\bang A$
internalizes the simply-typed $\lambda$-calculus in the linear
$\lambda$-calculus.

The elimination form for the exponential, written $\llet{ !u
= M}{ N}$, warrants the use of resource $A$ an unbounded
number of times in $N$ via the variable $u$:  
\[
    \infer*[right=($\bang E$)]
    {\Gamma ; \Delta_1 \vdash M : \bang A \and \Gamma, u{:}A ; \Delta_2 \vdash N : C}
    {\Gamma ; \Delta_1, \Delta_2 \vdash \llet{ !u = M}{ N} : C}
\]
Again, through the Curry-Howard correspondence, we can view the process
of finding a proof of a proposition in linear logic as the process of synthesizing a
linear functional program of the given type.


\mypara{Sequent Calculus} Inference rules in natural deduction (the
ones we have considered so far) are ill-suited for bottom-up
proof-search since elimination rules work top-down and introduction
rules work bottom-up.  A more suited candidate is the equivalent
\emph{sequent calculus} system in which \emph{all} inference rules can
be understood naturally in a \emph{bottom-up} manner.  The
introduction and elimination rules from natural deduction disappear,
and their place is taken by right and left rules, respectively.
%
Right rules correspond exactly to the introduction rules of natural
deduction, which were already understood bottom-up. The inference rule
for implication introduction ($\supset$ is used instead of
$\rightarrow$), seen above in natural deduction under the
\emph{propositions as types} subsection, corresponds to the following
right rule in sequent calculus:
\[
    \infer*[right=($\supset R$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \supset B}
  \]

Intuitively, left rules act as the elimination rules of natural deduction, but are altered
to work bottom-up, instead of top-down.
The inference rule for the also seen above implication elimination is
as follows:
\[
    \infer*[right=($\supset L$)]
    {\Gamma , u:B\vdash M : C \and \Gamma, u{:}A\supset B \vdash N : A}
    {\Gamma, u{:}A\supset B\vdash M\{(u\,N)/u\} : C}
\]
As implied by their name, left rules define how to decompose or make
use of a connective on the left of the turnstile $\vdash$. To use an
assumption of $A\supset B$ while attempting to show some proposition
$C$ we produce a proof of $A$, which allows us to use an assumption
of $B$ to prove $C$. In terms of the corresponding $\lambda$-terms,
the $\supset L$ rule corresponds to applying the variable $u$ to the
argument $N$ but potentially deep in the structure of $M$.

We'll define our inference rules for synthesis under the sequent
calculus system, for it simplifies the work to be done in the
synthesizer.


\mypara{Resource-Management}
Be it from the perspective of proof \emph{checking} (i.e.~type checking) or
proof \emph{search} linear logic poses a key challenge when compared
to the non-linear setting:
When constructing a derivation (bottom-up), we are seemingly forced to
guess how to correctly split the linear context such that the
sub-derivations have access to the correct resources (e.g.~the
$\tensor I$ rule above).
%
%

To solve this issue we will adopt the resource-management
approach
of~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:conf/lics/LiangM09},
which generalizes the judgment  from $\Delta \vdash M : A$ to
$\Delta_I \ \Delta_O \vdash M : A$, where $\Delta_I$ is an input
context and $\Delta_O$ is an output context.
Instead of requiring non-deterministic guesses of resource splits
during proof search, we track which resources are used and which are
remaining via the two contexts, leading to the following general strategy: to
prove $A\tensor B$ using (input) resources $\Delta$, prove $A$ with
input context $\Delta$,
consuming some subset of $\Delta$, and produce as output the leftover
resources $\Delta'$; prove $B$ using $\Delta'$ as its input context and then output the
remaining resources $\Delta''$; finally, after having proven
$A\tensor B$, output $\Delta''$, for subsequent derivations.


\mypara{Focusing}

Even with everything mentioned so far, non-determinism is still very
present in proof-search, e.g. at any given point, many proof
rules are applicable in general. The technique of focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05}
has been previously studied as a way to discipline proof search in
linear logic -- a method created to trim down the search space of
valid proofs in linear logic, by eagerly applying invertible rules
(i.e.~rules whose conclusion implies the premises), and then by
``focusing'' on a single connective when no more direct (invertible)
rules can be applied, that is, only applying rules that breakdown
the connective under focus or its subformulas. If the search is not
successful, the procedure backtracks and another connective is
chosen as the focus.

Focusing eliminates all of the ``don't care'' non-determinism from
proof search, since the order in which invertible rules are applied
does not affect the outcome of the search, leaving only the
non-determinism that pertains to unknowns (or ``don't know''
non-determinism), identifying precisely the points at which
backtracking is necessary.


\section{Overview}

At the core of the synthesis process, and of the developed synthetiser \synname,
is a \emph{sound} and \emph{complete?} system of focusing for linear logic with
resource management. It connects the concepts seen above -- a type system
consisting of inference rules in sequent calculus for linear logic connectives;
with resource management, and focusing, as formalized techniques to,
respectively, solve resource related non-determinism, and to eliminate from the
proof search don't-care non-determinism.  Guided solely by this system's formal
representation\ref{sec:formal_system}, we can (and did) create a very robust
synthetiser for a select part of the linear types that's able to synth
programs successfully from e.g. the major linear logic equivalences. For example,
given the specification $(A \lolli (B \lolli C)) \lolli (A \tensor B) \lolli C$,
the program $\lambda a \rightarrow \lambda b \rightarrow$ let $c \tensor d$ =
$b$ in $a$ $c$ $d$ (commonly known as \emph{uncurry}) is synthetised.

The \synname\ programming language is naturally born alongside the synthetiser
to create an environment for it. Synth goals are inserted in the program by use
of a \emph{synth} keyword or syntatic \emph{mark}, indicating a program should
be generated for a given, or inferred, type. Synth results are functions in this
language's syntax. Additionally, it opens the possibility of synthetisis with
context, that is, with knowledge of other functions and eventual structures
defined in the same program.

However, a programming language only with the canonical linear types lacks
expressiveness. To enhance \synname\ and its language, and consequently synth
more interesting programs -- and thus empirically prove the feasability of
more relevant synthesis in a linear context, we extend the syntax and type
system with richer types: -- polimorfic, refinements, adts parametrizáveis...  todo



\section{Architecture}

\subsection{Implementation}

A implementação foi feita em haskell e o código está disponível em X e há um
live demo com exemplos pré-feitos em Y?


\section{Formal System}\label{sec:formal_system}


\section{Related Work}

Type-based program synthesis is a vast field of study and so it
follows that a lot of literature is available to inspire and
complement our work. Most works~\cite{DBLP:conf/lopstr/HughesO20,DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16} follow some variation of the
synthesis-as-proof-search approach, however, the process is novel for
each, due to a variety of different rich types explored, and their
respective corresponding logics and programming languages; or nuances
of the synthesis process itself, such as complementing types with
program examples; or even the programming paradigm of the output
produced (e.g. generating heap manipulating
programs~\cite{DBLP:journals/pacmpl/PolikarpovaS19}).  Some other
common patterns we might want to follow are the use of type
refinements~\cite{DBLP:conf/pldi/PolikarpovaKS16} and the support for synthesis of recursive
functions~\cite{DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15}.  These works share some common ground, and base
themselves of the same works we do -- most famously the Curry-Howard
correspondence, but also, for example, works on focusing-based
proof-search~\cite{10.1093/logcom/2.3.297}.

\mypara{Type-and-Example-Directed Program Synthesis} This
work~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
explores a purely functional, recursive program synthesis technique
based on types/proof search (as we intend to do), with examples as an
auxiliar technique to trim down the program synthesis vast search
space. A good use is
made of the general strategy of using a type system to generate
programs (by ``inverting'' the type system), rather than using it to
type-check one.  Moreover, the paper uses a data structure called
``refinement tree'' in conjunction with the extra information provided
by the examples to allow for efficient synthesis of non-trivial
programs. Overall it employs good engineering to achieve type-based
synthesis of functional typed programs, with at least two additions
that we might want to follow too (type refinements and recursive
functions).

% or how to turn your type system upside down ~ não apagar a Kubrick reference? :)
  
\mypara{Program Synthesis from Polymorphic Refinement Types} The
work~\cite{DBLP:conf/pldi/PolikarpovaKS16} also studies synthesis
recursive functional programs but in a more ``advanced'' setting. The
specification is a combination of two rich forms of types: polymorphic and
refinement types (which correspond to a first-order logic through the
Curry-Howard isomorphism) -- this is an interesting combination that offers
``expressive power and decidability, which enables automatic
verification--and hence synthesis--of nontrivial programs''.  Their
approach to refinement types consists a new algorithm that supports
decomposition of the refinement specification, allowing for separation
between the language of specification and programs and making the
approach amenable to compositional synthesis techniques.

\mypara{Resourceful Program Synthesis from Graded Linear Types} The
work~\cite{DBLP:conf/lopstr/HughesO20} synthesizes programs using an
approach very similar to our own.  The work employs so-called graded
modal types, which is an refinement of pure linear types -- a more
\emph{fine-grained} version, since it allows for quantitative
specification of resource usage, in contrast to ours either
\emph{linear} or \emph{unrestricted} (via the linear logic
exponential) use of assumptions.  Their resource management is more
complex, and so they provide solutions which adapt Hodas and Miller's
resource management
approach~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:journals/tcs/LiangM09}
-- the model we will be using.

They also use focusing as a solution to trim down search space and to
ensure that synthesis only produces well-typed programs, and our
solution will use the same literature as a base.  However, since their
underlying logic is \emph{modal} rather than purely \emph{linear}, it
lacks a clear correspondence with concurrent session-typed
programs~\cite{DBLP:journals/mscs/CairesPT16,DBLP:conf/concur/CairesP10},
which is a crucial avenue of future work. Moreover, their use grading
effectively requires constraint solving to be integrated with the
synthesis procedure, which can limit the effectiveness of the overall
approach as one scales to more sophisticated settings (e.g.~refinement
types).

% \item Resource-guided synthesis \cite{DBLP:conf/pldi/KnothWP019}
% This resource-guided means something a bit different. Programs satisfy
% a functional specification and a symbolic resource bound in the sense
% of amortized analysis, but can provide technical insights.

\section{Goals and Work Plan}

We start with the definition of our type system and simply-typed
functional language with linear function, sum and product types
($\lolli$, $\tensor$, $\one$, $\with$, $\oplus$, $\bang$).  Through
the Curry-Howard correspondence, we'll formulate the synthesis of
typed programs in our functional language as bottom-up proof-search in
linear logic.

Since proof rules in \emph{natural deduction} do not entail a
proof-search strategy, we will use the equivalent \emph{sequent
calculus} system to formulate our typing rules in a way that is
amenable to proof search.
%
In sequent calculus, rules can be naturally understood in a
\emph{bottom-up} manner, simplifying the reasoning for the
synthesizer. Moreover, the linear nature of our system requires a way
to algorithmically reason about resource contexts: for instance, if we
read rule $\tensor I$ from Section~\ref{sec:background} bottom-up, we
must non-deterministically guess how to correctly split ambient
resources into $\Delta_1$ and $\Delta_2$ to separately prove $A$ and
$B$. The resource-management approach we'll adopt to solve this has been 
previously explained in more detail.

With the use of input/output contexts, and the sequent calculus
formulation of the inference rules, we draw closer to an effective
strategy for bottom-up proof search and so of program synthesis.
%
%
However, the search process still encompasses significant
non-determinism. To address this we draw on the technique of
focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05} as
discussed above.

With the basic framework in place, we will consider additional
(simply-typed) language constructs and synthesis of program fragments
or \emph{partial synthesis} -- given a program with a typed hole,
synthesize a term to fill out the hole accordingly. We will
subsequently also consider synthesis of recursive functions and,
might, later on, add refinement types also as a way to constrain the
valid programs space.

Putting together everything above, and the more detailed information
in the background section -- linear types, sequent calculus, resource
management, and focusing, we have a set of inference rules (written
out in the appendix) that define how the synthesis process will
progress.

This is the theory we're interested in and have studied so far,
which will allow us to build the actual synthetizer. From here,
our work plan gives an overview of the steps we'll take.

\subsection{Work Plan}

The end goal is to have a synthetizer capable of
taking a type (possibly refined) as input, and to produce (possibly
interactively) one or more functions with that type, if any exist.

As mentioned above, we will begin with (both partial and full)
synthesis for the linear $\lambda$-calculus. We will then extend the
language with additional type constructs (e.g. base types, records,
algebraic data types) and consider synthesis in that setting. Finally,
we will consider synthesis of recursive functions, and,
time-permitting, we will consider simple forms of type refinements
(e.g. arithmetic refinements).

We've already implemented a parser and a type-checker for a linear
$\lambda$-calculus using the resource management technique described
in the previous section -- more than
good preparation for the development of the synthesizer, they will be
useful to \emph{validate} the synthesized functions, and to enable
the synthesis of partial programs.
%
The implementations have been carried out in Haskell since it's easier
to model and implement inference rules in a functional setting.

\mypara{Step by step} Our work so far has been constantly accompanied
by the study of somewhat dense theoretical material, state of the art,
which we must understand to overcome challenges in these fields, in
order to develop a more scalable synthetizer -- we want to display
concrete results to prove the feasability of program synthesis from
linear types, and thus open the possibility for future application to
related fields, such as program synthesis based on session types for
message-passing concurrency.

Apart from the theory, when we started studying type systems, we
created a simple type-checker for the simply-typed $\lambda$-calculus.
While learning about linear type systems, and the simply-typed linear
$\lambda$-calculus, we developed a parser, a desugaring module, and an
evaluation module to write, analyze and execute simple programs in the
language; culminating in writing a type-checker for the linear
$\lambda$-calculus. At this point, we can parse, typecheck, and
execute this small linear functional language.  The next step is to
start developing the actual synthetizer, embedding it with all the
techniques we've seen. Following work will be done to augment
usability, expand the language domain, and add features such as those
mentioned above.

\mypara{Evaluation} Validation and evaluation of our work is fairly
easy. Simple questions such as \emph{does it typecheck}? \emph{can
  we synthesize}? \emph{how fast}?  are the core of our evaluation. In
more concrete terms, we can compare our times to state of the art
synthetizers, present a list of working examples of programs synthesized
by our framework, and corresponding benchmarks.

On the eventual addition of recursive functions, refinement types, and
interactive synthesis, we'll consider, respectively, other examples to
showcase recursive functions synthesis, updated times for benchmarks
with refinement types, and a display of the possible user interaction
in the user process, and the resulting function following it.


% Expand on earlier points. Start from a functional language with linear types
% ($\tensor$, $\lolli$, $\oplus$, $\bang$); build on it with more ``stuff''.
% General techniques, drawn from proof theory via props as types: explore
% focusing to tame non-determinism/search space. Partial synthesis is still
% proof search! Add other techniques as we go along (e.g. for recursion we need
% to constrain recursive calls).

% Mention that you've already implemented a type-checker for this (useful as a
% prelim. Exercise but also later, for \emph{validation}).

% Validation and evaluation: validation is as simple as ``does it typecheck''?
% can you synth? how fast?


\section{Evaluation}


% Adicionar já exemplos do que consigo

\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
