\documentclass{llncs}

\usepackage{cmll}
\usepackage{mathpartir}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{todonotes}
\usepackage{multirow}

\newcommand{\lolli}{\multimap}
\newcommand{\tensor}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bang}{{!}}
\newcommand{\mypara}[1]{\paragraph{\textbf{#1}.}}

\newcommand{\llet}[2]{\mathsf{let}\ #1\ \mathsf{in}\ #2}

\newcommand{\synname}{\emph{SILI}}
\def\Rho{P}
\newcommand{\te}[1]{\textrm{\emph{#1}}}

\usemintedstyle{manni}

% TODO: Temos de ter o título assim? ficava mais fixe sem o número e sem o
% supervisor :P
\title{Synthesis of Linear Functional Programs}
\author{Rodrigo Mesquita - Nº 55902 \\
    Supervisor: Bernardo Toninho}
\date{July 2021}
\institute{NOVA School of Science and Technology}

\begin{document}

\maketitle

\section{Introduction}

% General motivation?
Program synthesis is an automated or semi-automated process of deriving a
program, i.e.~generating code, from a high-level specification. Synthesis can be
seen as a means to improve programmer productivity and program correctness
(e.g. through suggestion and autocompletion).
%
Specifications can take many forms such as natural
language~\cite{chen2021evaluating},
examples~\cite{DBLP:conf/popl/FrankleOWZ16} or rich types such as
polymorphic refinement types~\cite{DBLP:conf/pldi/PolikarpovaKS16} or
graded types~\cite{DBLP:conf/lopstr/HughesO20}. Regardless of the kind
of specification, program synthesisers have to deal with two main
inherent sources of complexity -- searching over a vast space of
(potentially) valid programs, and interpreting user intent.

Synthesis is said to be \emph{type-driven} when it uses types as a form of program
specification and produces an expression whose type matches the specification.
Type-driven synthesis frameworks usually leverage rich types as a way to make
specifications more expressive and prune the valid programs search space, while
maintaining a ``familiar'' specification interface (types) for the user.
%
Richer type systems allow for more precise types, which can statically eliminate
various kinds of logical errors by making certain invalid program states
ill-typed (e.g.,~a ``null aware'' type system will ensure at compile-time that
you cannot dereference a null-pointer).
%
% However, they can also be a burden -- the whole point of these type systems is
% to ensure that \emph{fewer} programs are deemed well-typed, which can pose
% additional challenges to the development process.
%
For instance, the type $\mathsf{Int} \rightarrow \mathsf{Int} \rightarrow
\mathsf{Int}$ specifies a (curried) function that takes two integers and
produces an integer. Viewed as a specification, it is extremely imprecise (there
are an infinite number of functions that satisfy this specification).  However,
the richer type $(x{:}\mathsf{Int}) \rightarrow (y{:}\mathsf{Int}) \rightarrow
\{z{:}\mathsf{Int} \mid z = x+y\}$ very precisely specifies a function that
takes integer arguments $x$ and $y$ and returns an integer $z$ that is the sum
of $x$ and $y$.

The focus of our work is on type-driven synthesis where specifications take the
form of linear types.
%
% TODO: Deixar isto para mais à frente?  More concretely, we will explore the
% proof-theoretic foundations of linear types via the Curry-Howard
% correspondence with linear logic propositions, viewing program synthesis as
% proof search in linear logic.  (Said correspondence maps \emph{types} to
% \emph{propositions in a logic}, and \emph{programs} to \emph{proofs}).
%
Linear types differ from more traditional types in that they constrain resource
usage in programs by \emph{statically} limiting the number of times certain
resources can be used during their lifetime (linear resources must be used
\emph{exactly once}).
%
They can be applied to resource-aware programming such as concurrent programming
(e.g. session types for message passing
concurrency~\cite{DBLP:journals/mscs/CairesPT16}), memory-management
(e.g.~Rust's ownership types), safely updating-in-place mutable
structures~\cite{Bernardy_2018}, enforcing protocols for external \textsc{api}s~\cite{Bernardy_2018}, to name a few.
% TODO: Repensar esta parte dos exemplos de aplicação e de como podemos
% relacionar o nosso trabalho com futuras aplicações nestes campos

\paragraph{Contributions.} Despite their long-known potential (\emph{linear
types can change the world!}\cite{Wadler90lineartypes},
\cite{DBLP:journals/mscs/CairesPT16,Bernardy_2018}) and strong proof-theoretic
foundations
\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05,DBLP:journals/tcs/CervesatoHP00},
synthesis with linear types in a more advanced context
has generally been overlooked in the literature.  In this paper we present a framework for synthesis
with linear types extended with recursive algebraic data types, parametric
polymorphism and refinements.
% TODO: extensible? proving feasability of... ? showing potential for ... ?


\paragraph{Outline.} First, we discuss linear types as specifications, the
outline of the synthesis process, and what that entails for the synthetiser --
ilustrated with concrete results (Section~\ref{sec:overview}). Then, we present
the formal system driving the synthesis (Section~\ref{sec:formal_system}).
Aftwerwards, we describe the architecture of our synthetiser named \synname, and
examine technical details and challenges of the implementation
(Section~\ref{sec:architecture}). Finally, we evaluate our work through
expressiveness benchmarks (i.e.~can we synth $X$?) and time benchmarks (i.e.~how
fast can we synth a term of type $T$?) (Section~\ref{sec:evaluation}).
%TODO: And appendice with...

%This work explores the synthesis of linear functional
%programs from types based on linear logic propositions (i.e.~linear
%types) by leveraging the Curry-Howard correspondence. 

% It
% states that propositions in a logic have a direct
% mapping to types, and well-typed programs correspond to proofs of
% those propositions.  As such, having a type be a proposition in linear
% logic, we can relate a proof of it directly to a linear
% functional program — finding a proof is finding a program with that
% type. Formulating synthesis as proof search in linear logic
% allows us to inform our work with approaches from the relevant 
% literature.

%\mypara{Goals}
% % Goals
%In the end, we intend to be able to do full and partial synthesis of
%well-typed programs. Full synthesis consists of the production of a
%function (or set of) satisfying the specification; partial synthesis
%is the ``completion'' of a partial program (i.e. a function with a
%\emph{hole} in it). The work will start from a small core linear --
%i.e. \emph{resource-aware} functional language, building up to
%recursive types/functions, with potential other avenues of further
%extension. 
%%
%We will evaluate the work through
%expressiveness benchmarks (i.e. can we synthesize a term of type $T$?) and time
%measurements (i.e. how fast can we synthesize a term of type $T$?').

\section{Overview}\label{sec:overview}

% TODO: Why linear types?
The \synname\ synthetiser combines the features of linear types with recursion,
parametric polymorphism, recursive algebraic data types, and
refinement types. The synthetiser is built on top of a system of proof-search in
linear logic. Proof-search relates to program synthesis through the Curry-Howard
correspondence~\cite{}, which states that \emph{propositions in a logic} map
directly to \emph{types}, and \emph{proofs} of those propositions to well-typed
\emph{programs} -- finding a proof of a proposition is finding a program with
that type.

Linear types make for more precise specifications than simple traditional types
because information on which resources must necessarily be used is encoded in
the type. Their preciseness also influences the search space: all programs where
a linear resource is used non-linearly (i.e. not \emph{exactly once}) are
ill-typed. With linearity built-in to the synthesis process, usage of a
linear proposition more than once is not even considered, and unused ones are caught
mid-process, making proof-search in linear logic potentially faster than
in classical logic.  % TODO: Reword?

The core of the synthesis is a \emph{sound} and \emph{complete} system
consisting of \emph{bottom-up} proof-search in propositional linear logic based
on \emph{focusing}~\cite{}. Our approach, being grounded by
propositions-as-types, ensures that all synthetised programs (i.e.~proofs) are
well-typed \emph{by construction} (i.e.~if the synthesis procedure terminates,
then the program satisfies its specification). Moreover, we can leverage the
modularity of the proof-search based approach in two key axes: first, since
proof search need not construct only closed proofs, we can effectively
synthetise program sub-expressions (i.e.~synthesis based on typed holes);
secondly, the framework is amenable to extensions to the core propositional
language, allowing for the introduction of a richer type structure while
preserving the correctness of programs by construction.

The \synname\ programming language is naturally born alongside the synthetiser
to create an environment for it. Synth goals are inserted in the program by use
of a \emph{synth} keyword or mark, indicating a program should be generated for a given
type. Additionally, it opens the possibility of synthesis with context, that is,
with knowledge of other functions and structures defined in the same program.


\paragraph{Core linear calculus.} Initially, we synthetise from
specifications that only use literals and canonical linear types ($\lolli, \with, \tensor, 1,
\oplus, \bang$), such as linear logic theorems. Namely,
the specification $(A \lolli B \lolli C) \lolli (A \tensor B) \lolli C$ produces
the program \mintinline{haskell}{(\a -> \b -> let c*d = b in a c d)}
(known as \emph{uncurry}).


\paragraph{Beyond propositional logic.} To enhance \synname\ and its language,
counter the core's lack of expressiveness, synthetise more interesting programs
-- and thus, empirically prove the feasability of more relevant synthesis in a
linear context, we extend the syntax and type system with recursiveness,
parametric polymorphism, algebraic data types (ADTs) and type refinements.

%TODO: Polymorphism ? read todo below

\paragraph{ADTs.} We reiterate the expressiveness of linear types: by requiring some types to be
used linearly (consumed), we can assure, e.g., the deconstruction of an
ADT. As such, the specification $\bang(a \lolli b) \lolli List\ a \lolli
List\ b$ isn't satisfied by the program \mintinline{haskell}{(\x -> \y -> Nil)}.
To synthetise a valid program from it, \mintinline{haskell}{(List a)} has to be
deconstructed, and its constructor arguments used. \synname\ would output the following
function (\emph{map}) without relying on any additional information:
% TODO: Aqui não estou bem a falar de polymorphism mas parece estranho não por
% visto que falo dos outros temas? se calhar faço um à parte?
%
\begin{minted}{haskell}
map c d = let !e = c in
    case d of
          Nil  -> Nil
        | Cons f -> let g*h = f in Cons (e g, map (!e) h);
\end{minted}

\paragraph{Refinements.} Refinements are predicates added to a type which must hold for any
element that inhabits said type. The \synname\ language supports simple
refinements on integers with arithmetic expressions in the predicates. They
assert the robustness of our main framework, i.e., how it is amenable to
significant additions without necessarily interfering with the algorithm.
%
As an example, from the specification $x \{Int\} \lolli y \{Int\} \lolli z
\{Int\} \lolli k \{Int\ \vert\ x + k = y * z\}$, the
program\ \mintinline{haskell}{(fn a b c = add (mult (sub 0 1) a) (mult c b))} is
synthetised.

\paragraph{Configuration.} Specifications can be additionally augmented with three keywords: ``using'', ``depth'' and
``assert''. Through them, we can fine-tune the synthesis process and, respectively,
force certain functions to be present in the synthesis outcome, allow a
\emph{deeper} search in the valid programs space, and require given predicates
to hold true in the program after the synthesis is complete.

Taken directly from the Linear Haskell paper~\cite{Bernardy_2018}, we present a
complex example of synthesis: given the linear type signatures for array
primitives (\emph{newMArray} passes a new mutable array to a function that uses
it linearly, \emph{write} takes a mutable array and writes a pair to it,
\emph{freeze} consumes a mutable array and produces an imutable array,
\emph{foldl} has the default definition) we try to synthetise a function
``array'' that provides an immutable array given a list of pairs to write. The input program is formulated as
follows:
%
\begin{minted}{haskell}
foldl :: ((a -o b -o a) -> a -o (List b) -o a);
newMArray :: (Int -> (MArray a -o !b) -o b);
write :: (MArray a -o (Int * a) -> MArray a);
freeze :: (MArray a -o !(Array a));
synth array :: (Int -> List (!(Int * a)) -> Array a)
    | using (foldl) | depth 3;
\end{minted}
%
And, in a small fraction of a second, matching exactly the linear definition for
\emph{array} as seen in Linear Haskell~\cite{Bernardy_2018}, \synname\ outputs:
%
\begin{minted}{haskell}
array b c = let !d = b in let !e = c in
    newMArray (!d) (\j -> freeze (foldl (!write) j e));
\end{minted}

\section{Synthesis Framework}\label{sec:formal_system}
% with Linear Types, Polymorphism and Refinements

%\todo[inline]{Aqui parece-me que deves comecar por explicar o que vais fazer
%nesta seccao. Sinto falta da sintaxe da linguagem, possivelmente
%incrementalmente. Comecava por listar os tipos/termos do lambda linear
%``simples'', depois em cada subseccao acrescentava as construcoes novas e como a
%sintese funciona para esse contexto. Enfatizar que a novelty é a combinacao
%destas features todas.}

%The development of \synname\ was based on an almost direct reading of concrete
%inference rules and theoretical principles (~\cite{frank pfenning notes});
%%
%\todo{aqui queres dizer que começaste por te basear num sistema de focusing para
%a logica linear (era sequer linear a formulacao que comecaste a seguir?) O que
%são theoretical principles?}
%%
%and work on its environment was also guided by formal systems and methods
%(e.g.~Hindley-Milner type inference~\cite{}).
%%
%\todo{esta frase parece-me desnecessaria. O que queres dizer neste paragrafo,
%parece-me, é que partes de um sistema focused da literatura que depois vais
%extender, inspirando te em tecnicas standard de proof search / inferencia.}

%However, with the addition of recursion and richer types to the synthesis and
%language, formal guidelines
%%
%\todo{nao sao formal guidelines, nao existem trabalhos a combinar estas features
%todas, essa parte é importante. Além de que ao incluir estas coisas deixamos de
%estar completamente em proof search, pois muitas destas coisas (e.g. recursao)
%nao sao necessariamente sound logicamente.  Tenta usar termos especificos.
%Richer types? Richer como?}
%%
%to support the implementation become scarcer, or more complex.
%%
%\todo{nao existem ou sao complicados? ambos? citacao?}
%%
%Accordingly, rather than derived
%from rules, solutions appeared first as techniques to tame the extensions'
%inherent complex problems (such as infinite recursion), and we now present
%those techniques as rules.
%%
% \todo{ Nao percebo o que queres dizer aqui. O que sao explicit rules e implicit rules?
% Ou há regras ou nao há. O que estas a
% dizer é que inventaste tecnicas de sintese para estes casos, que correspondem
% as regras que vais apresentar. E tao explicito ou implicito como o que esta na
% literatura.}

Program synthesis from linear types in combination with polymorphism, recursive
algebraic data types, and refinements, is new in the synthesis 
literature. Despite the substantial amount of research on linear logic and
proof-search upon which we found our core synthetiser, formal guidelines for the
richer types and their inherent complex problems (such as infinite recursion)
don't exist. % TODO: Developed an algorithm...?

With this section we aim to formalize as rules the novel techniques that guide
synthesis from our more expressive specifications, alongside the already well
defined rules that model the core of the synthetiser, putting together a
\emph{sound} set of inference rules that characterizes our framework for synthesis of
recursive programs from specifications with the select richer types in a linear context,
% TODO: Better?....
and describes the system in enough detail for the synthetiser to be reproducible
by a theory-driven implementation.

Even so, note that \emph{soundness} and \emph{completeness} have two very
different meanings, and our \emph{sound} set of rules only guarantees we won't
synthetise incorrect programs, but still hides a possibly infinite number of correct
proofs; and that the valid programs derivable through it reflect the somewhat
subjective trade-offs we decided on. Different choices and approaches outside
the core might lead to completely distinct synths and spaces of valid programs.


\mypara{Core Rules} The core system comprises of proof-search in sequent
calculus rules for linear logic with resource-management and focusing.
%
Andreoli's \emph{focusing} for linear logic~\cite{Andreolli
focusing} is a technique to remove inessencial non-determinism from
proof-search by structuring the application of invertible and non-invertible
inference rules. Rules are classified as \emph{left} or \emph{right},
\emph{invertible} or \emph{non-invertible} as follows:...\todo{não estou a
conseguir formular bem esta introdução}
The proof search is separated into two phases:
%
inversion ($\Uparrow$), in which we apply all invertible rules eagerly, and
focusing ($\Downarrow$), in which we decide a proposition to focus on, and then
apply non-invertible rules, staying focused until either the focus is again an
asynchronous, i.e. invertible, proposition, or the proof is complete.  For the
invertible rules, a third, ordered, context ($\Omega$) is needed to hold
propositions resulting from decomposing connectives; propositions we'll later
try to invert, moving them to the linear context ($\Delta$) when we fail to.

% TODO: Explicar vários judgments?
% \todo{Primeiro explica os varios judgments (as ``formas'' das
%   conclusoes das regras) e o que significam as varias componentes,
%   nomeadamente a parte da gestao de recursos.  Depois podes explicar
%   regras concretas como fazes a seguir. Tambem seria util dizer quais
%   os tipos que sao invertiveis a esquerda/direita, pois o leitor nao
%   faz ideia do que estas a dizer.}
%
% Esta parte em geral está a ficar pouco coerente, com a introdução do focusing,
  % a suposta introdução dos judgments, e a introdução da gramática... deixa

The core language is a simply-typed linear lambda calculus with linear
functions, additive and multiplicative pairs, multiplicative unit, additive sums
and the exponential modality (to internalize unrestricted use of variables), for
which the terms ($M,N$) and types' ($\tau$) syntax is constructed with the
following grammar:
%
% TODO: Nao é um problema eu dizer que o A e B são átomos ali em cima e aqui
% usá-los como tipos gerais?
\[
\begin{tabular}{lclc}
$M,N$ & $\ ::=\ $ & $u, v$ & \emph{variables} \\
    & & $\vert\ \lambda x. M\ \vert\ M\ N$ & $\lolli$ \\
    & & $\vert\ M \with N\ \vert\ \mathsf{fst}\ M\ \vert\ \mathsf{snd}\ M$ & $\with$ \\
    & & $\vert\ M \tensor N\ \vert\ \llet{u\tensor v = M}{N}$ & $\tensor$ \\
    & & $\vert\ \star\ \vert\ \llet{\star = M}{N}$ & $\textbf{1}$ \\
    & & $\vert\ \mathsf{inl}\ M\ \vert\ \mathsf{inr}\ M\ \vert\ (\mathsf{case}\
    M\ \mathsf{of\ inl}\ u \Rightarrow N_1\ \vert\ \mathsf{inr}\ v \Rightarrow
    N_2)$ & $\oplus$ \\
    & & $\vert\ \bang M\ \vert\ \llet{\bang u = M}{N}$ & $\bang$ \\
    \\
$\tau$ & $\ ::=\ $ & $A, B, C\ \vert\ \tau \lolli \tau\ \vert\ \tau \with \tau\
    \vert\ \tau \tensor \tau\ \vert\ \textbf{1}\ \vert\ \tau \oplus \tau\ \vert\ \bang \tau$
\end{tabular}
\]
%
Putting together linear logic and linear lambda calculus through the Curry-Howard correspondence, resource
management, and focusing, we get the following core formal system (heavily inspired
by Frank Pfenning's notes on focusing for linear logic~\cite{FPnotes...}) -- in which the
first rule is read: to synthetise a program of type $A \lolli B$ while inverting
right (the $\Uparrow$ on the goal), with unrestricted context $\Gamma$, linear
context $\Delta$, and inversion context $\Omega$, synthetise a program of type $B$ with
an additional hypothesis of type $A$ named $x$ in the $\Omega$ context,
resulting in the program $M$ and output linear
context $\Delta'$ that cannot contain the added hypothesis $x{:}A$. Finally, the
resulting program is $\lambda x . M$ and the output linear context is
$\Delta'$.

We begin with the right invertible rules:
\begin{mathpar}

    % -o R
    \infer*[right=($\lolli R$)]
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \vdash M : B \Uparrow \and x
    \notin \Delta'}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash \lambda x . M : A
    \lolli B \Uparrow}

\and

    % & R
    \infer*[right=($\with R$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \vdash M : A \Uparrow \and \Gamma ;
    \Delta/ \Delta'' ; \Omega \vdash N : B \Uparrow \and \Delta' = \Delta''}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash  (M \with N) : A
    \with B \Uparrow}

\end{mathpar}

When we reach a non-invertible proposition on the right, we start inverting the
$\Omega$ context. The rule to transition to inversion on the left is:
\begin{mathpar}
    \infer*[right=($\Uparrow R$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash C \and C\ \textrm{not
    right asynchronous}}
    {\Gamma ; \Delta/\Delta' ; \Omega \vdash C \Uparrow}
\end{mathpar}

Followed by the left invertible rules:
\begin{mathpar}

    \infer*[right=($\tensor L$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega, y{:}A, z{:}B \Uparrow\ \vdash M : C
    \and y,z \notin \Delta'}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \tensor B \Uparrow\ \vdash\
    \textrm{let}\ y \tensor z = x\ \textrm{in}\ M : C}


\and

    \infer*[right=($1 L$)]
    {\Gamma ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash M : C}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}1 \Uparrow\ \vdash\ \textrm{let}\
    \star =
    x\ \textrm{in}\ M : C}

\and

    \mprset{flushleft}
    \infer*[right=($\oplus L$)]
    {
    \Gamma ; \Delta/ \Delta' ; \Omega, y{:}A \Uparrow\ \vdash M : C \and
    y \notin \Delta' \\
    \Gamma ; \Delta/ \Delta'' ; \Omega, z{:}B \Uparrow\ \vdash N : C \\
    z \notin \Delta'' \\
    \Delta' = \Delta''
    }
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}A \oplus B \Uparrow\ \vdash\
    \textrm{case}\ x\ \textrm{of}\ \textrm{inl}\ y \rightarrow M\ |\
    \textrm{inr}\ z \rightarrow N : C}

\and

    \infer*[right=($\bang L$)]
    {\Gamma, y{:}A ; \Delta/ \Delta' ; \Omega \Uparrow\ \vdash M : C}
    {\Gamma ; \Delta/\Delta' ; \Omega, x{:}\bang A \Uparrow\ \vdash\
    \textrm{let}\ \bang y = x\ \textrm{in}\ M : C}

\end{mathpar}

When we find a synchronous (i.e. non-invertible) proposition in $\Omega$,
we simply move it to the linear context $\Delta$, and keep inverting on the left:
\begin{mathpar}
    \infer*[right=($\Uparrow L$)]
    {\Gamma; \Delta, A/\Delta'; \Omega \Uparrow\ \vdash C \and A\ 
    \textrm{not left asynchronous}}
    {\Gamma; \Delta/\Delta'; \Omega, A \Uparrow\ \vdash C}
\end{mathpar}

After inverting all the asynchronous propositions in $\Omega$ we'll reach a state
where there are no more propositions to invert ($\Gamma'; \Delta'; \cdot
\Uparrow\ \vdash C$). At this point, we want to \emph{focus} on a proposition.
The focus object can be any of the three: the proposition on the right (the
goal), a proposition from the linear $\Delta$ context, or a proposition from the
unrestricted $\Gamma$ context. For these options we have three \emph{decision}
rules:
\begin{mathpar}
    \infer*[right=(decideR)]
    {\Gamma; \Delta/\Delta' \vdash C \Downarrow \and C\ \textrm{not atomic}}
    {\Gamma; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
\and
    \infer*[right=(decideL)]
    {\Gamma; \Delta/\Delta' ; A \Downarrow\ \vdash C}
    {\Gamma; \Delta, A/\Delta';\cdot \Uparrow\ \vdash C}
\and
    \infer*[right=(decideL!)]
    {\Gamma, A; \Delta/\Delta' ; A \Downarrow\ \vdash C}
    {\Gamma, A; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
\end{mathpar}

The decision rules are followed by either left or right focus rules:
\begin{mathpar}
    \infer*[right=($\tensor R$)]
    {\Gamma; \Delta/\Delta' \vdash M : A \Downarrow \and \Gamma ; \Delta'/\Delta'' \vdash N
    : B \Downarrow}
    {\Gamma; \Delta/\Delta'' \vdash (M \tensor N) : A \tensor B \Downarrow}
\and
    \infer*[right=($1 R$)]
    { }
    {\Gamma; \Delta/\Delta \vdash \star : \textbf{1} \Downarrow}
\and
    \infer*[right=($\oplus R_1$)]
    {\Gamma; \Delta/\Delta' \vdash M : A \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ \textrm{inl}\ M : A \oplus B \Downarrow}
\and
    \infer*[right=($\oplus R_2$)]
    {\Gamma; \Delta/\Delta' \vdash M : B \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ \textrm{inr}\ M : A \oplus B \Downarrow}
\and
    \infer*[right=($\bang R$)]
    {\Gamma; \Delta/\Delta'; \cdot \vdash M : A \Uparrow \and \Delta = \Delta'}
    {\Gamma; \Delta/\Delta \vdash \bang M : \bang A \Downarrow}
\and
    \infer*[right=($\lolli L$)]
    {\Gamma; \Delta/\Delta'; y{:}B \Downarrow\ \vdash M : C \and \Gamma;
    \Delta'/\Delta''; \cdot \vdash N : A \Uparrow}
    {\Gamma; \Delta/\Delta''; x{:}A \lolli B \Downarrow\ \vdash M\{(x\,N)/y\} : C}
\and
    \infer*[right=($\with L_1$)]
    {\Gamma; \Delta/\Delta'; y{:}A \Downarrow\ \vdash M : C}
    {\Gamma; \Delta/\Delta'; x{:}A \with B \Downarrow\ \vdash M\{(\textrm{fst}\ x)/y\} : C}
\and
    \infer*[right=($\with L_2$)]
    {\Gamma; \Delta/\Delta'; y{:}B \Downarrow\ \vdash M : C}
    {\Gamma; \Delta/\Delta'; x{:}A \with B \Downarrow\ \vdash M\{(\textrm{snd}\ x)/y\} : C}
\end{mathpar}
%
Eventually, the focus proposition will no longer be synchronous, i.e. it's
atomic or asynchronous. If we're left focused on an atomic proposition we either
instantiate the goal or fail. Otherwise the left focus is asynchronous and we
can start inverting it. If we're right focused on a proposition that isn't right
synchronous, we switch to inversion as well. Three rules model these conditions:
%
\begin{mathpar}
    \infer*[right=(init)]
    {  }
    {\Gamma; \Delta/\Delta'; x{:}A \Downarrow\ \vdash x : A}
\and
    \infer*[right=($\Downarrow L$)]
    {\Gamma; \Delta/\Delta'; A \Uparrow\ \vdash C \and A\ \textrm{not atomic and not left synchronous}}
    {\Gamma; \Delta/\Delta'; A \Downarrow\ \vdash C}
\and
    \infer*[right=($\Downarrow R$)]
    {\Gamma; \Delta/\Delta'; \cdot \vdash A \Uparrow}
    {\Gamma; \Delta/\Delta' \vdash A \Downarrow}
\end{mathpar}
The rules written above together make the core of our synthetiser. Next, we'll
present new rules that align and build on top of these to synthetise recursive
programs from more expressive (richer) types.

\mypara{Algebraic Data Types} In its simplest form, an algebraic data type (ADT)
is a named sum type of any type, i.e. a named type that can be instantiated by one of many
tags (also known as constructors) that take exactly one fixed type, which might
be, e.g., a product type ($A \tensor B$), or unit ($1$), in practice allowing
for constructors with an arbitrary number of parameters. In the \synname\
language, the programmer can define custom ADTs; as an example, we show the
definition of an ADT which might be constructed with zero, one, or two
propositions of type $A$, using its syntax: \mintinline{haskell}{data Container
= None 1 | One A | Two (A * A)}. The language grammar is extended as:
\[
\begin{tabular}{lclc}
    $M,N$ & $\ ::=\ $ & $\dots\ \vert\ \emph{Cons}_n\ M\ \vert\ (\mathsf{case}\
    M\ \mathsf{of}\ \dots\ \vert\ \emph{Cons}_n\ u \Rightarrow N)$ \\
    $\tau$ & $\ ::=\ $ & $\dots\ \vert\ \langle\emph{name}\rangle$ \\
    $ADT$ & $\ ::=\ $ & (\textsf{data} $\langle\emph{name}\rangle$\ $=$\ $\emph{Cons}_1\ \tau$\
$\vert$\ $\dots$\ $\vert$\ $\emph{Cons}_n\ \tau$) \\ \end{tabular}
\]
%TODO: Como definir gramática dos ADTs?

The semantics of the ADTs relate to those of the plus ($\oplus$) type --
both are additive disjunctions.  A proof of an ADT requires only a proof of one
of the types that a constructor takes, similar to the way $\oplus$ requires only
proof of either the left or right type it consists of. Analogously, a proof of
$C$ by deconstructing an ADT stipulates that all types that can construct the
ADT can prove $C$, akin to the left rule for the $\oplus$ connective. In effect,
the inference rules for a simple ADT mimic the $\oplus$ rules, for an
arbitrary ($>1$) number of types, rather than exactly two. Therefore, there's one
left rule for ADTs, and an arbitrary number of right rules, one for each
constructor:

\begin{mathpar}
    \infer*[right=(adtR)]
    {\Gamma; \Delta/\Delta' \vdash M : X_n \Downarrow}
    {\Gamma; \Delta/\Delta' \vdash\ C_n \ M : T \Downarrow}
\and
    \mprset{flushleft}
    \infer*[right=(adtL)]
    {
        \Gamma ; \Delta/ \Delta'_1 ; \Omega, y_1{:}X_1 \Uparrow\ \vdash M_1 : C
        \and
        y_1 \notin \Delta'_1
        \\
        \Gamma ; \Delta/ \Delta'_2 ; \Omega, y_2{:}X_2 \Uparrow\ \vdash M_2 : C
        \\
        y_2 \notin \Delta'_2
        \\\\
        \\ \dots
        \\
        \Gamma ; \Delta/ \Delta'_n ; \Omega, y_n{:}X_n \Uparrow\ \vdash M_n : C
        \\
        y_n \notin \Delta'_n
        \\
        \Delta'_1 = \Delta'_2 = \dots = \Delta'_n
    }
    {\Gamma ; \Delta/\Delta'_1 ; \Omega, x{:}T \Uparrow\
    \vdash\ \textrm{case}\ x\ \textrm{of}\ \dots\ |\ C_n\ y_n
    \rightarrow M_n : C}
\end{mathpar}


where the ADT $T$ and its constructors stand for any ADT with form
\begin{minted}{haskell}
    data T = C1 X1 | C2 X2 | ... | Cn Xn
\end{minted}

% repetition does not legitimize :p

A more general (and interesting) formulation of ADTs says an ADT can be
recursive (or "indutively defined"), meaning constructors can take as arguments
values of the type they are defining. Seemingly small, this change has a big
impact in the synthesis process. Take, for instance, the ADT defined as
\mintinline{haskell}{data T = C1 T}, the synth goal $T \lolli C$, and its
derivation:
%
\begin{mathpar}
    \infer*[right=($\lolli R$)]
    {
        \infer*[right=($\Uparrow R$)]
        {
            \infer*[right=(adtL)]
            {
                \infer*[right=(adtL)]
                {\dots}
                {\Gamma; \Delta/\Delta'; \Omega, y{:}T \Uparrow\ \vdash \textrm{case}\ y\ 
                \textrm{of}\ C_1\ z \rightarrow \dots : C}
            }
            {\Gamma; \Delta/\Delta'; \Omega, x{:}T \Uparrow\ \vdash \textrm{case}\ x\ 
            \textrm{of}\ C_1\ y \rightarrow \dots : C}
        }
        {\Gamma; \Delta/\Delta'; \Omega, x{:}T \vdash \dots : C
        \Uparrow}
    }
    {\Gamma; \Delta/\Delta'; \Omega \vdash \lambda x . \dots :
    T \lolli C \Uparrow}
\end{mathpar}
%
Using our current system, we are to apply an infinite number of times the left
ADT rule, never reaching a final state for the proof. Symmetrically, the
derivation for the synth goal $T$ is also infinite.
%
\begin{mathpar}
    \infer*[left=(adtR)]
    {
        \infer*[left=(adtR)]
        {
            \infer*[left=(adtR)]
            {\dots}
            {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
        }
        {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
    }
    {\Gamma; \Delta/\Delta'; \Omega \vdash C_1 \dots : T \Downarrow}
\end{mathpar}
%
To account for this situation, we impede the decomposition of an ADT in subsequent
proofs of its branches, and, symetrically, don't allow construction of an
ADT when trying to synthetise an argument for its constructor.
%
%
For this, we need two more contexts, $\Rho_C$
for constraints on construction and $\Rho_D$ for constraints on
deconstruction. Together, they hold a list of ADTs that can not be constructed or
deconstructed at some point in the proof. For convenience, they are represented by a single $\Rho$ if
unused. All rules must trivially propagate these. The ADT rules are explicitly
modified to take this into consideration:
%
\begin{mathpar}
    \infer*[right=(adtR)]
    {(\Rho_C'; \Rho_D) ; \Gamma; \Delta/\Delta' \vdash M : X_n \Downarrow \and
    T \notin \Rho_C}
    {(\Rho_C; \Rho_D);\Gamma; \Delta/\Delta' \vdash\ C_n \ M : T \Downarrow}
\and
    \mprset{flushleft}
    \infer*[right=(adtL)]
    {
        T \notin \Rho_D
        \and
        \Delta'_1 = \dots = \Delta'_n 
        \\
        (\Rho_C; \Rho'_D);\Gamma ; \Delta/ \Delta'_1 ; \Omega, y_1{:}X_1 \Uparrow\ \vdash M_1 : C
        \and
        y_1 \notin \Delta'_1
        \\\\
        \\ \dots
        \\\\
        (\Rho_C; \Rho'_D);\Gamma ; \Delta/ \Delta'_n ; \Omega, y_n{:}X_n \Uparrow\ \vdash M_n : C
        \and
        y_n \notin \Delta'_n
    }
    {(\Rho_C; \Rho_D); \Gamma ; \Delta/\Delta'_1 ; \Omega, x{:}T \Uparrow\
    \vdash\ \textrm{case}\ x\ \textrm{of}\ \dots\ |\ C_n\ y_n
    \rightarrow M_n : C}
\end{mathpar}
where
\begin{mathpar}
    \Rho'_C = \textrm{\textbf{if}}\ T\ \textrm{is recursive \textbf{then}}\ \Rho_C,
    T\ \textrm{\textbf{else}}\ \Rho_C

    \Rho'_D = \textrm{likewise}
\end{mathpar}
%
These modifications prevent the infinite derivations in the scenarios described
above, however, they also greatly limit the space of derivable programs, leaving
the synthetiser effectively unable to synthetise from specifications with recursive
types, which is undesirable, so we add three rules to complement the
restrictions on construction and destruction of recursive types.

First, after decomposing a recursive ADT, we must consume all constructor
parameters, and since we can't decompose the self type any further because of a
restriction, we should be able to use it from the linear context; even though,
non-restricted types are not to take that course since they weren't unnaturally
restricted, and because it would result in additional non-determinism and
infinite recursion. Second, without any additional rules, an ADT in the linear
context will loop back to the inversion context, and then loop again, going back and
forth between the two contexts for infinity; instead, when focused on an ADT, we should
either instance the goal (provided they're the same type), or switch to
inversion if and only if its decomposition isn't restricted. The three
following rules ensure this:
\begin{mathpar}
    \infer*[right=(adt$\Uparrow$L)]
    {
        (\Rho_C; \Rho_D);\Gamma; \Delta, x{:}T/\Delta'; \Omega \Uparrow\ \vdash M : C
        \and
        T \in \Rho_D
    }
    {(\Rho_C; \Rho_D);\Gamma; \Delta/\Delta'; \Omega, x{:}T \Uparrow\ \vdash M : C}
    \and
    \infer*[right=(adt-init)]
    {  }
    {\Rho; \Gamma; \Delta/\Delta'; x{:}T \Downarrow\ \vdash x : T}
    \and
    \infer*[right=(adt$\Downarrow$L)]
    {(\Rho_C; \Rho_D); \Gamma; \Delta/\Delta'; x{:}T \Uparrow\ \vdash M :
    T \and T \notin \Rho_D}
    {(\Rho_C; \Rho_D); \Gamma; \Delta/\Delta'; x{:}T \Downarrow\ \vdash M : T}
\end{mathpar}
% TODO: Adicionar regra que diz que se for ADT Rec -o A então a construção de
% ADT Rec é logo restrita?? parece que acelera minimamente mas não arranjo um
% exemplo em que seja crucial. pode ser uma coisa que tenha ficado para resolver
% uma coisa antiga que ficou resolvida mais tarde com outra modificação e então
% é agora inutil. Talvez seja demais adicionar à regra
%
Finally, these rules end up ensuring that a recursive ADT will deconstruct
itself once, and move the recursive argument from its constructor to the linear
context -- essentially forcing it to be used as a function argument, and most
usually that function will be the synthesis recursive call.


\mypara{Recursion} The main idea behind synthesis of recursive programs is the
labeling of the main goal and the addition of its type, under that name, to the
unrestricted context. That is, to synthetise a function of type $A \lolli B$ named
\emph{f}, the initial judgment can be written as
\begin{mathpar}
    \infer
    {\dots}
    {\Gamma, f{:}A \lolli B; \Delta/\Delta'; \Omega \vdash M :
    A \lolli B \Uparrow}
\end{mathpar}
and, by definition, all subsequent inference rules will have
($f{:}A \lolli B$) in the $\Gamma$ context too.
We can also force the usage of the recursive call by adding it not only to the
unrestricted context, but to the linear one as well.

Having said that, we won't allow the use of the recursive call in $\Gamma$ right
away. If we did, every synth goal would have a trivial proof (a non-terminating
function that just calls itself) that could even shadow other valid, relevant,
solutions.  Instead, our framework allows the use of recursion only after having
deconstructed a recursive ADT. To avoid complicating the rules, we'll add the
invariant: the recursive hypothesis can only be \textbf{used in recursive branches of
ADT deconstruction}, i.e. the recursive call should only take "smaller", recursive,
hypothesis as arguments. To illustrate, having \mintinline{haskell}{data List =
Nil | Cons (A * List)}, recursion is only
"available" when deconstructing ($\textrm {List} \vdash C$), and only in
the \emph{Cons} branch. % (TODO: rewrite? O bold é meio estranho não?)


\mypara{Polymorphic Types} A polymorphic specification is a type of form
$\forall \overline{\alpha}.\ \tau$ where $\overline{\alpha}$ is a set of bound
variables that quantify universally the type variables in $\tau$, which is also
called a \emph{scheme}.  Synthesis of a scheme comprises of turning it into a
non-quantified type, and then treating its type variables agnostically.  First,
\textbf{type variables are considered atomic types}, then, instantiate the bound
variables of the scheme as described by the Hindley-Milner inference
method's~\cite{HM-infer} instantiation rule (put simply, generate fresh names
for each bound type variable); e.g. the scheme $\forall \alpha.\ \alpha \lolli
\alpha$ could be instantiated to $\alpha0 \lolli \alpha0$. We add a rule for
this:
\begin{mathpar}
    \infer*[right=($\forall R$)]
    { \Rho; \Gamma; \Delta/\Delta'; \Omega \vdash \tau' \Uparrow \and \forall
    \overline{\alpha}.\ \tau
    \sqsubseteq \tau'}
    {\Rho; \Gamma; \Delta/\Delta'; \Omega \vdash \forall \overline{\alpha}.\
    \tau \Uparrow}
\end{mathpar}
where
\[
    \forall \overline{\alpha}.\ \tau \sqsubseteq \tau'\ \textrm{indicates type}\
    \tau'\ \textrm{is an \emph{instantiation} of type scheme}\ \forall
    \overline{\alpha}.\ \tau
\]
%
As such, the construction of a derivation in which, to some types, no specific
rules can be applied (except for the \textsc{init} rule, a type variable can
instance a goal that's a type variable of the same name), corresponds to the
synthesis of a program where some expressions are treated agnostically --
nothing constrains their type -- meaning the program is polymorphic. The
simplest example is the polymorphic function \emph{id} of type $\forall \alpha
.\ \alpha \lolli \alpha$. The
program synthetised from that specification is $\lambda x . x$, a lambda
abstraction that doesn't constrain its parameter $x$ in any way.
% TODO: \todo{R : rewrite this? não estranho?}

The interesting challenge of polymorphism in synthesis is the usage of
schemes from the unrestricted context.  To begin with, $\Gamma$ now
holds both types and schemes. Consequently, after the rule
\textsc{decideLeft!} is applied, we become left focused on either a
type or a scheme. Since left focus on a type is already well
formulated, only rules for left focus on a scheme are needed. Our
algorithm instances bound type variables of the focused scheme with
existential type variables, and the instanced type becomes the left
focus. Inspired by the Hindley-Milner system, we also generate
inference constraints on the existential type variables (postponing
the decision of what type it should be to be used in the proof), and
collect them in a new constraints context $\Theta$ that is propagated
across derivation branches the same way the linear context is (by
havin an input and output context ($\Theta/\Theta'$)).  In contrast to
Hindley-Milner's inference, everytime a constraint is added it is solved against all other
constraints -- a branch of the proof search is desired to fail as soon
as possible.
%TODO: Como fazer? \todo{O que
% significa $?\alpha$ ? Não foi explicado -- no Hindley-Milner nao ha
% propriamente o problema de misturar variaveis existenciais com universais}
Note that we
instance the scheme with \emph{existential} type variables ($?\alpha$)
rather than just type variables ($\alpha$) since the latter represent universal
types in the synth, and the first represent an existential instance of
a scheme, that might inclusively constrain traditional type
variables. Additionally, it's required that all existential type
variables are assigned a type. These concepts
are formalized with the rules:
\begin{mathpar}
    \infer*[right=($\forall L$)]
    {
        \Theta/\Theta'; \Rho; \Gamma; \Delta/\Delta'; \tau' \Downarrow\ \vdash C
        \\
        \forall \overline{\alpha}.\ \tau \sqsubseteq_E \tau'
        \\
        \textrm{ftv}_E(\tau') \cap \{ ?\alpha\ \vert\ (?\alpha \mapsto \tau_x) \in \Theta'\} = \emptyset
    }
    {\Theta/\Theta'; \Rho; \Gamma; \Delta/\Delta'; \forall \overline{\alpha}.\ \tau \Downarrow\ \vdash C}
    \and
    \infer*[right=($?L$)]
    {\textsc{unify}(?\alpha
    \mapsto C, \Theta)}
    {\Theta/\Theta, ?\alpha \mapsto C ; \Rho; \Gamma; \Delta/\Delta';
    x{:}?\alpha \Downarrow\ \vdash x : C}
    \and
    \infer*[right=($\Downarrow L?$)]
    {\textsc{unify}(?\alpha
    \mapsto A, \Theta)}
    {\Theta/\Theta, ?\alpha \mapsto A ; \Rho; \Gamma; \Delta/\Delta';
    x{:}A \Downarrow\ \vdash x : ?\alpha}
    \\
    \forall \overline{\alpha}.\ \tau \sqsubseteq_E \tau'\ \textrm{means type}\
    \tau'\ \textrm{is an \emph{existential instantiation} of scheme}\ \forall
    \overline{\alpha}.\ \tau
    \and
    \textrm{ftv}_E(\tau')\ \textrm{is the set of free \emph{existential} type variables in type}\ \tau'
    \and
    ?\alpha \mapsto \tau_x\ \textrm{is a mapping from \emph{existential} type}\ ?\alpha\
    \textrm{to type}\ \tau_x
    \\
    \textsc{unify}(c, \Theta)\ \textrm{indicates wether constraint}\ c\
    \textrm{can be unified with those in}\ \Theta
\end{mathpar}


\mypara{More Restrictions} We can now look at two more sources of infinite
recursion in the synthesis process. The first is the use of an unrestricted
function to synthetise a term of type $\tau$ that in turn will require a term of
the same type $\tau$. A good example is the sub-goal judgment $\Gamma,(a \lolli
b \lolli b); (a \lolli b \lolli b) \Downarrow\ \vdash b$ that
appears while synthetising \emph{foldr} -- we apply ($\lolli$L)
until we can use \textsc{init} ($b \Downarrow\ \vdash b$), and then we must
synthetise an argument of type $b$. Without any additional restrictions, we
can end up left focused on $(a \lolli b \lolli b)$ again, and again require $b$,
and on and on. The solution will be to disallow the usage of the same function
to synthetise the same goal a second time further down in the derivation.

% TODO TODO: Big rewrite
The other situation occurs when using an unrestricted polymorphic function that
requires synthesis of a term with an existential type when the goal is an
existential type. In contrast to the previous problem, the type of the goal and
of the argument that will cause the loop won't be the same, since instantiated
bound variables are always fresh. For example, for $\Gamma,\forall \alpha,\beta
.\ \alpha\lolli\beta\lolli\beta;?\alpha\lolli?\beta\lolli?\beta \Downarrow\
\vdash\ ?\sigma$, we'll unify $?\beta$ with $?\sigma$, and then require a term
of type $?\beta$ (not $?\sigma$). We want to forbid the usage of the \emph{same}
function to attain \emph{any} existential goal, provided that function might
create existential sub-goals (i.e. it's polymorphic). However, we noticed that,
even though for most tried problems this ``same function'' approach worked,
context-heavy problems such as \emph{array} (seen in Section~\ref{sec:overview})
wouldn't terminate in a reasonable amount of time.  In fact, with $n$
polymorphic functions in the unrestricted context, the complexity of searching for a
program of any existential type $?\alpha$, while restricting solely the used
function, is $O(n!)$~\ref{exemplo_apendice?}. As such, we'll instead forbid the
usage of \emph{all} unrestricted polymorphic functions to synthetise any
existential type if, altogether, we've already used $d_e$ times a polymorphic
function for \emph{any} existential sub-goal, where $d_e$ is a \emph{constant
existential depth} that controls the maximum depth of ``existential synthetis''.
This approach reduces the complexity of our proof-search algorithm for
existential types to $O(\tfrac{n!}{d_e!}) = O(n^{d_e})$.

Extending the restrictions context ($\Rho$) with restrictions on using the
unrestricted context ($\Rho_{L!}$), we modify \textsc{decideLeft!} to formalize
the previous paragraph:
\begin{mathpar}
    \mprset{flushleft}
    \infer*[right=(decideL!)]
    {
    (A, C) \notin \Rho_{L!}
    \\
    \textsc{isExist}(C) \Rightarrow |\{u\ |\
    (f,u)\in\Rho_{L!},\textsc{isExist}(u),\textsc{isPoly}(f)\}| < d_e
    \\
    \Theta/\Theta';(\Rho_C,\Rho_D,\Rho_{L!}');\Gamma, A; \Delta/\Delta' ; A
    \Downarrow\ \vdash C
    }
    {\Theta/\Theta';(\Rho_C,\Rho_D,\Rho_{L!});\Gamma, A; \Delta/\Delta';\cdot \Uparrow\ \vdash C}
    \\
    \Rho_{L!}' =\ \textbf{if}\ \textsc{isFunction}(A)\ \textbf{then}\ \Rho_{L!},(A, C)\ \textbf{else}\ \Rho_{L!}
\end{mathpar}


\mypara{Polymorphic ADTs}

\mypara{Refinement Types}



\section{Architecture}\label{sec:architecture}

The \synname\ synthetiser operates as part of the pipeline that processes a
full program in the \synname\ language, and is called to generate parts marked
(by the syntax \emph{\{\{ ... \}\}}) for
synthesis of said program. The main pipeline consists of:
\[
    \textrm{Parsing} \rightarrow \textrm{Desugaring} \rightarrow \textrm{Inference} \rightarrow \textrm{Synthesis}
\]

The parsing module converts ADT declarations and top level functions written in
the language's syntax to a list of abstract syntax trees (ASTs) understood by the rest of the
pipeline.

The desugaring module converts the frontend AST to a core AST without syntatic
sugar and using the locally-nameless representation~\cite{locally nameless}, in
which all bound variables have no name, and instead are addressed using the
number of binders traversed since the binding.
% TODO: não ficou muito mais claro :\todo{Explica o que e isto}
This way inference can be done without worrying about name conflicts.

The inference module will traverse the list of ASTs in order (top-down), and
infer the type of all functions defined by the user, checking the inferred
type against a possible user given type annotation. Whenever a synth mark is
found in the AST, all the inferrence context up to that point is added to the
mark, and if not explicit, its type is inferred, essentially defining the synth
context and synth goal.

The synthesis module iterates over the synth marks and runs the synthetiser with
its context and goal for each, minimizing the resulting expression, and
replacing the mark in the frontend AST with the synthed program.


\subsection{Implementation}

\mypara{General} The complete implementation of the synthetiser and environment
pipeline was done in Haskell...

\mypara{Backtracking} Even though the synthetiser formal system helps, there's
still non-determinism in the synthesis process. At diverse times in the proof
construction we must choose one of multiple rules to apply, and in the event of
not being able to construct the proof following that choice, we want to
backtrack to the decision point and attempt to create a proof taking a different
route. To this effect, our prototype synthetiser depends on \emph{logict}, "a backtracking logic-programming
monad"~\cite{logict}.

\mypara{SMT Solving} To typecheck and synthetise refinement types and dependent
functions, we need to contact an SMT solver to check satisfiability. For this,
we use the library \emph{sbv}~\cite{sbv}. Unfortunately, this library does not
support the usage of the SMT solvers to satisfy and output more complicated
first-order logic uninterpreted functions -- since this is needed to synthetise refinement types
when right focused, we instead use unsafe library primitives to communicate with
the solver directly, constructing the logical formulas ourselves. For 
satisfying formulas, for instance, when typechecking, or left focused on a
refinement type, we use the library api successfully to get a veridct on
satisfiability.

\mypara{Memoization} Some proofs attempt to synthetise the same sub-goal with
the same premises more than once. Since these proof attempts might be expensive
performance-wise, and we can rely on the program's determinism (equal calls to
synthetise result in equal outcomes), we add to our synthetiser memoization, an
optimization technique that stores function calls and returns cached values for
equal inputs. For our synthetiser, two calls are equivalent when contexts
$\Theta, \Rho, \Gamma, \Delta$, $\Omega$, (or alternatively the left focus), and
the goal, are equivalent. Because comparing all of these is slow, our
implementation hashes the relevant values and uses the result as the key.
Furthermore, a branch we want to memoize might fail, not providing any result --
we also want to record synthesis failure, as it happens more often than
successful results. We capture failure of a logict computation using the $ifte$
function and represent failure or success information with the Maybe type. We
ran benchmarks to measure the impact of memoization on the performance. It's
especially noticeable in synthesis with multiple functions in the unrestricted
context, or more complex specifications:
%TODO: ignore constraints if type isnnt existential
%
\begin{center}
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Goal & Time + $\sigma$ & TimeM + $\sigma$ \\
        \hline
        insertWithKey & 13.31 s $\pm$ 593.4 ms & 2.667 s $\pm$ 284.3 ms \\
        \hline
    \end{tabular}
\end{center}
%
The implementation could be improved by better combining the
memoization with the LogicT monad transformer (remembering decisions across
backtracking branches) and by finetuning key-equality for polymorphic types.



\mypara{Debugging} The rules described in the formal
system\ref{sec:formal_system} to handle infinite recursion might seem obvious in
retrospective, however, when faced with a program that doesn't terminate, what
went wrong is not so clear. While we were able to figure out a solution for the
first couple of "infinite examples", the following problems were much harder,
taking sometimes full days to solve (and as of now, some still remain unsolved,
e.g. List (List a) $\lolli$ List a). To make the debugging experience better, we
developed a tracing system that prints information whenever a rule is applied,
alongside the stack of rules applied up until that one. This improved our
debugging speed considerably, especially when put together with a correct
derivation done by hand, with which we could match what was happening against
what was supposed to happen.

Unfortunately, it seems that sometimes in the process of backtracking, stack
information is "lost", and the trace logs lose useability.

\mypara{Interface} A simple, syntax-highlighted, web interface was developed
alongside a server to make possible experimenting with the \synname\ 
language without having to download and compile the complete toolchain. The web
interface is available from the repository~\cite{github repo}.

% A implementação foi feita em haskell e o código está disponível em X e há um
% live demo com exemplos pré-feitos em Y?




\section{Related Work}

Type-based program synthesis is a vast field of study and so it
follows that a lot of literature is available to inspire and
complement our work. Most works~\cite{DBLP:conf/lopstr/HughesO20,DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16} follow some variation of the
synthesis-as-proof-search approach, however, the process is novel for
each, due to a variety of different rich types explored, and their
respective corresponding logics and programming languages; or nuances
of the synthesis process itself, such as complementing types with
program examples; or even the programming paradigm of the output
produced (e.g. generating heap manipulating
programs~\cite{DBLP:journals/pacmpl/PolikarpovaS19}).  Some other
common patterns we might want to follow are the use of type
refinements~\cite{DBLP:conf/pldi/PolikarpovaKS16} and the support for synthesis of recursive
functions~\cite{DBLP:conf/pldi/PolikarpovaKS16,DBLP:conf/pldi/OseraZ15}.  These works share some common ground, and base
themselves of the same works we do -- most famously the Curry-Howard
correspondence, but also, for example, works on focusing-based
proof-search~\cite{10.1093/logcom/2.3.297}.

\mypara{Type-and-Example-Directed Program Synthesis} This
work~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
explores a purely functional, recursive program synthesis technique
based on types/proof search (as we intend to do), with examples as an
auxiliar technique to trim down the program synthesis vast search
space. A good use is
made of the general strategy of using a type system to generate
programs (by ``inverting'' the type system), rather than using it to
type-check one.  Moreover, the paper uses a data structure called
``refinement tree'' in conjunction with the extra information provided
by the examples to allow for efficient synthesis of non-trivial
programs. Overall it employs good engineering to achieve type-based
synthesis of functional typed programs, with at least two additions
that we might want to follow too (type refinements and recursive
functions).

% or how to turn your type system upside down ~ não apagar a Kubrick reference? :)
  
\mypara{Program Synthesis from Polymorphic Refinement Types} The
work~\cite{DBLP:conf/pldi/PolikarpovaKS16} also studies synthesis
recursive functional programs but in a more ``advanced'' setting. The
specification is a combination of two rich forms of types: polymorphic and
refinement types (which correspond to a first-order logic through the
Curry-Howard isomorphism) -- this is an interesting combination that offers
``expressive power and decidability, which enables automatic
verification--and hence synthesis--of nontrivial programs''.  Their
approach to refinement types consists of a new algorithm that supports
decomposition of the refinement specification, allowing for separation
between the language of specification and programs and making the
approach amenable to compositional synthesis techniques.

\mypara{Resourceful Program Synthesis from Graded Linear Types} The
work~\cite{DBLP:conf/lopstr/HughesO20} synthetises programs using an
approach very similar to our own.  The work employs so-called graded
modal types, which is an refinement of pure linear types -- a more
\emph{fine-grained} version, since it allows for quantitative
specification of resource usage, in contrast to ours either
\emph{linear} or \emph{unrestricted} (via the linear logic
exponential) use of assumptions.  Their resource management is more
complex, and so they provide solutions which adapt Hodas and Miller's
resource management
approach~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:journals/tcs/LiangM09}
-- the model we will be using.

They also use focusing as a solution to trim down search space and to
ensure that synthesis only produces well-typed programs, and our
solution will use the same literature as a base.  However, since their
underlying logic is \emph{modal} rather than purely \emph{linear}, it
lacks a clear correspondence with concurrent session-typed
programs~\cite{DBLP:journals/mscs/CairesPT16,DBLP:conf/concur/CairesP10},
which is a crucial avenue of future work. Moreover, their use grading
effectively requires constraint solving to be integrated with the
synthesis procedure, which can limit the effectiveness of the overall
approach as one scales to more sophisticated settings (e.g.~refinement
types).

% \item Resource-guided synthesis \cite{DBLP:conf/pldi/KnothWP019}
% This resource-guided means something a bit different. Programs satisfy
% a functional specification and a symbolic resource bound in the sense
% of amortized analysis, but can provide technical insights.

%\section{Goals and Work Plan}

%We start with the definition of our type system and simply-typed
%functional language with linear function, sum and product types
%($\lolli$, $\tensor$, $\one$, $\with$, $\oplus$, $\bang$).  Through
%the Curry-Howard correspondence, we'll formulate the synthesis of
%typed programs in our functional language as bottom-up proof-search in
%linear logic.

%Since proof rules in \emph{natural deduction} do not entail a
%proof-search strategy, we will use the equivalent \emph{sequent
%calculus} system to formulate our typing rules in a way that is
%amenable to proof search.
%%
%In sequent calculus, rules can be naturally understood in a
%\emph{bottom-up} manner, simplifying the reasoning for the
%synthesizer. Moreover, the linear nature of our system requires a way
%to algorithmically reason about resource contexts: for instance, if we
%read rule $\tensor I$ from Section~\ref{sec:background} bottom-up, we
%must non-deterministically guess how to correctly split ambient
%resources into $\Delta_1$ and $\Delta_2$ to separately prove $A$ and
%$B$. The resource-management approach we'll adopt to solve this has been 
%previously explained in more detail.

%With the use of input/output contexts, and the sequent calculus
%formulation of the inference rules, we draw closer to an effective
%strategy for bottom-up proof search and so of program synthesis.
%%
%%
%However, the search process still encompasses significant
%non-determinism. To address this we draw on the technique of
%focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05} as
%discussed above.

%With the basic framework in place, we will consider additional
%(simply-typed) language constructs and synthesis of program fragments
%or \emph{partial synthesis} -- given a program with a typed hole,
%synthesize a term to fill out the hole accordingly. We will
%subsequently also consider synthesis of recursive functions and,
%might, later on, add refinement types also as a way to constrain the
%valid programs space.

%Putting together everything above, and the more detailed information
%in the background section -- linear types, sequent calculus, resource
%management, and focusing, we have a set of inference rules (written
%out in the appendix) that define how the synthesis process will
%progress.

%This is the theory we're interested in and have studied so far,
%which will allow us to build the actual synthetizer. From here,
%our work plan gives an overview of the steps we'll take.

%\subsection{Work Plan}

%The end goal is to have a synthetizer capable of
%taking a type (possibly refined) as input, and to produce (possibly
%interactively) one or more functions with that type, if any exist.

%As mentioned above, we will begin with (both partial and full)
%synthesis for the linear $\lambda$-calculus. We will then extend the
%language with additional type constructs (e.g. base types, records,
%algebraic data types) and consider synthesis in that setting. Finally,
%we will consider synthesis of recursive functions, and,
%time-permitting, we will consider simple forms of type refinements
%(e.g. arithmetic refinements).

%We've already implemented a parser and a type-checker for a linear
%$\lambda$-calculus using the resource management technique described
%in the previous section -- more than
%good preparation for the development of the synthesizer, they will be
%useful to \emph{validate} the synthesized functions, and to enable
%the synthesis of partial programs.
%%
%The implementations have been carried out in Haskell since it's easier
%to model and implement inference rules in a functional setting.

%\mypara{Step by step} Our work so far has been constantly accompanied
%by the study of somewhat dense theoretical material, state of the art,
%which we must understand to overcome challenges in these fields, in
%order to develop a more scalable synthetizer -- we want to display
%concrete results to prove the feasability of program synthesis from
%linear types, and thus open the possibility for future application to
%related fields, such as program synthesis based on session types for
%message-passing concurrency.

%Apart from the theory, when we started studying type systems, we
%created a simple type-checker for the simply-typed $\lambda$-calculus.
%While learning about linear type systems, and the simply-typed linear
%$\lambda$-calculus, we developed a parser, a desugaring module, and an
%evaluation module to write, analyze and execute simple programs in the
%language; culminating in writing a type-checker for the linear
%$\lambda$-calculus. At this point, we can parse, typecheck, and
%execute this small linear functional language.  The next step is to
%start developing the actual synthetizer, embedding it with all the
%techniques we've seen. Following work will be done to augment
%usability, expand the language domain, and add features such as those
%mentioned above.

% \mypara{Evaluation} Validation and evaluation of our work is fairly
% easy. Simple questions such as \emph{does it typecheck}? \emph{can
%   we synthesize}? \emph{how fast}?  are the core of our evaluation. In
% more concrete terms, we can compare our times to state of the art
% synthetizers, present a list of working examples of programs synthesized
% by our framework, and corresponding benchmarks.

% On the eventual addition of recursive functions, refinement types, and
% interactive synthesis, we'll consider, respectively, other examples to
% showcase recursive functions synthesis, updated times for benchmarks
% with refinement types, and a display of the possible user interaction
% in the user process, and the resulting function following it.


% Expand on earlier points. Start from a functional language with linear types
% ($\tensor$, $\lolli$, $\oplus$, $\bang$); build on it with more ``stuff''.
% General techniques, drawn from proof theory via props as types: explore
% focusing to tame non-determinism/search space. Partial synthesis is still
% proof search! Add other techniques as we go along (e.g. for recursion we need
% to constrain recursive calls).

% Mention that you've already implemented a type-checker for this (useful as a
% prelim. Exercise but also later, for \emph{validation}).

% Validation and evaluation: validation is as simple as ``does it typecheck''?
% can you synth? how fast?


\section{Evaluation}\label{sec:evaluation}

The benchmarks are separated into groups: theorems of linear logic, recursive
ADTs, refinements. For each goal we analyze if the synthesis process terminated
according to what was expected, the mean time and standard deviation, if the
resulting program fullfills its intended purpose

% TODO: Utilizamos qual argumento primeiro ??? para \omega com ++ ou : ?

\begin{center}
    \begin{tabular}{ |c|c|c|c|c| }
        \hline
        Group & Goal & Avg. time + $\sigma$ & Found & Is expected \\
        \hline
        \multirow{4}{4em}{Linear Logic Theor.} & uncurry & 1s & yes & yes \\ 
        & curry & 2s & yes & yes \\ 
        & call by name?? & 2s & yes & yes \\ 
        & 0/1?? & 2s & yes & yes \\ 
        \hline
        \multirow{8}{4em}{List} & singleton & 3s & yes & yes \\ 
        & insert & 2s & yes & unordered \\ 
        & map & 1s & yes & yes \\ 
        & append & 2s & yes & yes \\ 
        & foldl & 2s & yes & got (foldl . reverse) \\ 
        & foldr & 2s & yes & yes \\ 
        & concat & $\infty$ & no & no \\ 
        & uncons & 1s & yes & yes \\
        \hline
        \multirow{3}{4em}{Maybe} & return & 1s & yes & yes \\ 
        & $>>=$ & 1s & yes & yes \\
        & maybe & 1s & yes & yes \\
        & list to maybe & 12s & no & yes \\
        \hline
        \multirow{5}{4em}{Binary Tree} & singleton & 1s & yes & yes \\ 
        & insert & 1s & yes & unordered \\
        & map & 1s & yes & yes \\
        & append & 1s & yes & appends to rightmost leaf \\
        & join node & 1s & yes & joins at rightmost leaf \\
        \hline
        \multirow{7}{4em}{Map} & singleton & 1s & yes & yes \\ 
        & insert & 1s & yes & unordered \\
        & insertWithKey & 1s & yes & ignores function \\
        & union & 1s & yes & ignores keys \\
        & mapAccum & 1s & no & ?? \\
        & map & 1s & yes & yes \\
        & mapKeys & 1s & yes & yes \\
        \hline
        \multirow{1}{4em}{Misc} & either & 1s & yes & yes \\ 
        \hline
    \end{tabular}
\end{center}


Synthesis of 





\bibliographystyle{splncs04}
\bibliography{references}


\appendix

\section{Background}\label{sec:background}

\mypara{Type Systems} A type system can be formally described through
a set of inference rules that inductively define a judgment of the
form $\Gamma \vdash M : A$, stating that program expression $M$ has
type $A$ according to the \emph{typing assumptions} for variables
tracked in $\Gamma$. For instance,
$x{:}\mathsf{Int}, y{:}\mathsf{Int} \vdash x+y : \mathsf{Int}$ states
that $x+y$ has type $\mathsf{Int}$ under the assumption that $x$ and
$y$ have type $\mathsf{Int}$.  An expression $M$ is deemed well-typed
with a given type $A$ if one can construct a typing derivation with $M :
A$ as its conclusion, by repeated application of the inference rules.

The simply-typed $\lambda$-calculus is a typed core functional
language~\cite{10.5555/509043} that captures the essence of a type system in a simple and familiar environment. Its syntax consists of
functional abstraction, written $\lambda x{:}A.M$, denoting
an (anonymous) function that takes an argument of type $A$, bound to
$x$ in $M$; and application $M\,N$, with the standard meaning, and
variables $x$. For instance, the term $\lambda x{:}A.x$, denoting the identity function, is a functional abstraction
taking an argument of type $A$ and returning it back.

\mypara{Propositions as Types}
%
It turns out that the inference rules of the simply-typed
$\lambda$-calculus are closely related to those of a system of natural
deduction for intuitionistic logic~\cite{prawitznd65}. This
relationship, known as the Curry-Howard
correspondence~(see~\cite{DBLP:journals/cacm/Wadler15} for a
historical survey),
identifies that the propositions of intuitionistic logic can be read
as types for the simply-typed $\lambda$-calculus (``propositions as
types''), their proofs are exactly the program with the given type
(``proofs as programs''), and checking a proof is type checking a
program (``proof checking as type checking'').

Inference rules in natural deduction are categorized as introduction
or elimination rules; these correspond, respectively, to rules for
constructors and destructors in programming languages (e.g. function
abstraction vs application, construction of a pair vs projection).

We introduce select typing rules to both show how a type system can be
formalized and to show the relationship with (intuitionistic)
propositional logic. The following rule captures the nature of a
hypothetical judgment, allowing for reasoning from assumptions:
\[
    \infer*[right=(var)]
    {  }
    {\Gamma, u: A \vdash u : A}
\]
When seen as a typing rule for the $\lambda$-calculus, it
corresponds to the rule for typing variables -- variable $u$ has
type $A$ if the typing environment contains a variable $u$ of
type $A$.

As another concrete example, let us consider the rule for
implication ($A\rightarrow~B$):
\[
    \infer*[right=($\rightarrow I$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \rightarrow B}
\]
Logically, the rule states that to prove $A\rightarrow B$, we assume $A$
and prove $B$. Through the Curry-Howard correspondence, implication
corresponds to the function type -- the program $\lambda u. M$ has
type $A \rightarrow B$, provided $M$ has type
$B$ under the assumption that $u$ is a variable of type $A$. The rule
shown above is an \emph{introduction} rule, that introduces the
``implication'' connective.
 
Finally, let us consider an \emph{elimination} rule, also for implication:
\[
    \infer*[right=($\rightarrow E$)]
    {\Gamma \vdash M : A \rightarrow B \and \Gamma \vdash N : A}
    {\Gamma \vdash M\,N : B}
\]
Elimination rules are easier to think of in a top-down
manner. Logically, this rule states that if we prove $A \rightarrow B$
and $A$ we can prove $B$. Through the Curry-Howard correspondence,
implication elimination corresponds to function application, and its
type is the function return type -- the application $M N$ has type
$B$, provided $M$ has type $A \rightarrow B$ and $N$ has type $A$.

The Curry-Howard correspondence generalizes beyond the simply-typed
$\lambda$-calculus and propositional intuitionistic logic. It extends
to the realms of polymorphism (second-order
logic), dependent types (first-order
logic), and various other extensions of natural deduction and
simply-typed $\lambda$-calculus -- which include linear logic and
linear types.


\mypara{Linear Logic}

Linear logic \cite{DBLP:journals/tcs/Girard87} can be seen as a
resource-aware logic, where propositions are interpreted as resources
that are consumed during the inference process.  Where in standard
propositional logic we are able to use an assumption as many times as
we want, in linear logic every resource (i.e., every assumption) must
be used \emph{exactly once}, or \emph{linearly}. This usage
restriction gives rise to new logical connectives, based on the way
the ambient resources are used. For instance, conjunction, usually
written as $A\wedge B$, appears in two forms in linear logic:
multiplicative or simultaneous conjunction (written $A\tensor B$); and
additive or alternative conjunction (written $A\with
B$). Multiplicative conjunction denotes the simultaneous availability
of resources $A$ and $B$, requiring both of them to be
used. Alternative conjunction denotes the availability of $A$ and $B$,
but where only one of the two resources may be used. Similarly,
implication becomes linear implication, written $A\lolli B$,
denoting a resource that will consume (exactly one) resource $A$ to
produce a resource $B$.

To present the formalization of this logic, besides the new
connectives, we need to introduce the \emph{resource-aware context}
$\Delta$.  In contrast to the previously seen $\Gamma$, $\Delta$ is
also a list of variables and their types, but where each and every
variable must be used exactly once during inference.  So, to introduce
the connective $\tensor$ which defines a multiplicative pair of
propositions, we must use exactly all the resources
($\Delta_1, \Delta_2$) needed to realize the ($\Delta_1$) 
proposition $A$, and ($\Delta_2$) proposition $B$:
\[
    \infer*[right=($\tensor I$)]
    {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
    {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
\]
Out of the logical connectives, we need to mention one more, since it
augments the form of the judgment and it's the one that ensures
logical strength i.e. we're able to translate intuitionistic logic
into linear logic.  The proposition $\bang A$ (read \emph{of course}
$A$) is used (under certain conditions) to make a resource
``infinite'' i.e. to make it useable an arbitrary number of times. To
distinguish the ``infinite'' variables, a separate, unrestricted,
context is used -- $\Gamma$. So $\Gamma$ holds the ``infinite''
resources, and $\Delta$ the resources that can only be used once.  The
linear typing judgment for the introduction of the exponential $\bang A$
takes the form: 
\[
    \infer*[right=($\bang I$)]
    {\Gamma ; \emptyset \vdash M : A}
    {\Gamma ; \emptyset \vdash \bang M : \bang A}
\]
Logically, a proof of $\bang A$ cannot use linear resources since
$\bang A$ denotes an unbounded (potentially $0$) number of copies of
$A$. Proofs of $\bang A$ may use other unrestricted or exponential
resources, tracked by context $\Gamma$.
From a computational perspective, the type $\bang A$
internalizes the simply-typed $\lambda$-calculus in the linear
$\lambda$-calculus.

The elimination form for the exponential, written $\llet{ !u
= M}{ N}$, warrants the use of resource $A$ an unbounded
number of times in $N$ via the variable $u$:  
\[
    \infer*[right=($\bang E$)]
    {\Gamma ; \Delta_1 \vdash M : \bang A \and \Gamma, u{:}A ; \Delta_2 \vdash N : C}
    {\Gamma ; \Delta_1, \Delta_2 \vdash \llet{ !u = M}{ N} : C}
\]
Again, through the Curry-Howard correspondence, we can view the process
of finding a proof of a proposition in linear logic as the process of synthesizing a
linear functional program of the given type.


\mypara{Sequent Calculus} Inference rules in natural deduction (the
ones we have considered so far) are ill-suited for bottom-up
proof-search since elimination rules work top-down and introduction
rules work bottom-up.  A more suited candidate is the equivalent
\emph{sequent calculus} system in which \emph{all} inference rules can
be understood naturally in a \emph{bottom-up} manner.  The
introduction and elimination rules from natural deduction disappear,
and their place is taken by right and left rules, respectively.
%
Right rules correspond exactly to the introduction rules of natural
deduction, which were already understood bottom-up. The inference rule
for implication introduction ($\supset$ is used instead of
$\rightarrow$), seen above in natural deduction under the
\emph{propositions as types} subsection, corresponds to the following
right rule in sequent calculus:
\[
    \infer*[right=($\supset R$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \supset B}
  \]

Intuitively, left rules act as the elimination rules of natural deduction, but are altered
to work bottom-up, instead of top-down.
The inference rule for the also seen above implication elimination is
as follows:
\[
    \infer*[right=($\supset L$)]
    {\Gamma , u:B\vdash M : C \and \Gamma, u{:}A\supset B \vdash N : A}
    {\Gamma, u{:}A\supset B\vdash M\{(u\,N)/u\} : C}
\]
As implied by their name, left rules define how to decompose or make
use of a connective on the left of the turnstile $\vdash$. To use an
assumption of $A\supset B$ while attempting to show some proposition
$C$ we produce a proof of $A$, which allows us to use an assumption
of $B$ to prove $C$. In terms of the corresponding $\lambda$-terms,
the $\supset L$ rule corresponds to applying the variable $u$ to the
argument $N$ but potentially deep in the structure of $M$.

We'll define our inference rules for synthesis under the sequent
calculus system, for it simplifies the work to be done in the
synthetiser.


\mypara{Resource-Management}
Be it from the perspective of proof \emph{checking} (i.e.~type checking) or
proof \emph{search} linear logic poses a key challenge when compared
to the non-linear setting:
When constructing a derivation (bottom-up), we are seemingly forced to
guess how to correctly split the linear context such that the
sub-derivations have access to the correct resources (e.g.~the
$\tensor I$ rule above).
%
%

To solve this issue we will adopt the resource-management
approach
of~\cite{DBLP:journals/tcs/CervesatoHP00,DBLP:conf/lics/LiangM09},
which generalizes the judgment  from $\Delta \vdash M : A$ to
$\Delta_I \ \Delta_O \vdash M : A$, where $\Delta_I$ is an input
context and $\Delta_O$ is an output context.
Instead of requiring non-deterministic guesses of resource splits
during proof search, we track which resources are used and which are
remaining via the two contexts, leading to the following general strategy: to
prove $A\tensor B$ using (input) resources $\Delta$, prove $A$ with
input context $\Delta$,
consuming some subset of $\Delta$, and produce as output the leftover
resources $\Delta'$; prove $B$ using $\Delta'$ as its input context and then output the
remaining resources $\Delta''$; finally, after having proven
$A\tensor B$, output $\Delta''$, for subsequent derivations.


\mypara{Focusing}

Even with everything mentioned so far, non-determinism is still very
present in proof-search, e.g. at any given point, many proof
rules are applicable in general. The technique of focusing~\cite{10.1093/logcom/2.3.297,DBLP:conf/cade/ChaudhuriP05}
has been previously studied as a way to discipline proof search in
linear logic -- a method created to trim down the search space of
valid proofs in linear logic, by eagerly applying invertible rules
(i.e.~rules whose conclusion implies the premises), and then by
``focusing'' on a single connective when no more direct (invertible)
rules can be applied, that is, only applying rules that breakdown
the connective under focus or its subformulas. If the search is not
successful, the procedure backtracks and another connective is
chosen as the focus.

Focusing eliminates all of the ``don't care'' non-determinism from
proof search, since the order in which invertible rules are applied
does not affect the outcome of the search, leaving only the
non-determinism that pertains to unknowns (or ``don't know''
non-determinism), identifying precisely the points at which
backtracking is necessary.

\end{document}
