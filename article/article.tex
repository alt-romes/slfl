\documentclass{llncs}

\usepackage{cmll}
\usepackage{mathpartir}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{todonotes}

\newcommand{\lolli}{\multimap}
\newcommand{\tensor}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bang}{{!}}
\newcommand{\mypara}[1]{\paragraph{\textbf{#1}.}}

\newcommand{\llet}[2]{\mathsf{let}\,#1\,\mathsf{in}\,#2}


\title{Synthesis of Linear Functional Programs}
\author{Rodrigo Mesquita \and Bernardo Toninho}
\date{April 2021}
\institute{NOVA School of Science and Technology}

\begin{document}

\maketitle

\section{Introduction}

% General motivation?
Program synthesis is an automated or semi-automated process of
deriving a program (i.e.~generating code) from a high-level
specification.  
%
Synthesis can be seen as a means to improve a programmer's
productivity and/or program correctness (i.e. through suggestion
and/or autocompletion), or as a tool to automate certain parts of the
programming process (e.g. in the same way AI might generate some
boilerplate text to aid a journalist, rather than to replace the
journalist outright), just to name a few examples.

Specifications can take many forms (e.g.  polymorphic refinement
types~\cite{DBLP:conf/pldi/PolikarpovaKS16},
examples~\cite{DBLP:conf/popl/FrankleOWZ16}, graded linear
types~\cite{DBLP:conf/lopstr/HughesO20}, ...).  Regardless of the kind
of specification, program synthesizers have to deal with two main
inherent sources of complexity: searching over a vast space of
(potentially) valid programs, and interpreting user intent.
%
In this work we will explore type-based synthesis where specifications
take the form of linear types, which are types that limit resource
usage in programs.  Concretely, we will explore the proof-theoretic
foundations of linear types via the Curry-Howard correspondence with
linear logic, viewing program synthesis under the lens of (linear
logic) bottom-up proof search.
%

%


\mypara{Type-Based Synthesis}
% Type based synthesis
Type-based synthesis uses types as a form of program specification,
automatically or semi-automatically producing a program expression
with the given type and so matching the specification.
%
In order to make type-based specifications more expressive, type-based
synthesis frameworks use rich type systems such as refinement
types~\cite{} and polymorphic types~\cite{}.
\todo[inline]{[Estive a procurar e fiquei
um pouco confuso. polimorfismo é rich types?  dependent types are rich
types? aqueles constraints são os dependent types pelo que
percebi... mas n me parece correto. arithmetic types? temos de
rever. Por agora vou por apenas os tópicos que o professor escreveu]
-- Não há propriamente uma definição rigorosa de ``rich types'',
geralmente tudo o que vá além do tipico pode ser qualificado como
rico.}


Richer type systems allow for more precise types, which can
statically eliminate various kinds of logical errors by making certain
invalid program states ill-typed (e.g.,~a ``null aware'' type system,
will ensure at compile-time that you cannot dereference a
null-pointer). However, they can also be a burden -- the whole point
of these type systems is to ensure that \emph{fewer} programs are
deemed well-typed, which can pose additional challenges to the
development process. Type-based synthesis leverages rich types as a
way of pruning the search space, and by using types gives the user as
a more ``familiar'' specification. For instance, the type
$\mathsf{Int} \rightarrow \mathsf{Int} \rightarrow \mathsf{Int}$
specifies a (curried) function that takes two integers and produces an
integer. Viewed as a specification, it is extremely imprecise (there
are an infinite number of functions that satisfy this specification).
However, the richer type $(x{:}\mathsf{Int}) \rightarrow
(y{:}\mathsf{Int}) \rightarrow \{z{:}\mathsf{Int} \mid z = x+y\}$
very precisely specifies a function that takes integer arguments $x$
and $y$ and returns an integer $z$ that is the sum of $x$ and $y$. 


%
This work explores the process of synthesizing linear functional
programs from types based on linear logic propositions (i.e.~linear
types) by leveraging the Curry-Howard correspondence.  The
correspondence states that propositions in a logic have a direct
mapping to types, and well-typed programs correspond to to proofs of
those propositions.  As such, having a type be a proposition in linear
logic, we can relate a proof of that proposition directly to a linear
functional program — finding a proof is finding a program with that
type. Thus, we formulate synthesis as proof search in linear logic,
which allows us to inform our work with approaches from the proof
search literature.  Linear types differ from more traditional types in
that they constrain resource usage in programs by \emph{statically}
limiting the number of times certain resources can be used during
their lifetime.  They can be applied to resource-aware programming
such as concurrent programming (e.g. session types for message passing
concurrency~\cite{}), and to memory-management (e.g.~Rust's ownership
types).

\mypara{Goals.}
 % Goals
In the end, we intend to be able to do full and partial synthesis of
well-typed programs. Full synthesis consists of the production of a
function (or set of) satisfying the specification; partial synthesis
is the ``completion'' of a partial program (i.e. a function with a
\emph{hole} in it). The work will start from a small core linear --
i.e. \emph{resource-aware} functional language, building up to
recursive types/functions, with potential other avenues of further
extension. 
%
We will evaluate the work through
expressiveness benchmarks (i.e. can we synthesize ``X''?) and time
measurements (i.e. how fast can we synthesize ``X''?').

% \begin{enumerate}
% \item General motivation
%   \begin{itemize}
% \item 
% \item Specifications appear in many forms\dots
% \item Program synthesis as a means of improving program
%   correctness/programmer productivity.
% \item Challenges: space of valid programs, space of valid
%   specifications,  etc.
% \end{itemize}
% \item \emph{Type-based} program synthesis
%   \begin{itemize}
%  \item Rich types as specifications.
% \item Languages with richer type systems allow for more precise
%   types. This precision can statically eliminate various kinds of
%   logical errors by making certain invalid program states ill-typed
%   (e.g. a ``null aware'' type system, will ensure you cannot
%   dereference a null-pointer).
% \item But can also be a burden -- the whole point of these type
%   systems is to ensure that ``less'' things are well-typed, and
%   sometimes it's hard to convince the checker\dots
% \item Type-based synthesis leverages rich types as a way of pruning
%   the search space and by using types as a more ``familiar''
%   specification.
% \end{itemize}
% \item ``My'' problem: Synthesis based on linear types
%   \begin{itemize}
%   \item Linear types constrain resource usage in programs by \emph{statically} limiting
%     the number of times certain resources can be used during its
%     lifetime.
%   \item Applications in resource-aware programming such as concurrent
%     programming (e.g. session types) and memory-management
%     (e.g. Rust).
%   \item This work will study program synthesis in a functional,
%     linearly-typed setting, by leveraging the propositions-as-types
%     correspondence between linear logic and the (linear)
%     $\lambda$-calculus.
%   \item What is props-as-types? What is linear logic? Linear $\lambda$
%     is ``just'' a small functional core language.
%   \item Through props-as-types, synthesis can be formulated as proof
%     search and so we can leverage a lot of related technology from the
%     literature.
%   \end{itemize}
% \item Goals:
%   \begin{itemize}
% \item Full and partial synthesis of well-typed (in the above sense)
%   programs. Full synthesis means producing a function (or set of)
%   satisfying the spec. Partial synthesis means being given a partial
%   program (i.e. a function with a hole in it) and ``completing it''.
% \item Start from a small system, build up to recursive
%   types/functions.
% \item Potential to go further: type refinements; interactive
%   synthesis?
%  \item Evaluation through expressiveness benchmarks (can we synthesize
%    ``X''?); time measurements (how fast can we synthesize ``X''?');
%    etc.
%   \end{itemize}
% \end{enumerate}

\section{Background}

\todo{Merge! These 2 paragraphs}
A type system can be formally described through a set of inference
rules that inductively define a judgment of the form $\Gamma \vdash M
: A$, stating that program expression $M$ has type $A$ according to
the \emph{typing assumptions} for variables tracked in $\Gamma$.
\dots

\todo[inline]{Daqui, para tipificar uma função, para Curry-Howard,
  para lógica linear, para proof search, etc\dots Só faz sentido falar de dedução natural no
  contexto de ``lógica'' / Curry-Howard.}

A type system can be formally described through natural deduction -- as a set of rules that determines the concept of deduction for some language~\cite{prawitznd65}.
This comes from the close relation between logic and programming languages, the Curry-Howard isomorphism or correspondence. A logical proof describes a program, and a program describes a proof.
The main observations made are ``propositions as types'', ``proofs as
programs'', ``proof checking as type checking''
\cite{https://www.cs.cmu.edu/~fp/courses/15312-f04/handouts/23-curryhoward.pdf}.
Formalizing our type system, the language i.e. the propositions of the logic are the types themselves, the inference rules are used to construct a proof which corresponds directly to a program, and the correctness of the proof assures the program is well-typed.
The typical natural deduction judgment
\[
    A \textrm{ is a proposition}
\]
is extended to include a \emph{term}
(i.e. to make proofs explicit with a formal definition -- these proofs map directly to a $\lambda$-calculus program as stated by the Curry-Howard correspondence????),
and modified to include a list of hypothesis. 
This extended judgment, called?? a typing judgment, has the form:
\[
    \Gamma \vdash M : A
\]
In the type system context, the list of hypothesis, $\Gamma$, is called \emph{typing environment} (i.e. set of variables and their types),
 the term $M$ is the \emph{program}, and $A$, the natural deduction proposition, is the \emph{type}.
 This judgment can be read -- $M$ has type $A$ in $\Gamma$.

Type rules assert the validity of certain judgments on the basis of other judgments already known to be valid \cite{http://homepage.cs.uiowa.edu/~tinelli/classes/185/Fall06/notes/cardelli-95.pdf}.
We introduce a few typing i.e. inference rules to display how a type system would be formalized.
In natural deduction, there are both \emph{introduction} and \emph{elimination} rules for almost all logical connectives.
The following rule is a hypothesis rule ??, não sei muito bem como sair daqui:)
\[
    \infer*[right=(var)]
    {  }
    {\Gamma, u: A \vdash u : A}
\]
It reads -- the program $u$ has type $A$ if our typing environment holds a variable labeled $u$ of type $A$.
[também não sei muito bem onde quero chegar com isto :)]
As another concrete example, let us describe the inference rule for the function type ($A \supset B$):
\[
    \infer*[right=($\supset I$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \supset B }
\]
It reads -- the program $\lambda u. M$ has type $A \supset B$ ($A$ implies $B$) as long as $M$ is a program of type $B$ knowning that $u$ is a variable of type $A$.
The rule shown above is an \emph{introduction} rule, that introduces the ``implication'' connective, 

% Linear Logic
<<<<<<< HEAD
Linear logic \cite{DBLP:journals/tcs/Girard87} can be thought of as a logic of resources.
Where we were previously able to utilize a variable i.e. a hypothesis as many times as we wanted,
now we'll keep track of the amount available i.e. the ``resources'', and guarantee that every resource is used exactly once, hence the name ``Linear Logic''.
It comes that connectives must be decomposed to get more primitive ones that reflect our resource aware system.
So what would be the $\wedge$ (conjunction) in classical logic, decomposes into its respective multiplicative and additive connectives, $\tensor$ and $\with$.
To present the formalization of this logic, besides the new connectives, we need to introduce the \emph{resource-aware context} $\Delta$.
 In contrast to the previously seen $\Gamma$, $\Delta$ is also a list of variables and their types, but in which each and every variable must be used exactly once.
 So, to introduce the connective $\tensor$ which defines a multiplicative pair of propositions, we must use exactly all the resources ($\Delta_1, \Delta_2$) needed
 to make the ($\Delta_1$) first proposition, and ($\Delta_2$) second proposition:
\[
    \infer*[right=($\tensor I$)]
    {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
    {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
\]
Out of the logical connectives, we need to mention one more, since it'll alter the judgment form, and it's the one 
that ensures logical strength -- i.e. we're able to translate usual logic into linear logic.
The $\bang$ is used (under certain conditions) to make a variable ``infinite'' i.e. to make
it useable more than once. To distinguish the ``infinite'' variables, a separate context is used -- $\Gamma$. So $\Gamma$ holds the ``infinite'' resources, and $\Delta$ the resources that can only be used once.


=======
\subsection{Linear Logic}

Linear logic \cite{DBLP:journals/tcs/Girard87} can be seen as a
resource-aware logic, where propositions are interpreted as resources
that are consumed during the inference process.  Where in standard
propositional logic we are able to use an assumption as many times as
we want, in linear logic every resource (i.e., every assumption) must
be used \emph{exactly once}, or \emph{linearly}. This usage
restriction gives rise to new logical connectives, based on the way
the ambient resources are used. For instance, conjunction, usually
written as $A\wedge B$, appears in two forms in linear logic:
multiplicative or simultaneous conjunction (written $A\tensor B$); and
additive or alternative conjunction (written $A\with
B$). Multiplicative conjunction denotes the simultaneous availability
of resources $A$ and $B$, requiring both of them to be
used. Alternative conjunction denotes the availability of $A$ and $B$,
but where only one of the two resources may be used. Similarly,
implication becomes into linear implication, written $A\lolli B$,
denoting a resource that will consume (exactly one) resource $A$ to
produce a resource $B$.
 
>>>>>>> 34e0c5ccf73bf3cc0c0f8b4327a0dcf8928a2b70


\section{Separate}

More about the concepts than citing concrete works (but citations are
needed!):
\begin{enumerate}
\item PL/Type systems in a ``formal'' sense (inference rules, etc)
\item Propositions as types: Proofs as programs, proof search as
  synthesis, etc.
\item Linear logic: a ``resource aware logic''
\item Proof search in LL: challenges, focusing as a ``solution''
\item Linear $\lambda$-calculus: what does it look like.
\item Type-based synthesis: ``inverting'' a type system.
\end{enumerate}

\section{Related Work}

Concrete works:
\begin{itemize}
\item Synthesis for graded types (no recursion   -- in that work, no
  obvious connection of grading with
  concurrency)~\cite{DBLP:conf/lopstr/HughesO20}

\item Type-and-example-directed
  synthesis~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
  or how to turn your type system upside down.
  
\item Synthesis from (polymorphic) refinement types~\cite{DBLP:conf/pldi/PolikarpovaKS16}

Refinement types provide very precise specifications to assist the
synthesis process.

\item Synthesis of heap-manipulating
  programs~\cite{DBLP:journals/pacmpl/PolikarpovaS19}

Less related, but more evidence of this idea of turning ``checking''
systems into synthesis frameworks.

\item Resource-guided synthesis \cite{DBLP:conf/pldi/KnothWP019}

This resource-guided means something a bit different. Programs satisfy
a functional specification and a symbolic resource bound in the sense
of amortized analysis, but can provide technical insights.

\end{itemize}

\section{Goals and Work Plan}



Expand on earlier points. Start from a functional language with linear
types ($\tensor$, $\lolli$, $\oplus$, $\bang$); build on it with more
``stuff''. General techniques, drawn from proof theory via props as
types: explore focusing to tame non-determinism/search space. Partial
synthesis is still proof search! Add other techniques as we go along
(e.g. for recursion we need to constrain recursive calls).

Mention that you've already implemented a type-checker for this
(useful as a prelim. exercise but also later, for \emph{validation}).

Validation and evaluation: validation is as simple as ``does it typecheck''? can you synth? how fast?

\section{Bonus}

\[
  \begin{array}{lcl}
    A, B & ::= & A \tensor B \mid A \lolli B \mid A \with B \mid A
                 \oplus B \mid \bang A \mid \one \mid \dots\\[1ex]
    M,N & ::= & \lambda x{:}A.M\\
         & \mid & M\,N\\
         & \mid & (M \tensor N)\\
         & \mid & \llet{x\tensor y = M}{N} \\
         & \mid & x \\
         & \mid & \dots\\
    \end{array}
\]

\[
  \infer[$(\lolli\! I)$]
  {\Delta , x{:}A \vdash M : B }
  {\Delta \vdash \lambda x {:} A . M : A \lolli B}
  \quad
  \infer[$(\lolli\! E)$]
  {\Delta_1 \vdash M : A \lolli B \and \Delta_2 \vdash N : A}
  {\Delta_1, \Delta_2 \vdash M\,N : B}
\]

\[
  \infer*[left=($\tensor I$)]
  {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
  {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
  \quad
  \infer*[right=($\tensor E$)]
  {\Delta_1 \vdash M : A \tensor B \and \Delta_2 , x{:}A, y{:}B\vdash
    N : C }
  {\Delta_1 , \Delta_2\vdash \llet{x\tensor y = M}{N} : C }
\]




\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
