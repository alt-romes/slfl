\documentclass{llncs}

\usepackage{cmll}
\usepackage{mathpartir}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{todonotes}

\newcommand{\lolli}{\multimap}
\newcommand{\tensor}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\bang}{{!}}
\newcommand{\mypara}[1]{\paragraph{\textbf{#1}.}}

\newcommand{\llet}[2]{\mathsf{let}\,#1\,\mathsf{in}\,#2}


\title{Synthesis of Linear Functional Programs}
\author{Rodrigo Mesquita \and Bernardo Toninho}
\date{April 2021}
\institute{NOVA School of Science and Technology}

\begin{document}

\maketitle

\section{Introduction}

% General motivation?
Program synthesis is an automated or semi-automated process of
deriving a program (i.e.~generating code) from a high-level
specification.  
%
Synthesis can be seen as a means to improve a programmer's
productivity and/or program correctness (i.e. through suggestion
and/or autocompletion), or as a tool to automate certain parts of the
programming process (e.g. in the same way AI might generate some
boilerplate text to aid a journalist, rather than to replace the
journalist outright), just to name a few examples.

Specifications can take many forms (e.g.  polymorphic refinement
types~\cite{DBLP:conf/pldi/PolikarpovaKS16},
examples~\cite{DBLP:conf/popl/FrankleOWZ16}, graded linear
types~\cite{DBLP:conf/lopstr/HughesO20}, ...).  Regardless of the kind
of specification, program synthesizers have to deal with two main
inherent sources of complexity: searching over a vast space of
(potentially) valid programs, and interpreting user intent.
%
In this work we will explore type-based synthesis where specifications
take the form of linear types, which are types that limit resource
usage in programs.  Concretely, we will explore the proof-theoretic
foundations of linear types via the Curry-Howard correspondence with
linear logic, viewing program synthesis under the lens of (linear
logic) bottom-up proof search.

%

\mypara{Type-Based Synthesis}
% Type based synthesis
Type-based synthesis uses types as a form of program specification,
automatically or semi-automatically producing a program expression
with the given type and so matching the specification.
%
In order to make type-based specifications more expressive, type-based
synthesis frameworks use rich type systems such as refinement
types~\cite{} and polymorphic types~\cite{}.

Richer type systems allow for more precise types, which can
statically eliminate various kinds of logical errors by making certain
invalid program states ill-typed (e.g.,~a ``null aware'' type system,
will ensure at compile-time that you cannot dereference a
null-pointer). However, they can also be a burden -- the whole point
of these type systems is to ensure that \emph{fewer} programs are
deemed well-typed, which can pose additional challenges to the
development process. Type-based synthesis leverages rich types as a
way of pruning the search space, and by using types gives the user as
a more ``familiar'' specification. For instance, the type
$\mathsf{Int} \rightarrow \mathsf{Int} \rightarrow \mathsf{Int}$
specifies a (curried) function that takes two integers and produces an
integer. Viewed as a specification, it is extremely imprecise (there
are an infinite number of functions that satisfy this specification).
However, the richer type $(x{:}\mathsf{Int}) \rightarrow
(y{:}\mathsf{Int}) \rightarrow \{z{:}\mathsf{Int} \mid z = x+y\}$
very precisely specifies a function that takes integer arguments $x$
and $y$ and returns an integer $z$ that is the sum of $x$ and $y$. 

%
This work explores the process of synthesizing linear functional
programs from types based on linear logic propositions (i.e.~linear
types) by leveraging the Curry-Howard correspondence.  The
correspondence states that propositions in a logic have a direct
mapping to types, and well-typed programs correspond to proofs of
those propositions.  As such, having a type be a proposition in linear
logic, we can relate a proof of that proposition directly to a linear
functional program — finding a proof is finding a program with that
type. Thus, we formulate synthesis as proof search in linear logic,
which allows us to inform our work with approaches from the proof
search literature.  Linear types differ from more traditional types in
that they constrain resource usage in programs by \emph{statically}
limiting the number of times certain resources can be used during
their lifetime.  They can be applied to resource-aware programming
such as concurrent programming (e.g. session types for message passing
concurrency~\cite{}), and to memory-management (e.g.~Rust's ownership
types).

\mypara{Goals}
 % Goals
In the end, we intend to be able to do full and partial synthesis of
well-typed programs. Full synthesis consists of the production of a
function (or set of) satisfying the specification; partial synthesis
is the ``completion'' of a partial program (i.e. a function with a
\emph{hole} in it). The work will start from a small core linear --
i.e. \emph{resource-aware} functional language, building up to
recursive types/functions, with potential other avenues of further
extension. 
%
We will evaluate the work through
expressiveness benchmarks (i.e. can we synthesize a term of type $T$?) and time
measurements (i.e. how fast can we synthesize a term of type $T$?').


\section{Background}

\mypara{Type Systems} A type system can be formally described through
a set of inference rules that inductively define a judgment of the
form $\Gamma \vdash M : A$, stating that program expression $M$ has
type $A$ according to the \emph{typing assumptions} for variables
tracked in $\Gamma$. For instance,
$x{:}\mathsf{Int}, y{:}\mathsf{Int} \vdash x+y : \mathsf{Int}$ states
that $x+y$ has type $\mathsf{Int}$ under the assumption that $x$ and
$y$ have type $\mathsf{Int}$.  An expression $M$ is deemed well-typed
with a given type $A$ if one can construct a typing derivation with $M :
A$ as its conclusion, by repeated application of the inference rules.

The simply-typed $\lambda$-calculus is a typed core functional
language~\cite{} that captures the essence of a type system in a simple and familiar environment. Its syntax consists of
functional abstraction, written $\lambda x{:}A.M$, denoting
an (anonymous) function that takes an argument of type $A$, bound to
$x$ in $M$; and application $M\,N$, with the standard meaning, and
<<<<<<< HEAD
variables $x$. For instance, the term $\lambda x{:}A.x$, denoting the identity function, is a functional abstraction
taking an argument of type $A$ and returning it back.

\mypara{Propositions as Types}
%
It turns out that the inference rules of the simply-typed
$\lambda$-calculus are closely related to those of a system of natural
deduction for intuitionistic logic~\cite{prawitznd65}. This
relationship, known as the Curry-Howard correspondence~\cite{},
identifies that the propositions of intuitionistic logic can be read
<<<<<<< HEAD
as types for the simply-typed $\lambda$-calculus (``propositions as types'' observation),
their proofs are exactly the program with the given type (``proofs as programs'' observation),
and checking a proof is type checking a program (``proof checking as type checking'' observation)~\cite{}.

Inference rules in natural deduction are categorized as introduction or elimination, these
correspond to constructors and destructors in programming languages
(e.g. function abstraction vs application, construction of a pair vs
projection).

We introduce a few typing rules to both show how a type system can be
formalized and to show the relationship with (intuitionistic)
propositional logic. The following rule captures the nature of a
hypothetical judgment, allowing for reasoning from assumptions:
\[
    \infer*[right=(var)]
    {  }
    {\Gamma, u: A \vdash u : A}
\]
When seen as a typing rule for the $\lambda$-calculus, it
corresponds to the rule for typing variables -- variable $u$ has
type $A$ if the typing environment contains a variable $u$ of
type $A$.

As another concrete example, let us consider the rule for
implication ($A\rightarrow~B$):
\[
    \infer*[right=($\rightarrow I$)]
    {\Gamma, u: A \vdash M : B}
    {\Gamma \vdash \lambda u. M : A \rightarrow B}
\]
Logically, the rule states that to prove $A\rightarrow B$, we assume $A$
and prove $B$. Through the Curry-Howard correspondence, implication
corresponds to the function type -- the program $\lambda u. M$ has
type $A \rightarrow B$, provided $M$ has type
$B$ under the assumption that $u$ is a variable of type $A$. The rule
shown above is an \emph{introduction} rule, that introduces the
``implication'' connective.
 
Finally, let us consider an \emph{elimination} rule, also for the implication connective:
\[
    \infer*[right=($\rightarrow E$)]
    {\Gamma \vdash M : A \rightarrow B \and \Gamma \vdash N : A}
    {\Gamma \vdash M\,N : B}
\]
Elimination rules are easier to think of in a top-down manner. Logically, this rule states
that if we prove $A \rightarrow B$ and $A$ we can prove $B$. Through the Curry-Howard correspondence,
implication elimination corresponds to function application, and its type is the function return type
-- the program $M N$ has type $B$, provided $M$ has type $A \rightarrow B$ and $N$ has type $A$.

The Curry-Howard correspondence generalizes beyond the simply-typed
$\lambda$-calculus and propositional intuitionistic logic. It extends to
the realms of polymorphism (higher-order logic \todo{review - {https://en.wikipedia.org/wiki/Lambda_cube}} second-order propositional calculus),
dependent types (first-order predicate logic), and every other extension
of natural deduction and simply-typed $\lambda$-calculus -- which also means
linear logic and linear types.

\todo[inline]{Daqui, para tipificar uma função, para Curry-Howard,
  para lógica linear, para proof search, etc\dots Só faz sentido falar de dedução natural no
  contexto de ``lógica'' / Curry-Howard.}

\mypara{Linear Logic}

Linear logic \cite{DBLP:journals/tcs/Girard87} can be seen as a
resource-aware logic, where propositions are interpreted as resources
that are consumed during the inference process.  Where in standard
propositional logic we are able to use an assumption as many times as
we want, in linear logic every resource (i.e., every assumption) must
be used \emph{exactly once}, or \emph{linearly}. This usage
restriction gives rise to new logical connectives, based on the way
the ambient resources are used. For instance, conjunction, usually
written as $A\wedge B$, appears in two forms in linear logic:
multiplicative or simultaneous conjunction (written $A\tensor B$); and
additive or alternative conjunction (written $A\with
B$). Multiplicative conjunction denotes the simultaneous availability
of resources $A$ and $B$, requiring both of them to be
used. Alternative conjunction denotes the availability of $A$ and $B$,
but where only one of the two resources may be used. Similarly,
implication becomes into linear implication, written $A\lolli B$,
denoting a resource that will consume (exactly one) resource $A$ to
produce a resource $B$.

To present the formalization of this logic, besides the new
connectives, we need to introduce the \emph{resource-aware context}
$\Delta$.  In contrast to the previously seen $\Gamma$, $\Delta$ is
also a list of variables and their types, but where each and every
variable must be used exactly once during inference.  So, to introduce
the connective $\tensor$ which defines a multiplicative pair of
propositions, we must use exactly all the resources
($\Delta_1, \Delta_2$) needed to realize the ($\Delta_1$) 
proposition $A$, and ($\Delta_2$) proposition $B$:
\[
    \infer*[right=($\tensor I$)]
    {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
    {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
\]
Out of the logical connectives, we need to mention one more, since
it'll alter the judgment form, and it's the one that ensures logical
strength -- i.e. we're able to translate intuitionistic logic into
linear logic.
  The proposition $\bang A$ (read \emph{of course} $A$)
is used (under certain conditions) to make a resource ``infinite''
i.e. to make it useable an arbitrary number of times. To distinguish the
``infinite'' variables, a separate, unrestricted, context is used -- $\Gamma$. So
$\Gamma$ holds the ``infinite'' resources, and $\Delta$ the resources
that can only be used once.
The linear typing judgment for the introduction of the exponential $\bang$ takes the form:
\todo{A formula está um pouco estranha (faltam espaços)?}
\[
    \infer*[right=($\bang I$)]
    {\Gamma ; \emptyset \vdash M : A}
    {\Gamma ; \emptyset \vdash !M : !A}
\]
\todo{Describe exponent introduction and new gamma context}
\todo{Add h spaces in formula}
\[
    \infer*[right=($\bang E$)]
    {\Gamma ; \Delta_1 \vdash M : !A \and \Gamma, u{:}A ; \Delta_2 \vdash N : C}
    {\Gamma ; \Delta_1, \Delta_2 \vdash let !u = M \mathsf{in} N : C}
\]
\todo{Describe exponent elimination and new gamma context}

\todo{Through the Curry-Howard correspondence, we can view the process
  of finding a proof of a proposition as the process of synthesizing a
program of a given type.}


\section{Separate}

More about the concepts than citing concrete works (but citations are
needed!):
\begin{enumerate}
\item PL/Type systems in a ``formal'' sense (inference rules, etc) [DONE]
\item Propositions as types: Proofs as programs, proof search as
  synthesis, etc. [MOSTLY DONE]
\item Linear logic: a ``resource aware logic'' [ONGOING]
\item Proof search in LL: challenges, focusing as a ``solution'' [ONGOING]
\item Linear $\lambda$-calculus: what does it look like.
\item Type-based synthesis: ``inverting'' a type system.
\end{enumerate}

\section{Related Work}



\todo[inline]{Os~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
  como ``estrategia geral'' de ``ler'' um sistema de tipos +
  engenharia como um sistema de sintese de programas tipificados;
  Aqui~\cite{DBLP:conf/pldi/PolikarpovaKS16} estamos num contexto
  ``mais avançado'' (polimorfismo e tipos refinados -- ``logica de
  primeira ordem'') -- tipos ricos -- trabalho aspiracional,
  tratamento de funcao recursivas, tratamento de tipos de dados
  indutivos/algebricos; Aqui~\cite{DBLP:conf/lopstr/HughesO20},
  sintese para graded modal types, tem a ver com ``linearidade'' kind
  of, abordagem baseado em focusing (portanto Curry-Howard++), no caso
  deles acaba por ser uma lógica ``modal'' vs linear -- não permite
  futuramente explorar relações com e.g. concorrência, análise de
  recursos.}

Concrete works

\mypara{Type-and-Example-Directed Program Synthesis}~\cite{DBLP:conf/pldi/OseraZ15,DBLP:conf/popl/FrankleOWZ16}
In this paper, type synthesis is explored 

\item Type-and-example-directed
  synthesis~
  or how to turn your type system upside down.

  \item Synthesis for graded types (no recursion   -- in that work, no
  obvious connection of grading with
  concurrency)~\cite{DBLP:conf/lopstr/HughesO20}
  
  
\item Synthesis from (polymorphic) refinement types~\cite{DBLP:conf/pldi/PolikarpovaKS16}

Refinement types provide very precise specifications to assist the
synthesis process.

% \item Synthesis of heap-manipulating
%   programs~\cite{DBLP:journals/pacmpl/PolikarpovaS19}

% Less related, but more evidence of this idea of turning ``checking''
% systems into synthesis frameworks.

% \item Resource-guided synthesis \cite{DBLP:conf/pldi/KnothWP019}

% This resource-guided means something a bit different. Programs satisfy
% a functional specification and a symbolic resource bound in the sense
% of amortized analysis, but can provide technical insights.


\section{Goals and Work Plan}


Expand on earlier points. Start from a functional language with linear
types ($\tensor$, $\lolli$, $\oplus$, $\bang$); build on it with more
``stuff''. General techniques, drawn from proof theory via props as
types: explore focusing to tame non-determinism/search space. Partial
synthesis is still proof search! Add other techniques as we go along
(e.g. for recursion we need to constrain recursive calls).

Mention that you've already implemented a type-checker for this
(useful as a prelim. Exercise but also later, for \emph{validation}).

Validation and evaluation: validation is as simple as ``does it typecheck''? can you synth? how fast?

\section{Bonus}

\[
  \begin{array}{lcl}
    A, B & ::= & A \tensor B \mid A \lolli B \mid A \with B \mid A
                 \oplus B \mid \bang A \mid \one \mid \dots\\[1ex]
    M,N & ::= & \lambda x{:}A.M\\
         & \mid & M\,N\\
         & \mid & (M \tensor N)\\
         & \mid & \llet{x\tensor y = M}{N} \\
         & \mid & x \\
         & \mid & \dots\\
    \end{array}
\]

\[
  \infer[$(\lolli\! I)$]
  {\Delta , x{:}A \vdash M : B}
  {\Delta \vdash \lambda x {:} A . M : A \lolli B}
  \quad
  \infer[$(\lolli\! E)$]
  {\Delta_1 \vdash M : A \lolli B \and \Delta_2 \vdash N : A}
  {\Delta_1, \Delta_2 \vdash M\,N : B}
\]

\[
  \infer*[left=($\tensor I$)]
  {\Delta_1 \vdash M : A \and \Delta_2 \vdash N : B}
  {\Delta_1 , \Delta_2 \vdash (M \tensor N) : A \tensor B}
  \quad
  \infer*[right=($\tensor E$)]
  {\Delta_1 \vdash M : A \tensor B \and \Delta_2 , x{:}A, y{:}B\vdash
    N : C }
  {\Delta_1 , \Delta_2\vdash \llet{x\tensor y = M}{N} : C }
\]




\bibliographystyle{splncs04}
\bibliography{references}
\end{document}
